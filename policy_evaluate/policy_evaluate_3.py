# TODO:ä½¿ç”¨ç­–ç•¥è¿­ä»£å’Œå€¼è¿­ä»£ç®—æ³•æ±‚è§£ä¸Šæ¬¡æŒ‘é€‰çš„Gymç¯å¢ƒçš„æœ€ä¼˜ç­–ç•¥

##################è¿­ä»£ç­–ç•¥è¯„ä¼°ç®—æ³•å®ç°#########################
import numpy as np
import gymnasium as gym
from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv
#çŸ©é˜µæ‰©ç»´å’Œé™ç»´åº¦
#print("P_pi",Pi[1,:],np.expand_dims(Pi[1,:],axis=0))
#print(np.dot(np.expand_dims(Pi[1,:],axis=0),P_ssa[:,1,:]).squeeze())
##########åˆ©ç”¨è§£ææ³•è¿›è¡Œç­–ç•¥è¯„ä¼°###########
def V_ana_evaluate(Pi,r_sa,P_ssa,gamma):
    P_pi = np.zeros((16, 16))
    C_pi = np.zeros((16, 1))
    for i in range(16):
        # è®¡ç®—pi(a|s)*p(s'|s,a)
        P_pi[i, :] = np.dot(np.expand_dims(Pi[i, :], axis=0), P_ssa[:, i, :]).squeeze()
        # è®¡ç®—pi(a|s)*r(s,a)
        C_pi[i, :] = np.dot(r_sa[i, :], Pi[i, :])
    ############è§£ææ³•è®¡ç®—å€¼å‡½æ•°######################
    M = np.eye(16) - P_pi *gamma
    I_M = np.linalg.inv(M)
    V = np.dot(I_M, C_pi)
    return V
##########åˆ©ç”¨æ•°å€¼è¿­ä»£æ³•è¿›è¡Œç­–ç•¥è¯„ä¼°###########
def V_iter_evaluate(Pi,r_sa,P_ssa,gamma,V_init=np.zeros((16,1))):
    # åˆå§‹åŒ–å½“å‰å€¼å‡½æ•°
    V_cur = V_init
    #è®¡ç®—C_piå’ŒP_pi
    P_pi = np.zeros((16, 16))
    C_pi = np.zeros((16, 1))
    for i in range(16):
        # è®¡ç®—pi(a|s)*p(s'|s,a)
        P_pi[i, :] = np.dot(np.expand_dims(Pi[i, :], axis=0), P_ssa[:, i, :]).squeeze()
        # è®¡ç®—pi(a|s)*r(s,a)
        C_pi[i, :] = np.dot(r_sa[i, :], Pi[i, :])
    V_next = C_pi + np.dot(P_pi, V_cur)*gamma
    # è®¡ç®—è¿­ä»£ä¸€æ¬¡çš„è¯¯å·®
    delta = np.linalg.norm(V_next - V_cur)
    num=0
    while delta > 1e-6:
        print("num=",num)
        print("V",V_cur)
        V_cur = V_next
        V_next = C_pi + np.dot(P_pi, V_cur) *gamma
        delta = np.linalg.norm(V_next - V_cur)
        num+=1
    print("num:",num)
    print("V_cur",V_cur)
    return V_cur
#############ç­–ç•¥æ”¹è¿›æºä»£ç ##########         è´ªå©ªç­–ç•¥
def update_policy(r_sa,P_ssa,V,gamma):
    Pi_new = np.zeros((16, 4))
    Pi = np.zeros((16,4))
    # è®¡ç®—C_piå’ŒP_pi
    P_pi = np.zeros((16, 16))
    for i in range(16):
        q_sa = np.zeros((1, 4))
        for j in range(4):
            Pi[i, :] = 0
            Pi[i, j] = 1
            P_pi[i, :] = np.dot(np.expand_dims(Pi[i, :], axis=0), P_ssa[:, i, :]).squeeze()
            vi = np.dot(r_sa[i, :], Pi[i, :]) + np.dot(P_pi[i, :], V.squeeze())*gamma
            q_sa[0, j] = vi
        max_num = np.argmax(q_sa)
        Pi_new[i, max_num] = 1
    return Pi_new
###########ç­–ç•¥è¿­ä»£ç®—æ³•##########
def policy_iteration(Pi,r_sa,P_ssa,gamma):
    Pi_cur = Pi
    #ç­–ç•¥è¯„ä¼°
    V_cur = V_iter_evaluate(Pi_cur,r_sa,P_ssa,gamma)
    #V_cur = V_ana_evaluate(Pi_cur, r_sa, P_ssa,gamma)
    #ç­–ç•¥æ”¹è¿›
    Pi_new = update_policy(r_sa,P_ssa,V_cur,gamma)
    delta =  np.linalg.norm(Pi_new-Pi_cur)
    iter_num = 1
    while delta>1e-6:
        Pi_cur = Pi_new
        # ç­–ç•¥è¯„ä¼°
        V_cur = V_iter_evaluate(Pi_cur, r_sa, P_ssa,gamma,V_cur)
        #V_cur = V_ana_evaluate(Pi_cur, r_sa, P_ssa)
        # ç­–ç•¥æ”¹è¿›
        Pi_new = update_policy(r_sa, P_ssa, V_cur,gamma)
        delta = np.linalg.norm(Pi_new - Pi_cur)
        iter_num=iter_num+1
    return Pi_cur,iter_num

def show(env, policy, render=False):
    state, _ = env.reset()
    done = False
    while True:
        if render:
            env.render()
        action = np.argmax(policy[state])
        next_state, reward, done, truncated, _ = env.step(action)
        state = next_state
        if truncated:
            break

def test_policy(policy, num_episodes=1000):
    # åˆ›å»ºæ— æ¸²æŸ“çš„æµ‹è¯•ç¯å¢ƒï¼ˆå‚æ•°ä¸è®­ç»ƒç¯å¢ƒä¿æŒä¸€è‡´ï¼‰
    test_env = gym.make('FrozenLake-v1', 
                      is_slippery=True,
                      render_mode=None).unwrapped
    success_count = 0
    
    for _ in range(num_episodes):
        state, _ = test_env.reset()
        done = False
        
        while not done:
            action = np.argmax(policy[state])
            next_state, reward, done, truncated, _ = test_env.step(action)
            state = next_state
            
            # åˆ¤æ–­æ˜¯å¦æˆåŠŸåˆ°è¾¾ç›®æ ‡
            if done and reward == 1:
                success_count += 1
                break
    test_env.close()
    return success_count / num_episodes

def visualize_policy(policy, env):
    # è·å–ç¯å¢ƒå¸ƒå±€æè¿°
    desc = env.unwrapped.desc  # 4x4çš„å­—ç¬¦çŸ©é˜µ
    
    # åŠ¨ä½œåˆ°ç®­å¤´çš„æ˜ å°„
    arrow_map = {
        0: 'â†',  # å·¦
        1: 'â†“',  # ä¸‹
        2: 'â†’',  # å³
        3: 'â†‘'   # ä¸Š
    }
    
    # æ„å»ºå¯è§†åŒ–çŸ©é˜µ
    grid = []
    for row in range(4):
        current_row = []
        for col in range(4):
            state = row * 4 + col
            cell_char = desc[row, col].decode('utf-8')
            
            # å¤„ç†ç‰¹æ®Šå•å…ƒæ ¼
            if cell_char == 'H':
                current_row.append('â›³')  # å†°æ´
            elif cell_char == 'G':
                current_row.append('ğŸ¯')  # ç›®æ ‡
            else:
                # è·å–è¯¥çŠ¶æ€çš„æœ€ä¼˜åŠ¨ä½œ
                action = np.argmax(policy[state])
                current_row.append(arrow_map[action])
        grid.append(current_row)
    
    # æ‰“å°å¯è§†åŒ–ç»“æœ
    print("\næœ€ä¼˜ç­–ç•¥å¯è§†åŒ–ï¼š")
    print("+" + "-"*13 + "+")
    for i, row in enumerate(grid):
        print("| " + " | ".join(row) + " |")
        if i < 3: 
            print("|" + "----+---+---+----" + "|")
    print("+" + "-"*13 + "+")

if __name__ == '__main__':
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make('FrozenLake-v1',is_slippery=True,render_mode = "human").unwrapped
    nS = env.observation_space.n  # çŠ¶æ€æ•° (16)
    nA = env.action_space.n       # åŠ¨ä½œæ•° (4)
    gamma = 1
    # åˆå§‹åŒ–è½¬ç§»çŸ©é˜µå’Œå¥–åŠ±çŸ©é˜µ
    P_ssa = np.zeros((nA, nS, nS))
    r_sa = np.zeros((nS, nA))
    Pi = np.ones((nS, nA)) / nA
    # å¡«å……P_ssaå’Œr_sa
    for s in range(nS):
        for a in range(nA):
            transitions = env.P[s][a]
            for (prob, next_s, reward, done) in transitions:
                P_ssa[a, s, next_s] += prob
                r_sa[s, a] += prob * reward  # è®¡ç®—æœŸæœ›å¥–åŠ±
    Pi_optim,iter_num = policy_iteration(Pi,r_sa,P_ssa,gamma)
    print("æœ€ä¼˜ç­–ç•¥çŸ©é˜µä¸ºï¼š",Pi_optim,"è¿­ä»£è½®æ•°ä¸ºï¼š",iter_num)
    
    # æµ‹è¯•æˆåŠŸç‡ä»£ç 
    success_rate = test_policy(Pi_optim, num_episodes=1000)
    print(f"\næœ€ä¼˜ç­–ç•¥æˆåŠŸç‡ç»Ÿè®¡: {success_rate*100:.2f}% (1000æ¬¡å°è¯•)")
    # åœ¨ç»ˆç«¯å¯è§†åŒ–æœ€ä¼˜ç­–ç•¥
    print("\nå¯è§†åŒ–æœ€ä¼˜ç­–ç•¥ï¼š")
    visualize_policy(Pi_optim, env)
    # æ˜¾ç¤ºæœ€ä¼˜ç­–ç•¥
    show(env,Pi_optim)




