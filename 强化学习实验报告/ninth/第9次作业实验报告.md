<h1><center>强化学习实验第九次实验报告</center></h1>
<h4><center>智能科学与技术 2213530 张禹豪</center></h4>

## 一、实验目的

本实验旨在通过实现和训练DDPG（Deep Deterministic Policy Gradient，深度确定性策略梯度）算法，深入理解其在连续动作空间强化学习任务中的原理与应用。具体目标包括：

- 掌握DDPG算法的核心思想、数学推导及其关键实现细节；
- 熟悉DDPG在OpenAI Gym经典环境（LunarLanderContinuous-v2）中的训练流程；
- 通过与PPO等主流算法的对比，验证DDPG在样本效率、收敛速度和最终性能等方面的表现；
- 培养分析和解决实际强化学习问题的能力，为后续更复杂环境和算法的研究打下基础。

---

## 二、实验原理

### 2.1 DDPG算法基本思想与结构

DDPG（Deep Deterministic Policy Gradient，深度确定性策略梯度）是一种专为连续动作空间强化学习任务设计的off-policy、基于actor-critic结构的强化学习算法。它结合了确定性策略梯度（DPG）和深度神经网络的强大表达能力，并借鉴了DQN的经验回放和目标网络机制，极大提升了训练的稳定性和效率。

#### 2.1.1 Actor-Critic结构详解

在强化学习中，Actor-Critic结构是一种非常经典且高效的框架。它将策略优化（Actor）和价值评估（Critic）分开处理，各自专注于不同的任务：
- **Actor（策略网络）**：给定状态$( s )$，输出确定性动作$( a = \mu_\theta(s) )$。Actor的目标是学习一个最优策略，使得在每个状态下都能选择最优动作。
- **Critic（价值网络）**：给定状态$( s )$和动作$( a )$，输出Q值$( Q_\phi(s, a) )$，即该状态-动作对的期望回报。Critic的目标是评估Actor当前策略的好坏，为Actor的改进提供方向。

**结构优势与动机：**
- 这种结构将"做决策"和"评估决策"分离，便于分别优化。
- Actor通过Critic的反馈不断改进策略，Critic则通过环境反馈不断提升评估准确性。
- 在连续动作空间下，Actor直接输出动作，避免了离散动作空间下的动作概率分布采样问题。

#### 2.1.2 确定性策略梯度（DPG）原理

传统的策略梯度方法（如REINFORCE、PPO）通常针对**随机策略**$(\pi_\theta(a|s))$，即策略输出的是一个概率分布，动作通过采样获得。这种方法在连续动作空间下会遇到采样效率低、方差大等问题。

DDPG采用**确定性策略**$(\mu_\theta(s))$，即策略直接输出一个具体动作。这样做的好处是：
- 避免了对动作概率分布的采样，提升了采样效率。
- 适合高维、连续动作空间的控制任务，如机械臂、无人机等。

确定性策略梯度定理如下：
$$
\nabla_\theta J(\mu_\theta) = \mathbb{E}_{s \sim \mathcal{D}} \left[ \nabla_\theta \mu_\theta(s) \nabla_a Q^\mu(s, a) \big|_{a = \mu_\theta(s)} \right]
$$

**公式意义：**
- $J(\mu_\theta)$表示策略的期望回报。
- 梯度的计算依赖于当前策略输出的动作对Q值的影响（即$\nabla_a Q^\mu(s, a)$），以及策略参数对动作的影响（$\nabla_\theta \mu_\theta(s)$）。
- 这意味着Actor的参数更新方向是"让Q值变大的方向"，即让智能体在未来获得更高的回报。
- 这种方式极大降低了方差，提高了训练效率。

#### 2.1.3 Critic网络的目标与贝尔曼方程

Critic网络的目标是最小化贝尔曼误差，即使Q值估计尽量接近真实回报。损失函数为：
$$
L(\phi) = \mathbb{E}_{(s, a, r, s', d) \sim \mathcal{D}} \left[ \left( Q_\phi(s, a) - y \right)^2 \right]
$$
其中目标Q值为：
$$
y = r + \gamma (1 - d) Q_{\phi'}(s', \mu_{\theta'}(s'))
$$
- $\gamma$：折扣因子，控制未来奖励的影响。
- $d$：终止信号（done），若为终止状态则无未来回报。
- $\phi'$、$\theta'$：分别为目标Critic和目标Actor的参数。

**公式意义与机制解释：**
- Critic的训练目标是让当前Q值预测$Q_\phi(s, a)$尽量接近目标Q值$y$。
- 目标Q值$y$由即时奖励$r$和下一个状态的目标网络Q值加权和组成。
- 这种方式能有效缓解Q值估计的偏差和方差，提升训练稳定性。
- 与DQN类似，DDPG也采用目标网络来计算目标Q值，防止目标值随主网络剧烈波动。

#### 2.1.4 目标网络与软更新机制
为提升训练稳定性，DDPG引入了目标网络（Target Network）机制。目标网络参数$\theta'$、$\phi'$以较慢速率跟随主网络参数$\theta$、$\phi$更新：
$$
\theta' \leftarrow \tau \theta + (1 - \tau) \theta'
$$
$$
\phi' \leftarrow \tau \phi + (1 - \tau) \phi'
$$
其中$(\tau \ll 1)$（如0.005），称为软更新。

**机制意义与优势：**
- 目标网络的引入可以防止目标值$y$在训练过程中剧烈波动，提升训练的稳定性。
- 软更新（Polyak平均）让目标网络缓慢跟随主网络，避免了DQN中硬更新带来的不稳定。
- 这种机制在连续控制任务中尤为重要，因为Q值的估计更容易受到噪声和策略变化的影响。

#### 2.1.5 经验回放机制
DDPG采用经验回放池（Replay Buffer），将智能体与环境交互的每条经验$(s, a, r, s', d)$存储起来，训练时从中随机采样小批量数据，打破数据相关性，提高样本利用率。

**机制意义与优势：**
- 经验回放可以让每条经验被多次利用，提升样本效率。
- 随机采样能打破数据之间的相关性，提升训练的收敛性和泛化能力。
- 经验回放池的引入使得DDPG成为off-policy算法，可以利用历史数据反复训练。
- 与PPO等on-policy算法相比，DDPG对样本的利用率更高，适合样本获取成本高的实际场景。

#### 2.1.6 探索策略与噪声机制
由于Actor输出为确定性动作，DDPG在训练时通过在动作上添加噪声（如Ornstein-Uhlenbeck噪声）实现探索：
$$
a_t = \mu_\theta(s_t) + \mathcal{N}_t
$$
Ornstein-Uhlenbeck噪声适合物理控制任务，能产生时序相关的平滑噪声。

**机制意义与优势：**
- 确定性策略本身没有探索性，必须人为添加噪声。
- OU噪声相比高斯噪声更平滑，适合物理系统的连续控制。
- 噪声强度可随训练进程逐步衰减，前期鼓励探索，后期收敛。

#### 2.1.7 算法整体流程

DDPG的完整流程如下：
1. 初始化Actor、Critic及其目标网络，经验回放池。
2. 在环境中采集一条轨迹，动作加噪声。
3. 存储经验到回放池。
4. 从回放池采样小批量数据，更新Critic和Actor。
5. 软更新目标网络参数。
6. 持续训练，直到回报收敛。

伪代码如下：
```
初始化Actor、Critic及其目标网络
for 训练轮次 = 1, 2, ... do
    采集一条轨迹，存储经验(s, a, r, s', d)
    for 每个训练步 do
        从回放池采样(batch_state, batch_act, batch_reward, batch_next_state, batch_done)
        计算目标Q值: y = r + γ(1-d)Q'(s', μ'(s'))
        更新Critic: 最小化 (Q(s, a) - y)^2
        更新Actor: 最大化 Q(s, μ(s))
        软更新目标网络
    end for
end for
```
### 2.2 数学公式推导与细节

**1. 策略梯度推导**
对于确定性策略$\mu_\theta(s)$，目标为最大化期望回报：
$$
J(\theta) = \mathbb{E}_{s \sim \mathcal{D}} [ Q^\mu(s, \mu_\theta(s)) ]
$$
梯度为：
$$
\nabla_\theta J(\theta) = \mathbb{E}_{s \sim \mathcal{D}} \left[ \nabla_\theta \mu_\theta(s) \nabla_a Q^\mu(s, a) \big|_{a = \mu_\theta(s)} \right]
$$
- 该公式表明，策略参数的更新方向取决于当前策略输出动作对Q值的影响。
- 直观上，就是让Actor输出的动作能让Critic评估的Q值变大。

**2. Critic网络损失**
$$
L(\phi) = \mathbb{E}_{(s, a, r, s', d)} \left[ \left( Q_\phi(s, a) - (r + \gamma (1-d) Q_{\phi'}(s', \mu_{\theta'}(s'))) \right)^2 \right]
$$
- Critic的损失函数是均方误差，目标是让Q值预测尽量接近目标Q值。
- 目标Q值由即时奖励和下一个状态的目标网络Q值组成，体现了贝尔曼方程的思想。

**3. 目标网络软更新**
$$
\theta' \leftarrow \tau \theta + (1 - \tau) \theta'
$$
$$
\phi' \leftarrow \tau \phi + (1 - \tau) \phi'
$$
- Polyak平均（软更新）能有效提升训练稳定性。
- 目标网络参数缓慢跟随主网络，防止目标值剧烈波动。

**4. 探索噪声**
$$
a_t = \mu_\theta(s_t) + \mathcal{N}_t
$$
其中$\mathcal{N}_t$为OU噪声或高斯噪声。
- 该机制保证了确定性策略下的探索性。
- OU噪声的平滑特性适合物理系统的连续控制。

---

## 三、代码实现
### 3.1 经验回放缓冲区(Experience_Buffer)
`Experience_Buffer` 类是强化学习中至关重要的组件，它实现了​**​经验回放​**​机制，这是解决强化学习样本关联性和非平稳分布问题的核心方法。以下详细解析其实现和作用：
#### 3.1.1  类定义与初始化

```python
class Experience_Buffer():
    def __init__(self, buffer_size=40000):
        self.buffer = []  # 存储经验的列表
        self.buffer_size = buffer_size  # 最大容量
        # 定义状态和动作维度
        self.obs_dim = 8  # LunarLanderContinuous-v2 状态维度
        self.act_dim = 2  # LunarLanderContinuous-v2 动作维度
```
关键参数：
- ​**​buffer_size​**​：缓冲区最大容量，防止内存无限增长
- ​**​obs_dim​**​：环境状态维度（LunarLanderContinuous-v2 为 8）
- ​**​act_dim​**​：环境动作维度（LunarLanderContinuous-v2 为 2）
#### 3.1.2 add_experience 方法 - 添加经验
```python
def add_experience(self, state, action, reward, next_state, done):
    # 创建经验字典
    experience = {
        'state': state,
        'action': action,
        'reward': reward,
        'next_state': next_state,
        'done': done  # 新增终止信号
    }
    
    # 缓冲区溢出处理：移除旧经验
    if len(self.buffer) >= self.buffer_size:
        self.buffer.pop(0)  # 移除最旧的样本
        
    self.buffer.append(experience)  # 添加新样本
```
#### 功能与作用
1. ​**​存储完整经验单元​**​：以字典形式存储强化学习的五元组 `(s, a, r, s', done)`
    - `state`：当前状态
    - `action`：执行的动作
    - `reward`：获得的即时奖励
    - `next_state`：转移后的新状态
    - `done`：终止标志（关键改进）
2. ​**​先进先出队列管理​**​：
    - 当缓冲区满时(`len>=buffer_size`)，移除最早的经验
    - 保持缓冲区大小稳定，防止内存无限增长
3. ​**​终止信号​**​ `done` 的重要性：
    - 标记回合是否结束
    - 在计算目标值时：`done=True` 表示无后继状态
    - 公式：目标值$=r+γ∗(1−done)∗Q(s′,a′)$
#### 3.1.3 sample 方法 - 采样批次数据
```python
def sample(self, batch_size):
    # 确保不超出现有数据量
    batch_size = min(batch_size, len(self.buffer))
    
    # 随机不重复采样
    indices = np.random.choice(len(self.buffer), batch_size, replace=False)
    
    # 预分配内存提升效率
    states = np.zeros((batch_size, self.obs_dim))
    actions = np.zeros((batch_size, self.act_dim))
    rewards = np.zeros((batch_size, 1))
    next_states = np.zeros((batch_size, self.obs_dim))
    dones = np.zeros((batch_size, 1))  # 新增终止信号数组
    
    # 填充数据
    for i, idx in enumerate(indices):
        exp = self.buffer[idx]
        states[i] = exp['state']
        actions[i] = exp['action']
        rewards[i] = exp['reward']
        next_states[i] = exp['next_state']
        dones[i] = float(exp['done'])  # 转为浮点型
        
    # 转为PyTorch张量
    states = torch.as_tensor(states, dtype=torch.float32)
    actions = torch.as_tensor(actions, dtype=torch.float32)
    rewards = torch.as_tensor(rewards, dtype=torch.float32)
    next_states = torch.as_tensor(next_states, dtype=torch.float32)
    dones = torch.as_tensor(dones, dtype=torch.float32)
    
    return states, actions, rewards, next_states, dones
```
#### 功能与作用
1. ​**​随机采样策略​**​：
    - `np.random.choice(len(self.buffer), batch_size, replace=False)`
    - ​**​不重复采样​**​ (replace=False)：确保每个样本只被使用一次，提高样本多样性
    - 相比顺序采样，打破时间相关性
2. ​**​高效内存预分配​**​：
    - 预先创建空数组 `np.zeros((batch_size, dim))`
    - 避免动态扩展数组带来的性能开销
3. **数据处理流程**：
![[ninth/assets/1.png]]
### 3.2 Actor_Net：确定性策略网络详解
`Actor_Net` 类是DDPG算法中实现​**​确定性策略​**​的核心组件，负责学习状态到动作的映射关系，为智能体提供连续空间内的最优决策。
#### 3.2.1 类结构及设计原理
```python
class Actor_Net(nn.Module):
    def __init__(self, obs_dim, act_dim, hidden_sizes, action_bounds=None):
        super(Actor_Net,self).__init__()
        self.act_dim = act_dim
        self.action_bounds = action_bounds  # 关键：动作边界参数
        # 均值网络
        self.actor_net = nn.Sequential(
          layer_init(nn.Linear(obs_dim, hidden_sizes[0])),
          nn.ReLU(),
          layer_init(nn.Linear(hidden_sizes[0], hidden_sizes[1])),
          nn.ReLU(),
          layer_init(nn.Linear(hidden_sizes[1], act_dim), std=0.01),
          nn.Tanh()  # 核心激活函数
        )
        self.actor_net.requires_grad_()  # 确保参数需要梯度

    def forward(self, obs):
        # 主计算流程
        action_raw = self.actor_net(obs)  # 经过全网络计算
        # 动作边界处理
        if self.action_bounds is not None:
            # 处理不同类型边界
            action_low, action_high = self.action_bounds
            # 转换为张量（如需要）
            action_low = action_low.detach() if ... else torch.tensor(action_low)
            # 线性映射到动作空间
            action = action_raw * 0.5 * (action_high - action_low) + 0.5 * (action_high + action_low)
        else:
            action = action_raw
        return action
    
    def get_a(self, obs):
        # 无梯度推理接口
        with torch.no_grad():
            action = self.forward(obs)
        return action
```
#### 3.2.2 关键组件深度解析
##### 3.2.2.1 网络架构设计
```
输入层(obs_dim) → 全连接层(64) → ReLU → 全连接层(64) → ReLU → 输出层(act_dim) → Tanh
```
- **输入层​**​：接收状态向量，维度为`obs_dim`（如LunarLander的8维状态）
- ​**​隐藏层​**​：
    - 两个64单元的隐藏层
    - 使用ReLU激活：非线性建模能力 + 计算高效
- ​**​输出层​**​：
    - 维度匹配动作空间维度`act_dim`（如LunarLander的2维动作）
    - 使用​**​Tanh激活​**​：输出值域为[-1, 1]的标准范围
    - 特殊初始化：`std=0.01`小方差初始化，确保初始输出接近0
##### 3.2.2.2 双重动作获取接口
```python
def forward(self, obs):  # 带梯度的训练接口
    ...  # 完整计算图
def get_a(self, obs):    # 无梯度的推理接口
    with torch.no_grad():
        ...  # 无计算图
```
**设计哲学​**​：
- `forward()`：训练时使用，构造完整计算图用于梯度回传
- `get_a()`：经验收集时使用
    - 无梯度计算节省内存
    - 调用效率更高（适合高频环境交互）
    - 输出可直接用于`env.step()`
### 3.3 Critic_Net：值函数网络详解
`Critic_Net` 类在DDPG算法中实现​**​值函数评估器​**​，它负责精确评估状态-动作对(state-action pair)的预期累积回报。这是DDPG算法能够区分"好动作"和"差动作"的关键组件。
#### 3.3.1 网络架构解析
```python
class Critic_Net(nn.Module):
    def __init__(self, obs_dim, action_dim, hidden_sizes):
        super(Critic_Net, self).__init__()
        # 状态分支
        self.layer_1 = layer_init(nn.Linear(obs_dim, hidden_sizes[0]))
        # 动作分支
        self.layer_2 = layer_init(nn.Linear(action_dim, hidden_sizes[0]))
        # 联合处理层
        self.layer_3 = layer_init(nn.Linear(hidden_sizes[0], hidden_sizes[1]))
        # 输出层
        self.output = layer_init(nn.Linear(hidden_sizes[1], 1))
    
    def forward(self, obs, action):
        # 1. 独立处理状态和动作
        state_path = self.layer_1(obs) 
        action_path = self.layer_2(action)
        # 2. 特征融合
        q = state_path + action_path
        # 3. 非线性处理
        q = torch.relu(self.layer_3(q))
        # 4. 输出Q值
        q = self.output(q)
        return q
```
1. ​**​状态分支​**​：
    - 处理高维状态信息
    - 学习状态表示：地形、速度等关键特征
    - 维度：obs_dim → hidden_size[0]
2. ​**​动作分支​**​：
    - 处理连续动作向量
    - 学习动作效果表示：引擎推力等
    - 维度：act_dim → hidden_size[0]
3. ​**​特征融合层​**​：
    - 采用元素加法(feature summation)而非拼接(concatenation)
    - 优势：参数更少，避免特征维度爆炸
    - 数学表达式：features=state_feat+action_feat
##### 分层处理机制
1. ​**​初级特征提取​**​：
    - 状态和动作分别映射到同维空间
    - 正交初始化确保特征正交性
2. ​**​特征交互层​**​：
    - 联合处理层：FC(64)+ReLU
    - 学习状态-动作的交互关系
    - 示例：特定地形+特定引擎推力的效果
3. ​**​价值评估层​**​：
    - 输出维度为1：单值Q值
    - 输出无激活函数：回归连续值域
#### 3.3.2 功能接口设计
##### 3.3.2.1 训练接口：forward()
```python
def forward(self, obs, action):
    # 完整计算图构建
    q = ...  # 计算Q值
    return q
```
**作用​**​：
- 构建完整计算图用于梯度传播
- 输入：状态张量和动作张量
- 输出：Q值预测（带自动梯度记录）
**训练逻辑​**​：
```python
# 在线网络预测
predicted_q = critic(states, actions)
# 目标网络计算目标值
with torch.no_grad():
    target_q = rewards + γ * (1-dones) * critic_target(next_states, next_actions)
# 损失计算
critic_loss = mse_loss(predicted_q, target_q)
```
##### 3.3.2.2 推理接口：get_q()
```python
def get_q(self, obs, action):
    # 无梯度推理
    with torch.no_grad():
        q = self.forward(obs, action)
    return q.numpy()
```
**作用​**​：
- 环境交互中评估动作价值
- 目标值计算时防止梯度传播
- 输出格式：NumPy数组（兼容Gym环境）
### 3.4 DDPG算法核心实现详解
`DDPG`类是深度确定性策略梯度算法的核心实现，它集成了Actor-Critic架构、经验回放和目标网络等关键组件，形成一个完整的强化学习解决方案。以下是各个部分的详细解析：
#### 3.4.1 核心组件分析
##### 3.4.1.1 初始化方法(init)
```python
def __init__(self, env):
    # 环境信息获取
    self.obs_dim = env.observation_space.shape[0]  # 状态维度
    self.act_dim = env.action_space.shape[0]  # 动作维度
    self.action_bounds = [env.action_space.low, env.action_space.high]  # 动作范围
    
    # 算法参数初始化
    self.gamma = 0.99   # 未来奖励折扣因子
    self.tau = 0.005    # 目标网络更新系数
    self.batch_num = 64  # 训练批次大小
    self.epochs = 2000   # 总训练轮次
    
    # 四网络架构创建
    self.actor = Actor_Net(...)
    self.actor_target = Actor_Net(...)  # 策略目标网络
    self.critic = Critic_Net(...)
    self.critic_target = Critic_Net(...)  # 值函数目标网络
    
    # 硬拷贝初始化目标网络
    self.hard_update(self.actor_target, self.actor)
    self.hard_update(self.critic_target, self.critic)
    
    # 经验回放缓冲区
    self.exp_buffer = Experience_Buffer()
    
    # 训练监控指标
    self.return_traj = []  # 每轮累积奖励
    self.recent_rewards = []  # 最近100轮奖励
    self.best_return = -float('inf')  # 最佳奖励记录
```
**关键设计​**​：
1. ​**​目标网络分离​**​：创建策略目标网络(actor_target)和值函数目标网络(critic_target)
2. ​**​参数硬拷贝​**​：确保目标网络初始状态与在线网络一致
3. ​**​动作边界传递​**​：将环境的动作空间边界信息传递给策略网络
4. ​**​监控指标初始化​**​：为训练过程分析奠定基础
##### 3.4.1.2 策略损失计算(compute_loss_pi)
```python
def compute_loss_pi(self, obs):
    # 1. 策略网络生成动作
    act = self.actor(obs)
    # 2. 值函数评估动作价值
    q = self.critic(obs, act)
    # 3. 损失函数：最大化期望回报 = 最小化负Q值
    loss_pi = -torch.mean(q)
    return loss_pi
```
**数学本质​**​：  
最大化策略价值函数：  
$J(π_θ​)=E_{s∼ρ^π}​[Q^π(s,π(s))]$
​**​优化原理​**​：
- 梯度方向：$∇_θ​J(π_θ​)=E[∇_θ​π_θ​(s)∇_a​Q^π(s,a)∣a=π_θ​(s)​]$
- 实现方式：通过负Q值均值构造损失函数，用梯度下降近似策略梯度上升
##### 3.4.1.3 值函数损失计算(compute_loss_critic)
```python
def compute_loss_critic(self, obs, act, reward, obs_next, done):
    # 1. 目标策略生成下一状态动作
    a_next = self.actor_target(obs_next)
    # 2. 目标值函数评估目标Q值
    q_target = self.critic_target(obs_next, a_next).detach()
    # 3. 考虑终止状态的目标值计算
    backup = reward + self.gamma * (1 - done) * q_target
    # 4. 计算TD误差
    q_current = self.critic(obs, act)
    loss_critic = ((q_current - backup) ** 2).mean()
    return loss_critic
```
**贝尔曼最优方程实现​**​：  
$L=E_{(s,a,s′,r,d)​}[(Q(s,a)−(r+γ(1−d)Q_{target}​(s′,π_{target}​(s′)))^2]$
​**​关键技术点​**​：
1. `.detach()`：阻止梯度流向目标网络
2. `(1 - done)`：正确处理终止状态价值
3. ​**​双重目标网络​**​：使用稳定的目标策略和目标值函数计算备份值
4. ​**​平方损失​**​：均方误差最小化时间差分误差(TD-error)
##### 3.4.1.4 软更新机制(soft_update)
```python
def soft_update(self, target_net, eva_net, tau):
    for target_param, param in zip(target_net.parameters(), 
                                   eva_net.parameters()):
        target_param.data.copy_(
            tau * param.data + (1 - tau) * target_param.data
        )
```
**更新公式​**​：  
$θ_{target​}←τθ+(1−τ)θ_{target}$​
​**​作用分析​**​：
- ​**​稳定训练​**​：缓慢更新目标网络参数，避免价值估计突变
- ​**​tau选择​**​：0.005平衡了跟踪速度与稳定性
- ​**​效率优势​**​：比周期性硬拷贝更平滑，学习曲线更稳定
##### 3.4.1.4 硬更新机制(hard_update)
```python
def hard_update(self, target_net, eva_net):
    for target_param, param in zip(target_net.parameters(),
                                   eva_net.parameters()):
        target_param.data.copy_(param.data)
```
**应用场景​**​：
- 初始化阶段：确保目标网络从相同起点开始
- 最佳模型保存：固定高性能策略用于测试
##### 3.4.1.5 训练主循环 (ddpg_train)
```python
def ddpg_train(self):
    for epoch in range(self.epochs):
        # 1. OU噪声初始化
        noise = OUNoise(self.act_dim)
        
        while not done:
            # 2. 策略动作生成（加噪声）
            action = self.actor.get_a(state) + noise_scale * noise.sample()
            
            # 3. 环境交互与经验存储
            obs_next, reward, done, _ = env.step(action)
            self.exp_buffer.add_experience(...)
            
            # 4. 经验采样与网络更新
            if buffer_ready:
                states, acts, rewards, next_states, dones = exp_buffer.sample()
                
                # Critic更新
                critic_loss = self.compute_loss_critic(...)
                self.critic_optimizer.step()
                
                # Actor更新
                actor_loss = self.compute_loss_pi(...)
                self.pi_optimizer.step()
                
                # 目标网络软更新（周期性）
                if update_counter % 10 == 0:
                    self.soft_update(actor_target, actor, self.tau)
                    self.soft_update(critic_target, critic, self.tau)
        
        # 5. 性能监控与模型保存
        self.return_traj.append(episode_return)
        self.plot_training_curves(...)
        
        if episode_return > self.best_return:
            torch.save(self.actor.state_dict(), 'best_model.pth')
```
**关键过程详解​**​：
1. ​**​OU噪声探索​**​：
    - 使用Ornstein-Uhlenbeck过程生成时间相关噪声
    - 动态衰减：`noise_scale = max(0.005, 0.3 * exp(-epoch/200))`
2. ​**双网络更新顺序​****​：
	- 先Critic后Actor：确保价值评估准确再改进策略
	- 周期性目标更新：每10次网络更新执行一次
3. **目标值计算技巧​**​：
	- 目标动作：$π_{target}​(s′)$
	- 目标值：$Q_{target​}(s′,π_{target​}(s′))$
4. ​**​训练监控系统​**​：
	- 实时追踪100轮平均回报
	- 自动保存最佳模型
	- 训练曲线可视化
### 3.5 Ornstein-Uhlenbeck噪声实现详解
`OUNoise` 类实现了​**​Ornstein-Uhlenbeck过程​**​，这是一种为深度确定性策略梯度(DDPG)算法设计的智能探索机制。与传统高斯噪声不同，OU噪声具有时间相关性，特别适合解决物理连续控制任务的探索挑战。
#### 3.5.1 数学原理背景
Ornstein-Uhlenbeck过程描述为随机微分方程：  
$dx_t​=θ(μ−xt_​)dt+σdW_t​$
其中：
- $x_t$​: 噪声状态变量
- $μ$: 长期均值
- $θ$: 均值回归速度
- $σ$: 波动率
- $dW_t$​: 维纳过程（布朗运动）
离散形式（欧拉-丸山离散化）：  
$x_{t+1}​=x_t​+θ(μ−x_t​)+σ⋅N(0,1)$
#### 3.5.2 类实现解析
##### 3.5.2.1 初始化方法 (__init__)
```python
def __init__(self, size, mu=0.0, theta=0.15, sigma=0.2):
    # 参数配置
    self.mu = mu * np.ones(size)  # 均值向量
    self.theta = theta             # 回归速率
    self.sigma = sigma            # 波动系数
    self.reset()                  # 状态初始化
```
**参数说明​**​：
- `size`：噪声维度（匹配动作空间维度）
- `mu`：0.0（默认）
    - 意义：噪声长期均值（希望动作偏差平均为0）
- `theta`：0.15（建议值）
    - 作用：控制噪声回归速度
    - 物理意义：系统"惯性"，值越大→回归越快
- `sigma`：0.2（建议值）
    - 作用：控制噪声波动强度
    - 应用：调整探索强度
##### 3.5.2.2 状态重置方法 (reset)
```python
def reset(self):
    """重置噪声状态到均值"""
    self.state = np.copy(self.mu)
```
**功能​**​：
- 在每个训练回合开始时调用
- 初始化噪声状态 $x_0​=μ$
- 作用：确保回合间噪声相互独立
- 必要性：防止噪声状态累积导致动作偏差
##### 3.5.2.3 噪声采样方法 (sample)
```python
def sample(self):
    """生成时间相关的噪声样本"""
    # 生成标准正态分布随机数
    random_noise = np.random.randn(*self.state.shape)
    # OU过程核心计算
    dx = self.theta * (self.mu - self.state) + self.sigma * random_noise
    # 更新状态
    self.state += dx
    return self.state
```
**计算步骤​**​：
1. 生成随机项：N(0,1)
2. 计算状态增量：  
    $dx=θ(μ−x_t​)+σ⋅random_noise$
3. 更新状态：$x_{t+1}​=x_t​+dx$
4. 返回当前状态作为噪声样本
### 3.6 测试脚本 (Gradient_Pendulm_test.py)
在测试脚本中提供了三种测试方式分别是：人类玩家测试、benchmark基准测试与单模型测试。
#### 3.6.1 人类玩家测试模式
```python
def human_play_test(env, num_episodes=5):
    """方向键控制的人类玩家测试"""
    pygame.init()
    screen = pygame.display.set_mode((400, 300))
    pygame.display.set_caption("Lunar Lander Control - 按ESC退出")
    print("\n=== 方向键控制说明 ===")
    print("上方向键: 启动主引擎")
    print("左/右方向键: 控制侧向引擎")
    print("ESC键: 退出测试\n")
    scores = []
    try:
        for episode in range(1, num_episodes+1):
            state, _ = env.reset()
            done = False
            total_reward = 0.0
            clock = pygame.time.Clock()
            # 控制参数
            main_thrust = 0.0
            lateral_thrust = 0.0
            while not done:
                # 处理事件队列
                for event in pygame.event.get():
                    if event.type == pygame.QUIT or \
                      (event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE):
                        pygame.quit()
                        return scores
                # 获取按键状态
                keys = pygame.key.get_pressed()
                # 主引擎控制（上键）
                main_thrust = 1.0 if keys[pygame.K_UP] else 0.0
                # 侧向控制（左右键）
                lateral_thrust = 0.0
                if keys[pygame.K_LEFT]:
                    lateral_thrust = 1.0
                if keys[pygame.K_RIGHT]:
                    lateral_thrust = -1.0
                # 构建动作向量 [主引擎，侧向推力]
                action = [main_thrust, lateral_thrust]
                # 执行动作
                next_state, reward, terminated, truncated, _ = env.step(action)
                total_reward += reward
                done = terminated or truncated
                state = next_state
                # 渲染与帧率控制
                env.render()
                clock.tick(30)
            scores.append(total_reward)
            print(f"回合 {episode}/{num_episodes} 得分: {total_reward:.1f}")
    finally:
        pygame.quit()
    return scores
```
- **功能​**​：允许用户通过键盘手动控制着陆器，测试人类玩家表现。
- ​**​交互逻辑​**​：
    - ​**​上方向键​**​：启动主引擎（垂直推力）
    - ​**​左/右方向键​**​：控制侧向引擎（水平推力）
    - ​**​ESC键​**​：退出测试
- ​**​技术细节​**​：
    - 使用`pygame`处理实时键盘输入和窗口渲染。
    - 帧率控制（`clock.tick(30)`）确保操作流畅。
    - 异常处理保证资源正确释放。
#### 3.6.2 模型测试函数
```python
def model_test(env, model_path, num_episodes=5, render=True):
    """AI模型测试"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # 初始化策略网络
    policy = PolicyNetwork(env.observation_space.shape[0],
                          env.action_space.shape[0]).to(device)
    policy.load_state_dict(torch.load(model_path, map_location=device))
    policy.eval()

    rewards = []
    for _ in range(num_episodes):
        state, _ = env.reset()
        done = False
        episode_reward = 0
        while not done:
            state_tensor = torch.FloatTensor(state).to(device)
            with torch.no_grad():
                mean, _ = policy(state_tensor)
            next_state, reward, terminated, truncated, _ = env.step(mean.cpu().numpy())
            episode_reward += reward
            done = terminated or truncated
            state = next_state
        rewards.append(episode_reward)
    return {
        'mean': np.mean(rewards),
        'max': np.max(rewards),
        'min': np.min(rewards)
    }
```
与训练代码不同的是，当进行测试模型的时候，动作选取使用**确定性执行​**：​
- 直接使用均值动作：`action = mean`
- 消除随机性，保证策略稳定性
#### 3.6.3 基准测试函数
```python
def benchmark_test(model_dir, human_scores):
    """基准对比测试"""
    print("\n=== 基准对比测试 ===")
    # 收集模型文件
    model_files = sorted(
        [f for f in os.listdir(model_dir) if f.endswith('.pth')],
        key=lambda x: int(re.search(r'model_(\d+).pth', x).group(1))
    )
    print(f"找到 {len(model_files)} 个模型文件")
    # 初始化环境
    env = gym.make('LunarLanderContinuous-v2')
    # 测试所有模型
    results = []
    progress = tqdm(model_files, desc="测试进度")
    for model_file in progress:
        model_path = os.path.join(model_dir, model_file)
        res = model_test(env, model_path, num_episodes=5, render=False)
        episode_num = int(re.search(r'model_(\d+).pth', model_file).group(1))
        results.append((episode_num, res))
    # 处理结果
    x = [r[0] for r in results]
    y_mean = [r[1]['mean'] for r in results]
    y_max = [r[1]['max'] for r in results]
    y_min = [r[1]['min'] for r in results]
    # 绘制对比图
    plt.figure(figsize=(15, 8))
    plt.plot(x, y_mean, 'b-', label='AI average')
    plt.fill_between(x, y_min, y_max, color='blue', alpha=0.1)
    plt.axhline(np.mean(human_scores), color='r', linestyle='--',
                label=f'human baseline ({np.mean(human_scores):.1f}±{np.std(human_scores):.1f})')
    plt.title('AI vs human')
    plt.xlabel('training episodes')
    plt.ylabel('score')
    plt.legend()
    plt.grid(True)
    # 保存结果
    plt.savefig('benchmark_comparison.png', dpi=300)
    print("\n对比图已保存为 benchmark_comparison.png")
    env.close()
```
- ​**​功能​**​：对比多个训练阶段的模型与人类玩家的表现。
- ​**​关键步骤​**​：
    1. 通过正则表达式匹配并按训练步数排序模型文件。
    2. 使用进度条（`tqdm`）可视化测试进度。
    3. 绘制折线图展示模型性能随训练的变化，并标注人类基准线。
    4. 图表保存为高分辨率PNG文件。
## 四、代码优化
我DDPG算法进行了几个关键的改进，以解决训练过程中回报波动大和训练突然下降的问题。这些改进措施主要包括：
### 1. 经验回放缓冲区优化
- 添加了`done`终止信号，正确处理终止状态的Q值计算
- 改进了采样方法，提高经验回放效率
### 2. 网络更新策略改进
- 添加了梯度裁剪（gradient clipping），限制梯度大小避免过大更新
- 修改了Q值计算方法，考虑终止状态的特殊情况
### 3. 探索策略优化
- 使用Ornstein-Uhlenbeck (OU)噪声代替简单的高斯噪声，更适合连续控制任务
- 实现动态噪声衰减，随训练进程减小探索范围
## 五、结果展示
### 5.1 训练环境介绍
### LunarLanderContinuous-v2 环境介绍
![[lunarlander.gif]]
**介绍**：LunarLanderContinuous-v2 是 OpenAI Gym 提供的强化学习环境之一，属于经典控制问题的连续动作空间版本。它模拟了登月舱（lander）在月球表面着陆的任务，目标是让智能体通过控制引擎的推力，使登月舱安全、平稳地降落在指定着陆点。
**环境概述**:
- **​任务目标​**​：控制登月舱的引擎推力，使其以低速（垂直和水平速度接近零）降落在两个黄色旗帜之间的着陆平台上。
- ​**​动作空间​**​：连续动作空间（与离散版本不同），智能体需要输出两个连续值：
    - ​**​主引擎推力​**​（垂直方向）：取值范围 `[-1, 1]`，控制垂直推力的大小。
    - ​**​侧引擎推力​**​（水平方向）：取值范围 `[-1, 1]`，控制向左或向右的推力。
    - 动作的第一个坐标确定主引擎的油门，而第二个坐标指定侧向推进器的油门。给定一个动作 `np.array([main, lateral])`，如果 `main < 0`，主引擎将完全关闭，对于 `0 <= main <= 1`，油门从 50% 到 100% 呈仿射缩放（特别是，主引擎在低于 50% 功率时不起作用）。同样，如果 `-0.5 < lateral < 0.5`，则侧向推进器根本不会点火。如果 `lateral < -0.5`，则左侧推进器将点火，如果 `lateral > 0.5`，则右侧推进器将点火。同样，油门在 -1 和 -0.5 之间（以及 0.5 和 1 之间，分别）从 50% 到 100% 呈仿射缩放。
- ​**​状态空间​**​：包含 8 个维度的观测值：
    - 登月舱的 `(x, y)` 坐标（相对于着陆点）。
    - 水平和垂直速度 ($v_x$, $v_y$)。
    - 登月舱的倾斜角度 `θ` 和角速度 `ω`。
    - 布尔值表示左/右支脚是否接触地面。
    - 布尔值表示登月舱主体是否接触地面。
- **奖励机制** ：
    从屏幕顶部移动到着陆垫并静止的奖励约为 100-140 分。如果着陆器远离着陆垫，则失去奖励。如果着陆器坠毁，则额外扣除-100 分。如果静止，则额外获得+100 分。每个与地面接触的腿+10 分。主发动机点火每帧-0.3 分。侧发动机点火每帧-0.03 分。如果 episode 得分至少为 200 分，则认为是一个解决方案。
- **终止条件**： 
	1. 着陆器坠毁（着陆器主体与月球接触）；
	2. 着陆器飞出视野（ `x` 坐标大于 1）；
	3. 着陆器未唤醒。根据 [Box2D 文档](https://box2d.org/documentation/md__d_1__git_hub_box2d_docs_dynamics.html#autotoc_md61)，未唤醒的物体是不移动且不与其他任何物体碰撞的物体
### 5.2 结果展示
#### 5.2.1 参数设置

| 超参数             | 参数设置  |
| --------------- | ----- |
| 隐藏层大小           | 64    |
| 策略网络学习率         | 3e-4  |
| 价值网络学习率         | 1e-4  |
| SGD miniBatch大小 | 64    |
| 训练轮数            | 4000  |
| 软更新参数           | 0.005 |
由于实际训练过程中内存不足导致实际只训练了2280轮。
#### 5.2.2 结果展示
##### 训练回报曲线展示

可见随着训练轮数的增加，训练回报逐渐提升，当训练至400轮左右之后训练回报逐渐收敛到200以上，这已经超过了可以认定作为本环境一个解决方案的回报值(200)。后续虽然训练回报在1000轮左右存在一些波动，但是回报能够迅速回到一个非常高的水平。这证明了DDPG算法对于解决该强化学习环境的有效性。
![[ninth/assets/training_curve.png]]
##### 测试曲线展示
![[ninth/assets/benchmark_comparison.png]]
如上图所示，在训练过程中每训练10轮即保存一次策略网络训练模型，在测试过程中按顺序分别导入上述模型进行测试，每个模型测试5次分别记录最大值、最小值和平均回报。
如上图，测试所示随训练进行模型回报很快提升并超过人类基准回报分数，之后虽然偶有波动，但是基本能够稳定在200分以上，这证明了DDPG算法对于解决该连续动作的强化学习环境的有效性。
### 5.3 与策略梯度算法、PPO算法的比较
#### 5.3.1 超参数对比

| 超参数          | 策略梯度           | PPO                | DDPG               |
| ------------ | -------------- | ------------------ | ------------------ |
| 学习率          | 3e-4           | 策略网络 3e-4，值网络 1e-3 | 策略网络 3e-4，值网络 1e-3 |
| 网络结构         | 两层隐藏层，各256个神经元 | 两层隐藏层，各64个神经元      | 两层隐藏层，各64个神经元      |
| 经验回放缓冲区      | \              | \                  | 40000              |
| 折扣因子$\gamma$ | 0.99           | 0.99               | 0.99               |
| 软更新系数$\tau$  | \              | \                  | 0.005              |
| 批量大小         | \              | 64                 | 64                 |
| 训练轮数         | 4000           | 4000               | 2280               |
| 每次更新的轨迹数     | \              | 10                 | \                  |
| 每次更新的迭代次数    | \              | 80                 | \                  |
| 裁剪比率         | \              | 0.2                | \                  |
#### 5.3.2 训练曲线对比

| 算法       | 训练曲线                                   |
| -------- | -------------------------------------- |
| **策略梯度** | ![[seventh/assets/training_curve.png]] |
| **PPO**  | ![[eighth/assets/1.png]]               |
| **DDPG** | ![[ninth/assets/training_curve.png]]   |
对比以上曲线可以发现，策略梯度算法能够快速收敛，但是训练回报最大值无法收敛到200以上，且训练曲线波动较大，并不十分稳定。
PPO算法前期效果不佳，但是训练一定次数之后训练回报能够收敛到一个较高的水准，且基本保持稳定。
DDPG算法前期能够快速收敛，但是训练过程中训练曲线存在一定的波动，总体能够在短时间内达到一个与PPO算法相似的较高水准。
#### 5.3.3 测试曲线对比

| 算法       | 测试曲线                                         |
| -------- | -------------------------------------------- |
| **策略梯度** | ![[seventh/assets/benchmark_comparison.png]] |
| **PPO**  | ![[eighth/assets/benchmark_comparison.png]]  |
| **DDPG** | ![[ninth/assets/benchmark_comparison.png]]   |
对比以上曲线可以发现，策略梯度算法测试结果表现良好，基本能够稳定在200分以上，但是单个模型多次测试波动较大，存在较大方差。
PPO算法在三个算法之中表现最佳，测试曲线不仅波动较小，而且测试回报非常高，接近最佳水平。
DDPG算法处于两者之间，虽然测试回报也能处于与PPO算法相近的水平，但是其测试方差明显要更高，单个模型测试更加不稳定。
#### 5.3.4 对比结果分析
##### 5.3.4.1 算法特点比较
#### (1) DDPG

- **确定性策略**: DDPG输出确定性动作，而不是动作概率分布
- **经验回放**: 使用经验回放缓冲区打破样本相关性，提高数据效率
- **目标网络**: 使用缓慢更新的目标网络来稳定学习过程
- **探索策略**: 使用Ornstein-Uhlenbeck噪声过程进行探索，适合连续控制问题
- **Actor-Critic结构**: 同时学习策略(Actor)和值函数(Critic)
- **离线学习**: 从经验池中批量采样数据进行学习，不需要在线收集新数据
#### (2) PPO

- **随机策略**: 输出动作概率分布（连续动作空间中为高斯分布）
- **信任区域约束**: 通过裁剪目标函数来限制策略更新幅度，防止过大更新
- **广义优势估计(GAE)**: 使用λ回报计算优势，平衡偏差和方差
- **多次策略迭代**: 对同一批数据进行多次更新，提高样本效率
- **无目标网络**: 不使用目标网络，但通过裁剪比率稳定训练
- **优势归一化**: 对优势函数进行归一化，稳定训练
- **早停机制**: 通过监控KL散度来防止策略偏移过大
#### (3) 简单策略梯度法

- **随机策略**: 输出动作概率分布
- **最简结构**: 只有一个策略网络，没有值函数网络
- **蒙特卡洛回报**: 使用完整轨迹的折扣回报估计目标
- **无经验回放**: 每次只使用当前收集的轨迹进行一次更新
- **简单探索**: 依赖于策略分布的随机采样进行探索
- **样本效率低**: 每个样本只使用一次，然后丢弃
- **高方差**: 使用蒙特卡洛回报导致梯度估计方差较高
##### 5.3.4.2 算法性能差异分析

##### (1) 样本效率
- **DDPG**: 样本效率高，能够多次重用过去的经验
- **PPO**: 中等样本效率，每批数据可以多次重用
- **简单策略梯度**: 样本效率最低，数据只使用一次
##### (2) 稳定性
- **DDPG**: 通过目标网络和经验回放提高稳定性，但调参困难，特别是对于噪声的设置
- **PPO**: 通过裁剪比率和GAE提供较好的稳定性，对超参数不太敏感
- **简单策略梯度**: 稳定性最差，训练过程波动大，容易陷入局部最优
##### (3) 算法复杂度
- **DDPG**: 实现复杂，需要管理两对网络(actor、critic及其目标网络)和经验缓冲区
- **PPO**: 中等复杂度，需要管理策略网络和值网络，以及优势计算
- **简单策略梯度**: 最简单，只需要一个策略网络
##### (4) 收敛性
- **DDPG**: 对超参数敏感，调参困难，但在调参正确的情况下可获得较好的性能
- **PPO**: 收敛通常更稳定，不易崩溃
- **简单策略梯度**: 收敛慢，且易于陷入局部最优
##### (5) 探索能力
- **DDPG**: 依赖于添加的噪声，需要精心设计探索策略
- **PPO**: 通过随机策略自然地进行探索
- **简单策略梯度**: 依赖于策略分布的随机性，但可能探索不足
##### 5.3.4.3 结果差异分析

针对LunarLanderContinuous-v2这类连续控制问题:
1. **DDPG**可能会达到较高的最终性能（更高的最大回报），但训练过程波动大，对初始条件和超参数敏感。特别适合精确控制要求高的任务，但调参难度大。
2. **PPO**通常提供更稳定的学习曲线，虽然最终性能可能略低于完美调参的DDPG，但训练更可靠，收敛更快。对于大多数问题是很好的选择，特别是当你想快速获得一个可工作的解决方案时。
3. **简单策略梯度**收敛最慢，最终性能通常低于其他两种方法，但实现简单，适合作为基准或教学用途。在简单问题上也能工作，但在复杂问题上表现会差很多。

---

## 六、实验总结

本实验通过实现DDPG算法并应用于LunarLanderContinuous-v2环境，深入探究了该算法在连续动作空间强化学习任务中的性能表现。通过实验我得到以下几点重要结论：
### 6.1 DDPG算法的优势与特点

1. **高效的确定性策略**：DDPG使用确定性策略直接输出精确动作值，避免了连续空间中采样带来的方差，特别适合精确控制任务。
2. **样本利用率高**：通过经验回放机制，DDPG能够多次利用历史交互数据，显著提高了样本效率，这在实际应用中尤为重要。
3. **目标网络稳定学习**：采用软更新机制的目标网络有效抑制了值函数估计的波动，提高了训练稳定性。
### 6.2 实现中的关键技术点

1. **Ornstein-Uhlenbeck噪声**：实验证明，相比简单高斯噪声，OU噪声在连续控制任务中更为有效，产生时间相关的平滑探索行为。
2. **动态噪声调节**：随训练进程动态调整噪声强度（从0.3衰减到0.005）有效平衡了探索与利用。
3. **梯度裁剪技术**：对Actor和Critic的梯度进行裁剪（分别为0.5和1.0）显著提高了训练稳定性。
4. **终止状态处理**：正确处理终止状态的Q值计算（`(1-done)*Q_target`）对算法性能至关重要。
### 6.3 算法比较与分析

将DDPG与策略梯度和PPO算法进行对比后发现：

1. **收敛速度**：DDPG在前期训练中能够快速收敛（约400轮达到200+回报），相比PPO具有更快的初期学习速度。
2. **稳定性权衡**：DDPG的训练曲线波动较大，相比PPO的稳定性稍差，但好于简单策略梯度方法。
3. **调参难度**：DDPG对超参数（特别是噪声设置、目标网络更新率）较为敏感，调参难度大于PPO。
4. **最终性能**：在充分训练后，DDPG能够达到与PPO相当的高水平性能（平均回报约220-250）。
### 6.4 未来改进方向

1. **更优噪声策略**：可探索参数化噪声（Parametric Noise）或自适应噪声调节策略。
2. **优先经验回放**：通过实现优先级经验回放（Prioritized Experience Replay），进一步提高样本效率。
3. **多步回报目标**：尝试n步回报或λ回报代替单步TD目标，可能进一步改善算法性能。
4. **集成正则化技术**：如L2正则化或dropout，提高策略网络的泛化能力。

通过本实验，我深入理解了DDPG算法的工作原理及其在连续控制任务中的应用价值。实验结果表明，DDPG是解决连续动作空间问题的强有力工具，特别适合需要精确控制的场景。尽管算法调参相对复杂，但通过合理的实现和调整，DDPG能够达到非常优秀的性能水平。