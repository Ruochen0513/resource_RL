################## å€¼è¿­ä»£ç®—æ³•å®ç°ï¼ˆå«gammaï¼‰#########################
import numpy as np
import gymnasium as gym

def update_policy(r_sa, P_ssa, V, gamma):  # æ·»åŠ gammaå‚æ•°
    Pi_new = np.zeros((16, 4))
    for i in range(16):
        q_sa = np.zeros(4)
        for j in range(4):
            # ç›´æ¥è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„Qå€¼
            P_pi = P_ssa[j, i, :]  # åŠ¨ä½œjçš„è½¬ç§»æ¦‚ç‡
            q_sa[j] = r_sa[i, j] + gamma * np.dot(P_pi, V.squeeze())  # æ·»åŠ gamma
        max_num = np.argmax(q_sa)
        Pi_new[i, max_num] = 1
    return Pi_new

def value_iteration(Pi_0, r_sa, P_ssa, V_init, gamma, tol=1e-6):  # æ·»åŠ gammaå‚æ•°
    V_cur = V_init
    iter_num = 0
    
    while True:
        # å€¼å‡½æ•°æ›´æ–°
        V_next = np.zeros_like(V_cur)
        for i in range(16):
            q_values = [r_sa[i, a] + gamma * np.dot(P_ssa[a, i, :], V_cur.squeeze()) 
                       for a in range(4)]
            V_next[i] = np.max(q_values)  # ç›´æ¥å–æœ€å¤§Qå€¼
        
        delta = np.linalg.norm(V_next - V_cur)
        V_cur = V_next
        iter_num += 1
        
        if delta < tol:
            break
    
    # æœ€ç»ˆç­–ç•¥æå–
    Pi_optim = update_policy(r_sa, P_ssa, V_cur, gamma)
    return Pi_optim, iter_num

# æˆåŠŸç‡æµ‹è¯•å‡½æ•°
def test_policy(policy, num_episodes=1000):
    test_env = gym.make('FrozenLake-v1', is_slippery=True, render_mode=None)
    success = 0
    
    for _ in range(num_episodes):
        state, _ = test_env.reset()
        done = False
        
        while not done:
            action = np.argmax(policy[state])
            next_state, reward, done, _, _ = test_env.step(action)
            state = next_state
            
            if done and reward == 1:
                success += 1
                
    test_env.close()
    return success / num_episodes

def visualize_policy(policy, env):
    # è·å–ç¯å¢ƒå¸ƒå±€æè¿°
    desc = env.unwrapped.desc  # 4x4çš„å­—ç¬¦çŸ©é˜µ
    
    # åŠ¨ä½œåˆ°ç®­å¤´çš„æ˜ å°„
    arrow_map = {
        0: 'â†',  # å·¦
        1: 'â†“',  # ä¸‹
        2: 'â†’',  # å³
        3: 'â†‘'   # ä¸Š
    }
    
    # æ„å»ºå¯è§†åŒ–çŸ©é˜µ
    grid = []
    for row in range(4):
        current_row = []
        for col in range(4):
            state = row * 4 + col
            cell_char = desc[row, col].decode('utf-8')
            
            # å¤„ç†ç‰¹æ®Šå•å…ƒæ ¼
            if cell_char == 'H':
                current_row.append('â›³')  # å†°æ´
            elif cell_char == 'G':
                current_row.append('ğŸ¯')  # ç›®æ ‡
            else:
                # è·å–è¯¥çŠ¶æ€çš„æœ€ä¼˜åŠ¨ä½œ
                action = np.argmax(policy[state])
                current_row.append(arrow_map[action])
        grid.append(current_row)
    
    # æ‰“å°å¯è§†åŒ–ç»“æœ
    print("\næœ€ä¼˜ç­–ç•¥å¯è§†åŒ–ï¼š")
    print("+" + "-"*13 + "+")
    for i, row in enumerate(grid):
        print("| " + " | ".join(row) + " |")
        if i < 3: 
            print("|" + "----+---+---+----" + "|")
    print("+" + "-"*13 + "+")

def show(env, policy, render=False):
    state, _ = env.reset()
    done = False
    while not done:
        if render:
            env.render()
        action = np.argmax(policy[state])
        next_state, reward, done, truncated, _ = env.step(action)
        state = next_state
        if truncated:
            break

if __name__ == '__main__':
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make('FrozenLake-v1', map_name="4x4", 
                  is_slippery=True, render_mode="human").unwrapped
    
    # åˆå§‹åŒ–è½¬ç§»çŸ©é˜µå’Œå¥–åŠ±çŸ©é˜µ
    nS = env.observation_space.n
    nA = env.action_space.n
    P_ssa = np.zeros((nA, nS, nS))
    r_sa = np.zeros((nS, nA))
    
    for s in range(nS):
        for a in range(nA):
            for (prob, next_s, reward, _) in env.P[s][a]:
                P_ssa[a, s, next_s] += prob
                r_sa[s, a] += prob * reward

    # æ‰§è¡Œå€¼è¿­ä»£
    Pi_optim, iter_num = value_iteration(
        Pi_0=np.ones((nS, nA))/nA,  # åˆå§‹éšæœºç­–ç•¥
        r_sa=r_sa,
        P_ssa=P_ssa,
        V_init=np.zeros((nS, 1)),
        gamma=1  # å¯è°ƒæ•´å‚æ•°
    )
    
    # è¾“å‡ºç»“æœ
    print(f"è¿­ä»£æ¬¡æ•°: {iter_num}")
    print("æœ€ä¼˜ç­–ç•¥çŸ©é˜µ:")
    print(Pi_optim)
    
    # æµ‹è¯•æˆåŠŸç‡
    success_rate = test_policy(Pi_optim)
    print(f"\næˆåŠŸç‡: {success_rate*100:.2f}%")

    # å¯è§†åŒ–ç­–ç•¥
    visualize_policy(Pi_optim, env)
    # ç­–ç•¥æ¼”ç¤º
    show(env, Pi_optim)
    

