##################è¿­ä»£ç­–ç•¥è¯„ä¼°ç®—æ³•å®ç°#########################
import numpy as np
import copy
import gymnasium as gym
#çŸ©é˜µæ‰©ç»´å’Œé™ç»´åº¦
#print("P_pi",Pi[1,:],np.expand_dims(Pi[1,:],axis=0))
#print(np.dot(np.expand_dims(Pi[1,:],axis=0),P_ssa[:,1,:]).squeeze())
class FrozenLakeEnv:
    def __init__(self):
        # åˆ›å»º FrozenLake ç¯å¢ƒ
        self.env = gym.make('FrozenLake-v1', is_slippery=True)
        
        # åˆå§‹åŒ–è¡Œä¸ºå€¼å‡½æ•°
        self.qvalue = -0.01 * np.zeros((16, 4))
        # åˆå§‹åŒ–æ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹çš„æ¬¡æ•°
        self.n = 0 * np.ones((16, 4))
        
        self.states = np.arange(16)
        self.actions = np.arange(4)
        self.gamma = 0.99
        
        # åˆå§‹åŒ–ç­–ç•¥
        self.Pi = 0.25 * np.ones((16, 4))
        self.cur_state = 0
        self.old_policy = np.ones((16, 4))
        
    def reset(self):
        # é‡ç½®ç¯å¢ƒå’Œè¡Œä¸ºå€¼å‡½æ•°
        self.qvalue = -0.01 * np.zeros((16, 4))
        self.n = 0 * np.ones((16, 4))
        self.cur_state, _ = self.env.reset()
        return self.cur_state
        
    def explore_init(self):
        # éšæœºé€‰æ‹©åˆå§‹çŠ¶æ€å’ŒåŠ¨ä½œ
        self.cur_state, _ = self.env.reset()
        a0 = np.random.choice(self.actions, p=[0.25, 0.25, 0.25, 0.25])
        return self.cur_state, a0
        
    def sample_action(self, state):
        # æ ¹æ®å½“å‰ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        action = np.random.choice(self.actions, p=self.Pi[state,:])
        return action
        
    def step(self, action):
        # æ‰§è¡ŒåŠ¨ä½œå¹¶è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±
        next_state, reward, terminated, truncated, _ = self.env.step(action)
        done = terminated or truncated
        return next_state, reward, done

    def update_policy(self):
        # è´ªå©ªç­–ç•¥æ›´æ–°
        for i in range(16):
            self.Pi[i,:] = 0
            max_num = np.argmax(self.qvalue[i,:])
            self.Pi[i, max_num] = 1
            
    def update_epsilon_greedy(self):
        # Îµ-è´ªå©ªç­–ç•¥æ›´æ–°
        epsilon = 0.2
        for i in range(16):
            self.Pi[i,:] = epsilon/4
            max_num = np.argmax(self.qvalue[i,:])
            self.Pi[i, max_num] = 0.85

    def MC_learning(self):
        num = 0
        while num < 6000:
            num += 1
            flag = False
            # é‡‡æ ·ä¸€æ¡è½¨è¿¹
            state_traj = []
            action_traj = []
            reward_traj = []
            g = 0
            episode_num = 0
            
            # æ¢ç´¢åˆå§‹åŒ–
            cur_state, cur_action = self.explore_init()
            
            while not flag and episode_num < 200:
                if episode_num > 0:
                    cur_action = self.sample_action(self.cur_state)
                    
                state_traj.append(self.cur_state)
                action_traj.append(cur_action)
                
                next_state, reward, flag = self.step(cur_action)
                reward_traj.append(reward)
                self.cur_state = next_state
                episode_num += 1

            # æ›´æ–°è¡Œä¸ºå€¼å‡½æ•°
            for i in reversed(range(len(state_traj))):
                self.n[state_traj[i], action_traj[i]] += 1.0
                g *= self.gamma
                g += reward_traj[i]
                self.qvalue[state_traj[i], action_traj[i]] = \
                    (self.qvalue[state_traj[i], action_traj[i]] * (self.n[state_traj[i], action_traj[i]]-1) + g) / \
                    self.n[state_traj[i], action_traj[i]]

            # å®šæœŸæ›´æ–°ç­–ç•¥
            if num % 501 == 0:
                self.old_policy = copy.deepcopy(self.Pi)
                self.update_policy()
                self.n = np.zeros((16, 4))
                delta = np.linalg.norm(self.old_policy - self.Pi)
                print("delta", delta)

def q_ana_evaluate(Pi,r_sa,P_ssa):
    P_pi = np.zeros((16, 16))
    C_pi = np.zeros((16, 1))
    for i in range(16):
        # è®¡ç®—pi(a|s)*p(s'|s,a)
        P_pi[i, :] = np.dot(np.expand_dims(Pi[i, :], axis=0), P_ssa[:, i, :]).squeeze()
        # è®¡ç®—pi(a|s)*r(s,a)
        C_pi[i, :] = np.dot(r_sa[i, :], Pi[i, :])
    ############è§£ææ³•è®¡ç®—å€¼å‡½æ•°######################
    M = np.eye(16) - P_pi
    I_M = np.linalg.inv(M)
    V = np.dot(I_M, C_pi)
    #è®¡ç®—è¡Œä¸ºå€¼å‡½æ•°
    q_value = np.zeros((16, 4))
    for i in range(16):
        q_sa = np.zeros((1, 4))
        for j in range(4):
            Pi[i, :] = 0
            Pi[i, j] = 1
            P_pi[i, :] = np.dot(np.expand_dims(Pi[i, :], axis=0), P_ssa[:, i, :]).squeeze()
            vi = np.dot(r_sa[i, :], Pi[i, :]) + np.dot(P_pi[i, :], V.squeeze())
            q_sa[0, j] = vi
        q_value[i, :] = q_sa[0, :]
    return q_value

def visualize_policy(policy, env):
    # è·å–ç¯å¢ƒå¸ƒå±€æè¿°
    desc = env.unwrapped.desc  # 4x4çš„å­—ç¬¦çŸ©é˜µ
    
    # åŠ¨ä½œåˆ°ç®­å¤´çš„æ˜ å°„
    arrow_map = {
        0: 'â†',  # å·¦
        1: 'â†“',  # ä¸‹
        2: 'â†’',  # å³
        3: 'â†‘'   # ä¸Š
    }
    
    # æ„å»ºå¯è§†åŒ–çŸ©é˜µ
    grid = []
    for row in range(4):
        current_row = []
        for col in range(4):
            state = row * 4 + col
            cell_char = desc[row, col].decode('utf-8')
            
            # å¤„ç†ç‰¹æ®Šå•å…ƒæ ¼
            if cell_char == 'H':
                current_row.append('â›³')  # å†°æ´
            elif cell_char == 'G':
                current_row.append('ğŸ¯')  # ç›®æ ‡
            else:
                # è·å–è¯¥çŠ¶æ€çš„æœ€ä¼˜åŠ¨ä½œ
                action = np.argmax(policy[state])
                current_row.append(arrow_map[action])
        grid.append(current_row)
    
    # æ‰“å°å¯è§†åŒ–ç»“æœ
    print("\næœ€ä¼˜ç­–ç•¥å¯è§†åŒ–ï¼š")
    print("+" + "-"*13 + "+")
    for i, row in enumerate(grid):
        print("| " + " | ".join(row) + " |")
        if i < 3: 
            print("|" + "----+---+---+----" + "|")
    print("+" + "-"*13 + "+")

def visualize_policy_matrix(policy, env):
    import matplotlib.pyplot as plt
    import numpy as np
    
    # è·å–ç¯å¢ƒå¸ƒå±€
    desc = env.unwrapped.desc
    
    # åˆ›å»ºå›¾å½¢
    fig, ax = plt.subplots(figsize=(8, 8))
    
    # è®¾ç½®ç½‘æ ¼
    ax.grid(True)
    ax.set_xticks(np.arange(-0.5, 4.5, 1))
    ax.set_yticks(np.arange(-0.5, 4.5, 1))
    
    # éšè—åˆ»åº¦æ ‡ç­¾
    ax.set_xticklabels([])
    ax.set_yticklabels([])
    
    # åŠ¨ä½œåˆ°ç®­å¤´çš„æ˜ å°„
    arrow_symbols = {
        0: 'â†',  # å·¦
        1: 'â†“',  # ä¸‹
        2: 'â†’',  # å³
        3: 'â†‘'   # ä¸Š
    }
    
    # å¡«å……é¢œè‰²å’Œç®­å¤´
    for i in range(4):
        for j in range(4):
            state = i * 4 + j
            cell = desc[i][j].decode('utf-8')
            
            # è®¾ç½®å•å…ƒæ ¼é¢œè‰²
            if cell == 'H':
                color = '#ffcccc'  # æµ…çº¢è‰²è¡¨ç¤ºæ´
                plt.fill([j-0.5, j+0.5, j+0.5, j-0.5], 
                        [3.5-i, 3.5-i, 4.5-i, 4.5-i], 
                        color=color)
            elif cell == 'G':
                color = '#ccffcc'  # æµ…ç»¿è‰²è¡¨ç¤ºç›®æ ‡
                plt.fill([j-0.5, j+0.5, j+0.5, j-0.5], 
                        [3.5-i, 3.5-i, 4.5-i, 4.5-i], 
                        color=color)
            else:
                color = '#ffffff'  # ç™½è‰²è¡¨ç¤ºå¯ç§»åŠ¨æ ¼å­
                plt.fill([j-0.5, j+0.5, j+0.5, j-0.5], 
                        [3.5-i, 3.5-i, 4.5-i, 4.5-i], 
                        color=color)
            
            # æ·»åŠ ç®­å¤´
            if cell != 'H' and cell != 'G':
                action = np.argmax(policy[state])
                plt.text(j, 4-i-0.5, arrow_symbols[action], 
                        ha='center', va='center', fontsize=20)
    
    plt.title('æœ€ä¼˜ç­–ç•¥å¯è§†åŒ–')
    plt.tight_layout()
    plt.show()

if __name__ == '__main__':
    # å®ä¾‹åŒ–å¯¹è±¡
    env = FrozenLakeEnv()
    env.reset()
    print("åˆå§‹ç­–ç•¥:", env.Pi)
    env.MC_learning()
    print("æœ€ç»ˆç­–ç•¥:", env.Pi)
    print("æœ€ä¼˜Qå€¼:", env.qvalue)
    print("è®¿é—®é¢‘æ¬¡:", env.n)

    # æ·»åŠ å¯è§†åŒ–
    env2 = gym.make('FrozenLake-v1', is_slippery=True)
    visualize_policy_matrix(env.Pi, env2)