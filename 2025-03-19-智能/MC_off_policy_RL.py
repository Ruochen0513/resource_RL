##################è¿­ä»£ç­–ç•¥è¯„ä¼°ç®—æ³•å®ç°#########################
import numpy as np
import copy
import gymnasium as gym
import time
import seaborn as sns
#çŸ©é˜µæ‰©ç»´å’Œé™ç»´åº¦
#print("P_pi",Pi[1,:],np.expand_dims(Pi[1,:],axis=0))
#print(np.dot(np.expand_dims(Pi[1,:],axis=0),P_ssa[:,1,:]).squeeze())
class FrozenLakeEnv:
    def __init__(self):
        # åˆ›å»º FrozenLake ç¯å¢ƒ
        self.env = gym.make('FrozenLake-v1', is_slippery=True)
        
        # åˆå§‹åŒ–è¡Œä¸ºå€¼å‡½æ•°
        self.qvalue = -10 * np.ones((16, 4))
        # åˆå§‹åŒ–æ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹çš„æ¬¡æ•°
        self.C = 0 * np.ones((16, 4))
        
        self.states = np.arange(16)
        self.actions = np.arange(4)
        self.gamma = 0.99
        
        # åˆå§‹åŒ–é‡‡æ ·ç­–ç•¥
        self.behaviour_Pi = 0.25 * np.ones((16, 4))
        # åˆå§‹åŒ–ç›®æ ‡ç­–ç•¥
        self.target_Pi = np.zeros((16, 4))
        for i in range(16):
            j = np.random.choice(self.actions, p=[0.25, 0.25, 0.25, 0.25])
            self.target_Pi[i,j] = 1
            
        self.Greedy_Pi = np.zeros((16, 4))
        self.cur_state = 0
        self.epsilon = 0.5

    def reset(self):
        self.qvalue = -10 * np.zeros((16, 4))
        self.C = 0 * np.ones((16, 4))
        self.cur_state, _ = self.env.reset()
        return self.cur_state

    def sample_action(self, state):
        action = np.random.choice(self.actions, p=self.behaviour_Pi[state,:])
        return action

    def step(self, action):
        next_state, reward, terminated, truncated, _ = self.env.step(action)
        done = terminated or truncated
        return next_state, reward, done

    def update_target_policy(self):
        epsilon = self.epsilon/10
        for i in range(16):
            self.target_Pi[i,:] = epsilon/4
            max_num = np.argmax(self.qvalue[i,:])
            self.target_Pi[i, max_num] = epsilon/4 + (1-epsilon)

    def update_behaviour_policy(self):
        for i in range(16):
            self.behaviour_Pi[i,:] = self.epsilon/4
            max_num = np.argmax(self.qvalue[i,:])
            self.behaviour_Pi[i, max_num] = self.epsilon/4 + (1-self.epsilon)

    def get_greedy_policy(self):
        for i in range(16):
            self.Greedy_Pi[i,:] = 0
            max_num = np.argmax(self.qvalue[i,:])
            self.Greedy_Pi[i, max_num] = 1
        return self.Greedy_Pi

    def Off_MC_learning(self):
        num = 0
        self.update_target_policy()
        self.update_behaviour_policy()
        
        while num < 6000:
            num += 1
            flag = False
            state_traj = []
            action_traj = []
            reward_traj = []
            g = 0
            W = 1
            episode_num = 0
            
            # é‡ç½®ç¯å¢ƒ
            self.cur_state, _ = self.env.reset()
            
            while not flag and episode_num < 200:
                cur_action = self.sample_action(self.cur_state)
                state_traj.append(self.cur_state)
                action_traj.append(cur_action)
                
                next_state, reward, flag = self.step(cur_action)
                reward_traj.append(reward)
                self.cur_state = next_state
                episode_num += 1

            # æ›´æ–°è¡Œä¸ºå€¼å‡½æ•°
            for i in reversed(range(len(state_traj))):
                self.C[state_traj[i], action_traj[i]] += W
                g *= self.gamma
                g += reward_traj[i]
                self.qvalue[state_traj[i], action_traj[i]] = self.qvalue[state_traj[i], action_traj[i]] + \
                    (W/self.C[state_traj[i], action_traj[i]]) * (g-self.qvalue[state_traj[i], action_traj[i]])
                W = W * self.target_Pi[state_traj[i], action_traj[i]] / self.behaviour_Pi[state_traj[i], action_traj[i]]

            if num % 501 == 0:
                self.epsilon = self.epsilon * 0.99
                self.update_target_policy()
                self.update_behaviour_policy()
                self.C = np.zeros((16, 4))

def visualize_policy(policy, env):
    # è·å–ç¯å¢ƒå¸ƒå±€æè¿°
    desc = env.unwrapped.desc  # 4x4çš„å­—ç¬¦çŸ©é˜µ
    
    # åŠ¨ä½œåˆ°ç®­å¤´çš„æ˜ å°„
    arrow_map = {
        0: 'â†',  # å·¦
        1: 'â†“',  # ä¸‹
        2: 'â†’',  # å³
        3: 'â†‘'   # ä¸Š
    }
    
    # æ„å»ºå¯è§†åŒ–çŸ©é˜µ
    grid = []
    for row in range(4):
        current_row = []
        for col in range(4):
            state = row * 4 + col
            cell_char = desc[row, col].decode('utf-8')
            
            # å¤„ç†ç‰¹æ®Šå•å…ƒæ ¼
            if cell_char == 'H':
                current_row.append('â›³')  # å†°æ´
            elif cell_char == 'G':
                current_row.append('ğŸ¯')  # ç›®æ ‡
            else:
                # è·å–è¯¥çŠ¶æ€çš„æœ€ä¼˜åŠ¨ä½œ
                action = np.argmax(policy[state])
                current_row.append(arrow_map[action])
        grid.append(current_row)
        # æ‰“å°å¯è§†åŒ–ç»“æœ
    print("\næœ€ä¼˜ç­–ç•¥å¯è§†åŒ–ï¼š")
    print("+" + "-"*13 + "+")
    for i, row in enumerate(grid):
        print("| " + " | ".join(row) + " |")
        if i < 3: 
            print("|" + "----+---+---+----" + "|")
    print("+" + "-"*13 + "+")

def visualize_policy_matrix(policy, env):
    import matplotlib.pyplot as plt
    import numpy as np
    
    # è·å–ç¯å¢ƒå¸ƒå±€
    desc = env.unwrapped.desc
    
    # åˆ›å»ºå›¾å½¢
    fig, ax = plt.subplots(figsize=(8, 8))
    
    # è®¾ç½®ç½‘æ ¼
    ax.grid(True)
    ax.set_xticks(np.arange(-0.5, 4.5, 1))
    ax.set_yticks(np.arange(-0.5, 4.5, 1))
    
    # éšè—åˆ»åº¦æ ‡ç­¾
    ax.set_xticklabels([])
    ax.set_yticklabels([])
    
    # åŠ¨ä½œåˆ°ç®­å¤´çš„æ˜ å°„
    arrow_symbols = {
        0: 'â†',  # å·¦
        1: 'â†“',  # ä¸‹
        2: 'â†’',  # å³
        3: 'â†‘'   # ä¸Š
    }
    
    # å¡«å……é¢œè‰²å’Œç®­å¤´
    for i in range(4):
        for j in range(4):
            state = i * 4 + j
            cell = desc[i][j].decode('utf-8')
            
            # è®¾ç½®å•å…ƒæ ¼é¢œè‰²
            if cell == 'H':
                color = '#ff6666'  # æµ…çº¢è‰²è¡¨ç¤ºæ´
                plt.fill([j-0.5, j+0.5, j+0.5, j-0.5], 
                        [3.5-i, 3.5-i, 4.5-i, 4.5-i], 
                        color=color)
            elif cell == 'G':
                color = '#ccffcc'  # æµ…ç»¿è‰²è¡¨ç¤ºç›®æ ‡
                plt.fill([j-0.5, j+0.5, j+0.5, j-0.5], 
                        [3.5-i, 3.5-i, 4.5-i, 4.5-i], 
                        color=color)
            else:
                color = '#ffffff'  # ç™½è‰²è¡¨ç¤ºå¯ç§»åŠ¨æ ¼å­
                plt.fill([j-0.5, j+0.5, j+0.5, j-0.5], 
                        [3.5-i, 3.5-i, 4.5-i, 4.5-i], 
                        color=color)
            
            # æ·»åŠ ç®­å¤´

            action = np.argmax(policy[state])
            plt.text(j, 4-i, arrow_symbols[action], 
                    ha='center', va='center', fontsize=20)
    
    plt.title('Policy Visualization')
    plt.tight_layout()
    plt.show()

def test_policy(policy, num_episodes=1000):
    # åˆ›å»ºæ— æ¸²æŸ“çš„æµ‹è¯•ç¯å¢ƒï¼ˆå‚æ•°ä¸è®­ç»ƒç¯å¢ƒä¿æŒä¸€è‡´ï¼‰
    test_env = gym.make('FrozenLake-v1', 
                      is_slippery=True,
                      render_mode=None).unwrapped
    success_count = 0
    
    for _ in range(num_episodes):
        state, _ = test_env.reset()
        done = False
        
        while not done:
            action = np.argmax(policy[state])
            next_state, reward, done, truncated, _ = test_env.step(action)
            state = next_state
            
            # åˆ¤æ–­æ˜¯å¦æˆåŠŸåˆ°è¾¾ç›®æ ‡
            if done and reward == 1:
                success_count += 1
                break
    test_env.close()
    return success_count / num_episodes
def plot_heatmaps(qvalue, visit_counts):
    import matplotlib.pyplot as plt
    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))
    
    # Qå€¼çƒ­åŠ›å›¾
    sns.heatmap(np.max(qvalue, axis=1).reshape(4, 4), annot=True, fmt=".2f", cmap="YlGnBu", ax=axes[0])
    axes[0].set_title("State Value Heatmap (Max Q)")
    axes[0].set_xticks([])
    axes[0].set_yticks([])
    
    # è®¿é—®é¢‘æ¬¡çƒ­åŠ›å›¾
    sns.heatmap(np.sum(visit_counts, axis=1).reshape(4, 4), annot=True, fmt=".0f", cmap="YlOrRd", ax=axes[1])
    axes[1].set_title("Visit Frequency Heatmap")
    axes[1].set_xticks([])
    axes[1].set_yticks([])
        
    plt.tight_layout()
    plt.show()

if __name__ == '__main__':
    env = FrozenLakeEnv()
    env.reset()
    start = time.time()
    env.Off_MC_learning()
    end = time.time()
    print("è´ªå©ªç­–ç•¥:\n", env.get_greedy_policy())
    print("ä¼°è®¡å€¼å‡½æ•°ï¼š\n", np.around(env.qvalue, 2))
    print("è®¿é—®é¢‘æ¬¡ï¼š\n", np.around(env.C, 1))
    print("è¿è¡Œæ—¶é—´ï¼š", end-start, "s")
    # æµ‹è¯•æˆåŠŸç‡ä»£ç 
    success_rate = test_policy(env.get_greedy_policy(), num_episodes=1000)
    print(f"\næœ€ä¼˜ç­–ç•¥æˆåŠŸç‡ç»Ÿè®¡: {success_rate*100:.2f}% (1000æ¬¡å°è¯•)")
    # æ·»åŠ å¯è§†åŒ–
    env2 = gym.make('FrozenLake-v1', is_slippery=True)
    visualize_policy_matrix(env.get_greedy_policy(), env2)
    plot_heatmaps(env.qvalue, env.C)