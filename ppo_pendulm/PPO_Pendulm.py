import numpy as np
import gym
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torch.optim import Adam
from torch.distributions.normal import Normal
import os
import time
from collections import deque


class Sample:
    def __init__(self, env):
        self.env = env
        self.gamma = 0.99  # æŠ˜æ‰£å› å­
        self.lamda = 0.95  # GAEå‚æ•°
        self.batch_state = None
        self.batch_act = None
        self.batch_logp = None
        self.batch_adv = None
        self.batch_val_target = None
        self.index = None
        self.sum_return = 0
        
    def sample_one_episode(self, actor_net, critic_net):
        """é‡‡æ ·ä¸€æ¡è½¨è¿¹"""
        episode_obs = []
        episode_actions = []
        episode_logps = []
        episode_rewards = []
        episode_vals = []
        cur_obs, _ = self.env.reset()
        done = False
        episode_sum = 0
        while not done:
            episode_obs.append(cur_obs)
            obs_tensor = torch.as_tensor(cur_obs, dtype=torch.float32)
            # é‡‡æ ·è¿ç»­åŠ¨ä½œå’Œlogæ¦‚ç‡
            action, logp = actor_net.get_a(obs_tensor)
            value = critic_net.get_v(obs_tensor)
            episode_actions.append(action)
            episode_logps.append(logp)
            episode_vals.append(value)
            # ç¯å¢ƒäº¤äº’
            next_obs, reward, terminated, truncated, _ = self.env.step(action)
            done = terminated or truncated
            cur_obs = next_obs
            episode_rewards.append(reward)
            episode_sum += reward
        
        # GAEä¼˜åŠ¿è®¡ç®—
        vals = episode_vals + [0]  # æœ«çŠ¶æ€ä»·å€¼è®¾ä¸º0
        episode_adv = []
        adv = 0
        
        for t in reversed(range(len(episode_rewards))):
            delta = episode_rewards[t] + self.gamma * vals[t+1] - vals[t]
            adv = delta + self.gamma * self.lamda * adv
            episode_adv.insert(0, adv)
        
        # ç›®æ ‡å€¼å‡½æ•°ï¼ˆç´¯è®¡æŠ˜æ‰£å›æŠ¥ï¼‰
        val_target = []
        ret = 0
        for r in reversed(episode_rewards):
            ret = r + self.gamma * ret
            val_target.insert(0, ret)
            
        return (np.array(episode_obs), np.array(episode_actions),
                np.array(episode_logps).reshape(-1, 1), np.array(episode_adv).reshape(-1, 1),
                np.array(val_target).reshape(-1, 1), episode_sum)
    
    def sample_many_episodes(self, actor_net, critic_net, num):
        """é‡‡æ ·å¤šæ¡è½¨è¿¹"""
        self.sum_return = 0
        
        # é‡‡é›†ç¬¬ä¸€æ¡è½¨è¿¹
        batch_state, batch_act, batch_logp, batch_adv, batch_val_target, episode_sum = self.sample_one_episode(actor_net, critic_net)
        self.sum_return += episode_sum
        
        # é‡‡é›†å‰©ä½™è½¨è¿¹
        for i in range(num - 1):
            episode_state, episode_act, episode_logp, episode_adv, episode_val_target, episode_sum = self.sample_one_episode(actor_net, critic_net)
            batch_state = np.concatenate((batch_state, episode_state), 0)
            batch_act = np.concatenate((batch_act, episode_act), 0)
            batch_logp = np.concatenate((batch_logp, episode_logp), 0)
            batch_adv = np.concatenate((batch_adv, episode_adv), 0)
            batch_val_target = np.concatenate((batch_val_target, episode_val_target), 0)
            self.sum_return += episode_sum
            
        self.batch_state = batch_state
        self.batch_act = batch_act
        self.batch_logp = batch_logp
        self.batch_adv = batch_adv
        self.batch_val_target = batch_val_target
    
    def get_data(self, start_index, sgd_num):
        """è·å–mini-batchæ•°æ®"""
        idx = self.index[start_index:start_index+sgd_num]
        sgd_batch_state = self.batch_state[idx]
        sgd_batch_act = self.batch_act[idx]
        sgd_batch_logp = self.batch_logp[idx]
        sgd_batch_adv = self.batch_adv[idx]
        sgd_batch_val_target = self.batch_val_target[idx]
        
        # å½’ä¸€åŒ–ä¼˜åŠ¿
        if sgd_batch_adv.std() > 1e-8:
            sgd_batch_adv = (sgd_batch_adv - sgd_batch_adv.mean()) / (sgd_batch_adv.std() + 1e-8)
        else:
            sgd_batch_adv = sgd_batch_adv - sgd_batch_adv.mean()
            
        # è½¬æ¢ä¸ºtensor
        sgd_batch_state = torch.as_tensor(sgd_batch_state, dtype=torch.float32)
        sgd_batch_act = torch.as_tensor(sgd_batch_act, dtype=torch.float32)  
        sgd_batch_logp = torch.as_tensor(sgd_batch_logp, dtype=torch.float32)
        sgd_batch_adv = torch.as_tensor(sgd_batch_adv, dtype=torch.float32)
        sgd_batch_val_target = torch.as_tensor(sgd_batch_val_target, dtype=torch.float32)
        
        return sgd_batch_state, sgd_batch_act, sgd_batch_logp, sgd_batch_adv, sgd_batch_val_target


def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
    """ç¥ç»ç½‘ç»œå±‚åˆå§‹åŒ–"""
    torch.nn.init.orthogonal_(layer.weight, std)
    torch.nn.init.constant_(layer.bias, bias_const)
    return layer


class Actor_Net(nn.Module):
    def __init__(self, obs_dim, act_dim, hidden_sizes, action_bounds=None):
        super(Actor_Net, self).__init__()
        self.act_dim = act_dim
        self.action_bounds = action_bounds  # åŠ¨ä½œèŒƒå›´
        
        # å…±äº«ç‰¹å¾æå–ç½‘ç»œ
        self.shared = nn.Sequential(
            layer_init(nn.Linear(obs_dim, hidden_sizes[0])),
            nn.ReLU(),
            layer_init(nn.Linear(hidden_sizes[0], hidden_sizes[1])),
            nn.ReLU(),
            layer_init(nn.Linear(hidden_sizes[1], act_dim), std=0.01),
            nn.Tanh()
        )
        
        
        # æ ‡å‡†å·®ç½‘ç»œ
        self.log_std = nn.Parameter(torch.zeros(act_dim))
        
    def forward(self, obs, act=None):
        mu = self.shared(obs)
        
        # åº”ç”¨åŠ¨ä½œè¾¹ç•Œ
        if self.action_bounds is not None:
            # ç¼©æ”¾åŠ¨ä½œåˆ°è¾¹ç•ŒèŒƒå›´
            action_low, action_high = self.action_bounds
            mu = torch.tanh(mu) * 0.5 * (action_high - action_low) + 0.5 * (action_high + action_low)
        
        # è·å–æ ‡å‡†å·®å¹¶é™åˆ¶èŒƒå›´
        std = torch.exp(self.log_std.clamp(-20, 2))
        
        # åˆ›å»ºæ­£æ€åˆ†å¸ƒ
        dist = Normal(mu, std)
        
        if act is not None:
            logp = dist.log_prob(act).sum(dim=-1)  # è¿ç»­åŠ¨ä½œæ¯ä¸ªç»´åº¦çš„log_probæ±‚å’Œ
        else:
            logp = None
            
        entropy = dist.entropy().sum(dim=-1) if self.act_dim > 1 else dist.entropy()
        return dist, logp, entropy
    
    def get_a(self, obs):
        """è·å–è¿ç»­åŠ¨ä½œå’Œlogæ¦‚ç‡"""
        with torch.no_grad():
            dist, _, _ = self.forward(obs)
            action = dist.sample()
            logp = dist.log_prob(action).sum(dim=-1)
        return action.detach().numpy(), logp.item()


class Critic_Net(nn.Module):
    def __init__(self, obs_dim, hidden_sizes):
        super(Critic_Net, self).__init__()
        # é’ˆå¯¹LunarLanderContinuousä¼˜åŒ–ï¼šä½¿ç”¨Tanhæ¿€æ´»å‡½æ•°
        self.critic_net = nn.Sequential(
            layer_init(nn.Linear(obs_dim, hidden_sizes[0])),
            nn.Tanh(),
            layer_init(nn.Linear(hidden_sizes[0], hidden_sizes[1])),
            nn.Tanh(),
            layer_init(nn.Linear(hidden_sizes[1], 1), std=1.0)
        )
        
    def forward(self, obs):
        return self.critic_net(obs).squeeze()
    
    def get_v(self, obs):
        """è·å–çŠ¶æ€ä»·å€¼"""
        with torch.no_grad():
            return self.forward(obs).item()


class PPO:
    def __init__(self, env, hidden_sizes, pi_lr, critic_lr):
        self.env = env
        self.obs_dim = env.observation_space.shape[0]
        self.act_dim = env.action_space.shape[0]  # è¿ç»­åŠ¨ä½œç©ºé—´ç»´åº¦
        
        # è·å–åŠ¨ä½œèŒƒå›´å¹¶è½¬æ¢ä¸ºå¼ é‡
        self.action_bounds = [
            torch.as_tensor(env.action_space.low, dtype=torch.float32),
            torch.as_tensor(env.action_space.high, dtype=torch.float32)
        ]

        # ç½‘ç»œåˆå§‹åŒ–
        self.actor = Actor_Net(self.obs_dim, self.act_dim, hidden_sizes, self.action_bounds)
        self.critic = Critic_Net(self.obs_dim, hidden_sizes)
        # ä¼˜åŒ–å™¨
        self.pi_optimizer = Adam(self.actor.parameters(), lr=pi_lr)
        self.critic_optimizer = Adam(self.critic.parameters(), lr=critic_lr)
        # PPOè¶…å‚æ•°
        self.clip_ratio = 0.2
        self.epochs = 4000
        self.episodes_num = 10  # æ¯æ¬¡æ›´æ–°é‡‡é›†çš„è½¨è¿¹æ•°
        self.train_pi_iters = 80
        self.sgd_num = 64  # æ‰¹å¤§å°
        self.save_freq = 10
        # è®­ç»ƒç›‘æ§
        self.sampler = Sample(env)
        self.return_traj = []
        self.policy_losses = []
        self.entropy_losses = []
        self.critic_losses = []
        self.kl_divergences = []
        self.recent_rewards = deque(maxlen=100)  # ç”¨äºè®¡ç®—ç§»åŠ¨å¹³å‡
        # æ¨¡å‹ä¿å­˜
        self.training_path = r'D:\\code\\resource_RL\\ppo_pendulm\\models'
        os.makedirs(self.training_path, exist_ok=True)
        self.best_return = -float('inf')
        self.best_model_path = None
        # æ—©åœæœºåˆ¶
        self.early_stop_kl = 0.02  # KLæ•£åº¦é˜ˆå€¼
        self.success_threshold = 200  # æ¨¡å‹è®­ç»ƒæˆåŠŸæ ‡å‡†
        
    def compute_loss_pi(self, obs, act, logp_old, adv):
        """è®¡ç®—ç­–ç•¥æŸå¤±"""
        dist, logp, entropy = self.actor(obs, act)
        ratio = torch.exp(logp - logp_old.squeeze())
        
        # PPO-clipæŸå¤±
        clip_adv = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * adv.squeeze()
        loss_pi = -torch.min(ratio * adv.squeeze(), clip_adv).mean()
        
        # ç†µæ­£åˆ™åŒ–
        loss_entropy = entropy.mean()
        
        # KLæ•£åº¦ä¼°è®¡
        approx_kl = (logp_old.squeeze() - logp).mean().item()
        
        return loss_pi, loss_entropy, approx_kl
    
    def compute_loss_critic(self, obs, val_target):
        """è®¡ç®—ä»·å€¼å‡½æ•°æŸå¤±"""
        return ((self.critic(obs) - val_target.squeeze()) ** 2).mean()
    
    def update(self):
        """PPOæ›´æ–°æ­¥éª¤"""
        # é‡‡é›†æ•°æ®
        self.sampler.sample_many_episodes(self.actor, self.critic, self.episodes_num)
        avg_return = self.sampler.sum_return / self.episodes_num
        self.return_traj.append(avg_return)
        self.recent_rewards.append(avg_return)
        
        batch_size = self.sampler.batch_state.shape[0]
        self.sampler.index = np.arange(batch_size)
        
        # ç´¯ç§¯æŸå¤±ç”¨äºç›‘æ§
        sum_pi_loss = 0.0
        sum_entropy_loss = 0.0
        sum_critic_loss = 0.0
        sum_kl = 0.0
        update_count = 0
        
        # è®­ç»ƒå¾ªç¯
        for i in range(self.train_pi_iters):
            np.random.shuffle(self.sampler.index)
            
            for start_index in range(0, batch_size - self.sgd_num, self.sgd_num):
                batch_state, batch_act, batch_logp, batch_adv, batch_val_target = self.sampler.get_data(start_index, self.sgd_num)
                
                # è®­ç»ƒç­–ç•¥ç½‘ç»œ
                self.pi_optimizer.zero_grad()
                loss_pi, loss_entropy, approx_kl = self.compute_loss_pi(batch_state, batch_act, batch_logp, batch_adv)
                total_pi_loss = loss_pi - 0.01 * loss_entropy 
                total_pi_loss.backward()
                nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
                self.pi_optimizer.step()
                
                # è®­ç»ƒä»·å€¼ç½‘ç»œ
                self.critic_optimizer.zero_grad()
                loss_critic = self.compute_loss_critic(batch_state, batch_val_target)
                loss_critic.backward()
                nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
                self.critic_optimizer.step()
                
                # ç´¯ç§¯ç»Ÿè®¡
                sum_pi_loss += loss_pi.item()
                sum_entropy_loss += loss_entropy.item()
                sum_critic_loss += loss_critic.item()
                sum_kl += approx_kl
                update_count += 1
                
                # æ—©åœæ£€æŸ¥
                if approx_kl > self.early_stop_kl:
                    print(f"KLæ•£åº¦è¿‡å¤§({approx_kl:.4f})ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                    break
            
            if sum_kl / max(update_count, 1) > self.early_stop_kl:
                break
        
        # è®°å½•å¹³å‡æŸå¤±
        if update_count > 0:
            self.policy_losses.append(sum_pi_loss / update_count)
            self.entropy_losses.append(sum_entropy_loss / update_count)
            self.critic_losses.append(sum_critic_loss / update_count)
            self.kl_divergences.append(sum_kl / update_count)
        
        moving_avg = np.mean(self.recent_rewards) if self.recent_rewards else avg_return
        print(f"å¹³å‡å›æŠ¥: {avg_return:.2f}, è¿‘100è½®å¹³å‡: {moving_avg:.2f}")
        
        # æ£€æŸ¥æ˜¯å¦è¾¾æˆåŠŸ
        if moving_avg >= self.success_threshold:
            print(f"ğŸ‰ è¾¾åˆ°æˆåŠŸæ ‡å‡†ï¼è¿‘100è½®å¹³å‡å›æŠ¥: {moving_avg:.2f}")
    
    def ppo_train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        start_time = time.time()
        success_count = 0
        
        print("å¼€å§‹LunarLanderContinuous PPOè®­ç»ƒ...")
        print(f"ç›®æ ‡ï¼šå›æŠ¥è¾¾åˆ° {self.success_threshold}")
        
        for epoch in range(self.epochs):
            print(f"\n=== è®­ç»ƒè½®æ¬¡ {epoch+1}/{self.epochs} ===")
            self.update()
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            current_return = self.return_traj[-1]
            if current_return > self.best_return:
                self.best_return = current_return
                best_model_path = os.path.join(self.training_path, 'best_actor.pth')
                best_critic_path = os.path.join(self.training_path, 'best_critic.pth')
                torch.save(self.actor.state_dict(), best_model_path)
                torch.save(self.critic.state_dict(), best_critic_path)
                self.best_model_path = best_model_path
                print(f"ğŸ’¾ ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹ (å›æŠ¥: {self.best_return:.2f})")
            
            # å®šæœŸä¿å­˜
            if (epoch + 1) % self.save_freq == 0:
                torch.save(self.actor.state_dict(), os.path.join(self.training_path, f'{epoch + 1}_actor.pth'))
                torch.save(self.critic.state_dict(), os.path.join(self.training_path, f'{epoch + 1}_critic.pth'))
                print(f"ğŸ’¾ ä¿å­˜ç¬¬{epoch + 1}è½®æ¨¡å‹")
        
        end_time = time.time()
        training_duration = end_time - start_time
        print(f"\nè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_duration:.2f}ç§’")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves()
    
    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        plt.rcParams['font.sans-serif'] = ['SimHei']  # è®¾ç½®ä¸­æ–‡é»‘ä½“
        plt.rcParams['axes.unicode_minus'] = False    # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # å›æŠ¥æ›²çº¿
        axes[0, 0].plot(self.return_traj, 'b-', alpha=0.7)
        if len(self.return_traj) >= 20:
            # è®¡ç®—ç§»åŠ¨å¹³å‡
            window = min(50, len(self.return_traj)//10)
            moving_avg = np.convolve(self.return_traj, np.ones(window)/window, mode='valid')
            axes[0, 0].plot(range(window-1, len(self.return_traj)), moving_avg, 'r-', linewidth=2, label=f'ç§»åŠ¨å¹³å‡({window})')
        axes[0, 0].axhline(y=self.success_threshold, color='g', linestyle='--', label=f'æˆåŠŸçº¿: {self.success_threshold}')
        axes[0, 0].axhline(y=self.best_return, color='orange', linestyle='--', label=f'æœ€ä½³: {self.best_return:.2f}')
        axes[0, 0].set_title('è®­ç»ƒå›æŠ¥æ›²çº¿')
        axes[0, 0].set_xlabel('è½®æ¬¡')
        axes[0, 0].set_ylabel('å¹³å‡å›æŠ¥')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # ç­–ç•¥æŸå¤±
        if self.policy_losses:
            axes[0, 1].plot(self.policy_losses, 'r-')
            axes[0, 1].set_title('ç­–ç•¥æŸå¤±')
            axes[0, 1].set_xlabel('è½®æ¬¡')
            axes[0, 1].set_ylabel('æŸå¤±')
            axes[0, 1].grid(True, alpha=0.3)
        
        # ä»·å€¼æŸå¤±
        if self.critic_losses:
            axes[1, 0].plot(self.critic_losses, 'b-')
            axes[1, 0].set_title('ä»·å€¼ç½‘ç»œæŸå¤±')
            axes[1, 0].set_xlabel('è½®æ¬¡')
            axes[1, 0].set_ylabel('æŸå¤±')
            axes[1, 0].grid(True, alpha=0.3)
        
        # KLæ•£åº¦
        if self.kl_divergences:
            axes[1, 1].plot(self.kl_divergences, 'g-')
            axes[1, 1].axhline(y=self.early_stop_kl, color='r', linestyle='--', label=f'æ—©åœçº¿: {self.early_stop_kl}')
            axes[1, 1].set_title('KLæ•£åº¦')
            axes[1, 1].set_xlabel('è½®æ¬¡')
            axes[1, 1].set_ylabel('KLæ•£åº¦')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.training_path, 'training_curves.png'), dpi=300)
        plt.show()
    
    def load_model(self, training_path, actor_filename, critic_filename):
        """åŠ è½½æ¨¡å‹"""
        self.actor.load_state_dict(torch.load(os.path.join(training_path, actor_filename)))
        self.critic.load_state_dict(torch.load(os.path.join(training_path, critic_filename)))
        print(f"æ¨¡å‹åŠ è½½å®Œæˆ: {actor_filename}, {critic_filename}")


if __name__ == '__main__':
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make('LunarLanderContinuous-v2')
    # åˆ›å»ºPPOæ™ºèƒ½ä½“
    lunarlander_ppo = PPO(env, 
                         hidden_sizes=[64, 64], 
                         pi_lr=3e-4,             # ç­–ç•¥å­¦ä¹ ç‡
                         critic_lr=1e-3)         # ä»·å€¼ç½‘ç»œå­¦ä¹ ç‡
    # å¼€å§‹è®­ç»ƒ
    lunarlander_ppo.ppo_train()
    
    # ç¯å¢ƒæ¸…ç†
    env.close()
    print("\nè®­ç»ƒå®Œæˆï¼")
