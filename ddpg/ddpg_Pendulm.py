import os
import numpy as np
import gym
import random
import matplotlib.pyplot as plt
RENDER = False
import torch
import torch.nn as nn
from torch.optim import Adam
from torch.distributions.normal import Normal
import os
loss_history = []
#åˆå§‹åŒ–ç¥ç»ç½‘ç»œå±‚æƒé‡
def layer_init(layer,std = np.sqrt(2),bias_const=0.0):
  torch.nn.init.orthogonal_(layer.weight,std)
  torch.nn.init.constant_(layer.bias, bias_const)
  return layer
class Experience_Buffer():
    def __init__(self, buffer_size=40000):
      self.buffer = []
      self.buffer_size = buffer_size
      # å®šä¹‰çŠ¶æ€å’ŒåŠ¨ä½œç»´åº¦
      self.obs_dim = 8  # LunarLanderContinuous-v2 çŠ¶æ€ç»´åº¦
      self.act_dim = 2  # LunarLanderContinuous-v2 åŠ¨ä½œç»´åº¦
    
    def add_experience(self, state, action, reward, next_state, done):
      """æ·»åŠ ä¸€æ¡ç»éªŒåˆ°ç¼“å†²åŒºï¼ŒåŒ…å«ç»ˆæ­¢ä¿¡å·done"""
      experience = {
          'state': state,
          'action': action,
          'reward': reward,
          'next_state': next_state,
          'done': done  # æ–°å¢ç»ˆæ­¢ä¿¡å·
      }
      
      # ç§»é™¤æ—§æ ·æœ¬ä»¥ä¿æŒç¼“å†²åŒºå¤§å°
      if len(self.buffer) >= self.buffer_size:
          self.buffer.pop(0)
          
      self.buffer.append(experience)
    
    def sample(self, batch_size):
      """ä»ç¼“å†²åŒºä¸­é‡‡æ ·ä¸€æ‰¹ç»éªŒ"""
      # ç¡®ä¿ç¼“å†²åŒºæœ‰è¶³å¤Ÿæ•°æ®
      batch_size = min(batch_size, len(self.buffer))
      
      # éšæœºé‡‡æ ·ç´¢å¼•
      indices = np.random.choice(len(self.buffer), batch_size, replace=False)
      
      # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
      states = np.zeros((batch_size, self.obs_dim))
      actions = np.zeros((batch_size, self.act_dim))
      rewards = np.zeros((batch_size, 1))
      next_states = np.zeros((batch_size, self.obs_dim))
      dones = np.zeros((batch_size, 1))  # æ–°å¢ç»ˆæ­¢ä¿¡å·æ•°ç»„
      
      # å¡«å……æ•°æ®
      for i, idx in enumerate(indices):
          experience = self.buffer[idx]
          states[i] = experience['state']
          actions[i] = experience['action']
          rewards[i] = experience['reward']
          next_states[i] = experience['next_state']
          dones[i] = float(experience['done'])  # è½¬æ¢ä¸ºfloatç±»å‹
      
      # è½¬æ¢ä¸ºPyTorchå¼ é‡
      states = torch.as_tensor(states, dtype=torch.float32)
      actions = torch.as_tensor(actions, dtype=torch.float32)
      rewards = torch.as_tensor(rewards, dtype=torch.float32)
      next_states = torch.as_tensor(next_states, dtype=torch.float32)
      dones = torch.as_tensor(dones, dtype=torch.float32)  # è½¬æ¢ä¸ºå¼ é‡
      
      return states, actions, rewards, next_states, dones
#ç­–ç•¥ç½‘ç»œç±»ï¼Œæ„å»ºç­–ç•¥ï¼Œå¹¶è¿›è¡Œé‡‡æ ·
class Actor_Net(nn.Module):
  def __init__(self,obs_dim,act_dim,hidden_sizes,action_bounds=None):
    super(Actor_Net,self).__init__()
    self.act_dim = act_dim
    self.action_bounds = action_bounds
    #å‡å€¼ç½‘ç»œï¼Œåˆ©ç”¨å‰å‘ç¥ç»ç½‘ç»œ
    self.actor_net = nn.Sequential(
      layer_init(nn.Linear(obs_dim,hidden_sizes[0])),
      nn.ReLU(),
      layer_init(nn.Linear(hidden_sizes[0],hidden_sizes[1])),
      nn.ReLU(),
      layer_init(nn.Linear(hidden_sizes[1],act_dim),std=0.01),
      nn.Tanh()
    )
    self.actor_net.requires_grad_()

  def forward(self, obs):
    # è¾“å‡ºåŠ¨ä½œï¼Œå¹¶æ ¹æ®åŠ¨ä½œè¾¹ç•Œè¿›è¡Œç¼©æ”¾
    action_raw = self.actor_net(obs)
    if self.action_bounds is not None:
      action_low, action_high = self.action_bounds
      # ç¡®ä¿åŠ¨ä½œè¾¹ç•Œä¸åŒ…å«æ¢¯åº¦
      action_low = action_low.detach() if isinstance(action_low, torch.Tensor) else torch.tensor(action_low, dtype=torch.float32)
      action_high = action_high.detach() if isinstance(action_high, torch.Tensor) else torch.tensor(action_high, dtype=torch.float32)
      # åº”ç”¨åŠ¨ä½œèŒƒå›´
      action = action_raw * 0.5 * (action_high - action_low) + 0.5 * (action_high + action_low)
    else:
      action = action_raw
    return action
    
  #é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œï¼Œä¸è®¡ç®—æ¢¯åº¦,äº§ç”Ÿæ•°æ®æ ·æœ¬æ—¶ä½¿ç”¨
  def get_a(self,obs):
    with torch.no_grad():
      action = self.forward(obs)
    return action
#å€¼å‡½æ•°ç½‘ç»œç±»ï¼Œæ„å»ºå€¼å‡½æ•°
class Critic_Net(nn.Module):
  def __init__(self,obs_dim,action_dim,hidden_sizes):
    super(Critic_Net,self).__init__()
    self.layer_1 = layer_init(nn.Linear(obs_dim,hidden_sizes[0]))
    self.layer_2 = layer_init(nn.Linear(action_dim,hidden_sizes[0]))
    self.layer_3 = layer_init(nn.Linear(hidden_sizes[0],hidden_sizes[1]))
    self.output = layer_init(nn.Linear(hidden_sizes[1],1))
  #è¿”å›å€¼å‡½æ•°é¢„æµ‹å€¼ï¼Œè®¡ç®—æ¢¯åº¦ï¼Œè®¡ç®—å€¼å‡½æ•°æŸå¤±æ—¶ä½¿ç”¨
  def forward(self,obs,action):
    q =self.layer_1(obs)+self.layer_2(action)
    q = torch.relu(self.layer_3(q))
    q =self.output(q)
    return q
  #è®¡ç®—å€¼å‡½æ•°çš„æ•°å€¼ï¼Œä¸è®¡ç®—æ¢¯åº¦ï¼Œè®¡ç®—ç›®æ ‡å‡½æ•°æ—¶ä½¿ç”¨
  def get_q(self,obs,action):
    with torch.no_grad():
      q = self.forward(obs,action)
    return q.numpy()
#PPOç®—æ³•ç±»ï¼Œå®ç°ç­–ç•¥çš„æ›´æ–°
class DDPG():
  def __init__(self, env):
    self.env = env
    self.gamma = 0.99
    self.exp_buffer = Experience_Buffer()
    self.obs_dim = env.observation_space.shape[0]  # è·å–è§‚å¯Ÿç©ºé—´ç»´åº¦
    self.act_dim = env.action_space.shape[0]  # è·å–åŠ¨ä½œç©ºé—´ç»´åº¦
    
    # è·å–åŠ¨ä½œèŒƒå›´
    self.action_bounds = [
        env.action_space.low,
        env.action_space.high
    ]
    
    # è°ƒæ•´ç½‘ç»œå¤§å°
    self.hidden = [64, 64]  # æ‰©å¤§ç½‘ç»œå®¹é‡
    self.pi_lr = 3e-4  # æŒ‰PPOçš„å­¦ä¹ ç‡è®¾ç½®
    self.critic_lr = 1e-3
    self.tau = 0.005 
    self.batch_num = 64
    self.epochs = 4000  # è®¾ç½®æ›´å¤šè®­ç»ƒè½®æ¬¡
    self.save_freq = 10
    
    # è®­ç»ƒç›‘æ§
    self.return_traj = []
    self.recent_rewards = []  # è®°å½•æœ€è¿‘çš„å¥–åŠ±
    self.success_threshold = 200  # LunarLanderçš„æˆåŠŸé˜ˆå€¼
    
    # åˆå§‹åŒ–ç½‘ç»œ
    self.actor = Actor_Net(self.obs_dim, self.act_dim, self.hidden, self.action_bounds)
    self.actor_target = Actor_Net(self.obs_dim, self.act_dim, self.hidden, self.action_bounds)
    self.pi_optimizer = Adam(self.actor.parameters(), lr=self.pi_lr)
    
    self.critic = Critic_Net(self.obs_dim, self.act_dim, self.hidden)
    self.critic_target = Critic_Net(self.obs_dim, self.act_dim, self.hidden)
    self.critic_optimizer = Adam(self.critic.parameters(), lr=self.critic_lr)
    
    # ç¡¬æ‹·è´å‚æ•°åˆ°ç›®æ ‡ç½‘ç»œ
    self.hard_update(self.actor_target, self.actor)
    self.hard_update(self.critic_target, self.critic)
    
    # è®¾ç½®æ¨¡å‹ä¿å­˜è·¯å¾„
    self.training_path = 'D:\\code\\resource_RL\\ddpg\\models'
    os.makedirs(self.training_path, exist_ok=True)
    self.actor_filename = 'ddpg_lunar_actor.pth'  # æ›´æ”¹æ–‡ä»¶å
    self.critic_filename = 'ddpg_lunar_critic.pth'
    
    # è·Ÿè¸ªæœ€ä½³æ¨¡å‹
    self.best_return = -float('inf')
    self.best_model_path = None
    
  # è®¡ç®—ç­–ç•¥æŸå¤±å‡½æ•°
  def compute_loss_pi(self, obs):
    act = self.actor(obs)
    q = self.critic(obs,act)
    loss_pi = -torch.mean(q)
    return loss_pi
    
  # è®¡ç®—å€¼æŸå¤±
  def compute_loss_critic(self, obs, act, reward, obs_next, done):
    # è€ƒè™‘ç»ˆæ­¢çŠ¶æ€
    a_next = self.actor_target(obs_next)
    q_target = self.critic_target(obs_next, a_next).detach()
    # å¦‚æœæ˜¯ç»ˆæ­¢çŠ¶æ€ï¼Œä¸‹ä¸€çŠ¶æ€æ²¡æœ‰å›æŠ¥
    backup = reward + self.gamma * (1 - done) * q_target
    return ((self.critic(obs, act) - backup) ** 2).mean()
    
  # é‡‡é›†æ•°æ®å¹¶æ›´æ–°
  def update(self):
    # ä»ç»éªŒå›æ”¾ä¸­é‡‡æ ·æ•°æ®
    batch_state, batch_act, batch_reward, batch_state_next, batch_done = self.exp_buffer.sample(self.batch_num)
      
    # ä¼˜åŒ–è¯„ä»·ç½‘ç»œ - å…ˆæ›´æ–°criticï¼Œä½¿ç”¨æ›´å¥½çš„criticè®¡ç®—policyæŸå¤±
    loss_critic = self.compute_loss_critic(batch_state, batch_act, batch_reward, batch_state_next, batch_done)
    self.critic_optimizer.zero_grad()
    loss_critic.backward()
    # é™åˆ¶æ¢¯åº¦ï¼Œæé«˜ç¨³å®šæ€§
    nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)
    self.critic_optimizer.step()
    
    # ä¼˜åŒ–ç­–ç•¥å‚æ•°
    loss_pi = self.compute_loss_pi(batch_state)
    self.pi_optimizer.zero_grad()
    loss_pi.backward()
    # é™åˆ¶æ¢¯åº¦ï¼Œæé«˜ç¨³å®šæ€§
    nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
    self.pi_optimizer.step()
    
    return loss_pi.item(), loss_critic.item()
    
  # è½¯æ›´æ–°
  def soft_update(self, target_net, eva_net, tau):
    for target_param, param in zip(target_net.parameters(), eva_net.parameters()):
      target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
      
  # ç¡¬æ›´æ–°
  def hard_update(self, target_net, eva_net):
    for target_param, param in zip(target_net.parameters(), eva_net.parameters()):
      target_param.data.copy_(param.data)
      
  # ddpgè®­ç»ƒç®—æ³•
  def ddpg_train(self, epochs=None):
    if epochs:
      self.epochs = epochs
      
    print("å¼€å§‹LunarLanderContinuous-v2 DDPGè®­ç»ƒ...")
    print(f"ç›®æ ‡ï¼šå›æŠ¥è¾¾åˆ° {self.success_threshold}")
      
    # è®°å½•è®­ç»ƒæŸå¤±
    pi_losses = []
    critic_losses = []
    
    # è®¾ç½®Ornstein-Uhlenbeckå™ªå£°è¿›ç¨‹ï¼ˆæ›´é€‚åˆè¿ç»­æ§åˆ¶ä»»åŠ¡ï¼‰
    class OUNoise:
      def __init__(self, size, mu=0.0, theta=0.15, sigma=0.2):
        self.mu = mu * np.ones(size)
        self.theta = theta
        self.sigma = sigma
        self.reset()
          
      def reset(self):
        self.state = np.copy(self.mu)
          
      def sample(self):
        x = self.state
        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))
        self.state = x + dx
        return self.state
    
    # åˆå§‹åŒ–å™ªå£°å¯¹è±¡
    noise = OUNoise(self.act_dim)
    
    # åˆå§‹åŒ–å­¦ä¹ è¿›åº¦æ§åˆ¶
    update_counter = 0
    target_update_freq = 10 
    
    for epoch in range(self.epochs):
      observation, _ = self.env.reset()
      noise.reset()  # é‡ç½®å™ªå£°
      episode_return = 0
      done = False
      steps = 0
      
      # åŠ¨æ€å™ªå£°è¡°å‡
      noise_scale = max(0.005, 0.3 * np.exp(-epoch / 200))
      
      # è®°å½•æ¯è½®çš„æŸå¤±
      epoch_pi_losses = []
      epoch_critic_losses = []
      
      while not done:
        # è·å–åŠ¨ä½œ
        state = observation.reshape(1, -1)
        action = self.actor(torch.as_tensor(state, dtype=torch.float32))
        action = action.detach().numpy()
        
        # åº”ç”¨OUå™ªå£°ä»£æ›¿ç®€å•çš„é«˜æ–¯å™ªå£°
        action = action + noise_scale * noise.sample()
        
        # è£å‰ªåŠ¨ä½œåˆ°åˆæ³•èŒƒå›´
        action = np.clip(action, self.action_bounds[0], self.action_bounds[1])
        
        # ç¯å¢ƒäº¤äº’
        observation_next, reward, terminated, truncated, _ = self.env.step(action[0])
        done = terminated or truncated
        # ç´¯ç§¯å›æŠ¥
        episode_return += reward
        steps += 1
        
        # å­˜å‚¨ç»éªŒ
        self.exp_buffer.add_experience(observation, action[0], reward, observation_next, done)
        
        # å½“ç»éªŒç¼“å†²åŒºè¶³å¤Ÿå¤§æ—¶å¼€å§‹æ›´æ–°
        if len(self.exp_buffer.buffer) > self.batch_num * 5:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®ä½†ä¸è¦ç­‰å¾…å¤ªé•¿
          # å¤šæ¬¡æ›´æ–°ï¼Œæé«˜æ ·æœ¬æ•ˆç‡
          for _ in range(1):  # æ¯æ”¶é›†1ä¸ªæ ·æœ¬æ›´æ–°1æ¬¡ç½‘ç»œ
            pi_loss, critic_loss = self.update()
            epoch_pi_losses.append(pi_loss)
            epoch_critic_losses.append(critic_loss)
            update_counter += 1
            
            # å‘¨æœŸæ€§æ›´æ–°ç›®æ ‡ç½‘ç»œï¼Œé¿å…é¢‘ç¹æ›´æ–°
            if update_counter % target_update_freq == 0:
              self.soft_update(self.actor_target, self.actor, self.tau)
              self.soft_update(self.critic_target, self.critic, self.tau)
          
        observation = observation_next
        
        # é¿å…è¿‡é•¿çš„episode
        if steps >= 1000:  # LunarLanderé»˜è®¤ä¸Šé™
          break
        
      # è®°å½•å›æŠ¥
      self.return_traj.append(episode_return)
      self.recent_rewards.append(episode_return)
      if len(self.recent_rewards) > 100:  # ä¿æŒ100ä¸ªæœ€è¿‘å›æŠ¥
        self.recent_rewards.pop(0)
      
      # è®¡ç®—ç§»åŠ¨å¹³å‡
      avg_return = np.mean(self.recent_rewards[-min(len(self.recent_rewards), 100):])
      
      # è®°å½•å¹³å‡æŸå¤±
      if epoch_pi_losses:
        avg_pi_loss = np.mean(epoch_pi_losses)
        avg_critic_loss = np.mean(epoch_critic_losses)
        pi_losses.append(avg_pi_loss)
        critic_losses.append(avg_critic_loss)
        print(f"è®­ç»ƒè½®æ¬¡: {epoch+1}/{self.epochs}, å›æŠ¥: {episode_return:.2f}, å¹³å‡å›æŠ¥: {avg_return:.2f}, Ï€æŸå¤±: {avg_pi_loss:.4f}, QæŸå¤±: {avg_critic_loss:.4f}, æ­¥æ•°: {steps}")
      else:
        print(f"è®­ç»ƒè½®æ¬¡: {epoch+1}/{self.epochs}, å›æŠ¥: {episode_return:.2f}, å¹³å‡å›æŠ¥: {avg_return:.2f}, æ­¥æ•°: {steps}")
      # ä¿å­˜æœ€ä½³æ¨¡å‹
      if episode_return > self.best_return:
        self.best_return = episode_return
        best_model_path = os.path.join(self.training_path, 'best_' + self.actor_filename)
        best_critic_path = os.path.join(self.training_path, 'best_' + self.critic_filename)
        torch.save(self.actor.state_dict(), best_model_path)
        torch.save(self.critic.state_dict(), best_critic_path)
        self.best_model_path = best_model_path
        print(f"ğŸ’¾ ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹ (å›æŠ¥: {self.best_return:.2f})")
      
      # å®šæœŸä¿å­˜æ¨¡å‹
      if ((epoch + 1) % self.save_freq == 0) or (epoch == self.epochs - 1):
        torch.save(self.actor.state_dict(), 
                  os.path.join(self.training_path, f'{epoch + 1}_{self.actor_filename}'))
        torch.save(self.critic.state_dict(), 
                  os.path.join(self.training_path, f'{epoch + 1}_{self.critic_filename}'))
        print(f"ğŸ’¾ ä¿å­˜ç¬¬{epoch + 1}è½®æ¨¡å‹")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves(pi_losses, critic_losses)
        
    # ä¿å­˜å›æŠ¥å†å²åˆ°æ–‡ä»¶
    np.savetxt(os.path.join(self.training_path, 'ddpg_lunar_return_history.txt'), self.return_traj)
    
    # æœ€ç»ˆç»˜åˆ¶è®­ç»ƒæ›²çº¿
    self.plot_training_curves(pi_losses, critic_losses)
    print(f"è®­ç»ƒå®Œæˆï¼Œæœ€ä½³å›æŠ¥: {self.best_return:.2f}")
    
  def plot_training_curves(self, pi_losses, critic_losses):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    plt.rcParams['font.sans-serif'] = ['SimHei']  # è®¾ç½®ä¸­æ–‡é»‘ä½“
    plt.rcParams['axes.unicode_minus'] = False    # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
    
    plt.figure(figsize=(12, 6))
    
    # å›æŠ¥æ›²çº¿
    plt.plot(self.return_traj, 'b-', alpha=0.7, label='æ¯è½®å›æŠ¥')
    
    if len(self.return_traj) >= 20:
      # è®¡ç®—ç§»åŠ¨å¹³å‡
      window = min(50, len(self.return_traj)//10)
      moving_avg = np.convolve(self.return_traj, np.ones(window)/window, mode='valid')
      plt.plot(range(window-1, len(self.return_traj)), moving_avg, 'r-', linewidth=2, label=f'ç§»åŠ¨å¹³å‡({window})')
      
    plt.axhline(y=self.success_threshold, color='g', linestyle='--', label=f'æˆåŠŸçº¿: {self.success_threshold}')
    plt.axhline(y=self.best_return, color='orange', linestyle='--', label=f'æœ€ä½³: {self.best_return:.2f}')
    
    plt.title('DDPG on LunarLanderContinuous-v2 è®­ç»ƒå›æŠ¥æ›²çº¿')
    plt.xlabel('è®­ç»ƒè½®æ¬¡')
    plt.ylabel('å›æŠ¥å€¼')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(self.training_path, 'training_curve.png'), dpi=300)
    
  def ddpg_test(self, epochs=None):
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹"""
    if epochs is None:
      epochs = 10
      
    total_reward = 0
    print("\nå¼€å§‹æµ‹è¯•DDPGæ¨¡å‹...")
    
    for epoch in range(epochs):
      observation, _ = self.env.reset()
      episode_return = 0
      done = False
      steps = 0
      
      while not done:
        # è·å–åŠ¨ä½œ
        state = observation.reshape(1, -1)
        action = self.actor(torch.as_tensor(state, dtype=torch.float32)).detach().numpy()
        
        # ç¯å¢ƒäº¤äº’ (æ— å™ªå£°)
        observation_next, reward, terminated, truncated, _ = self.env.step(action[0])
        done = terminated or truncated
        
        # ç´¯ç§¯å›æŠ¥
        episode_return += reward
        steps += 1
        observation = observation_next
        
      total_reward += episode_return
      print(f"æµ‹è¯• {epoch+1}/{epochs}, å›æŠ¥: {episode_return:.2f}, æ­¥æ•°: {steps}")
      
    avg_reward = total_reward / epochs
    print(f"\næµ‹è¯•å®Œæˆï¼Œå¹³å‡å›æŠ¥: {avg_reward:.2f}")
    return avg_reward
    
  def load_model(self, training_path, actor_filename, critic_filename):
    """åŠ è½½æ¨¡å‹"""
    self.actor.load_state_dict(torch.load(os.path.join(training_path, actor_filename)))
    self.critic.load_state_dict(torch.load(os.path.join(training_path, critic_filename)))
    print(f"æ¨¡å‹åŠ è½½å®Œæˆ: {actor_filename}, {critic_filename}")

if __name__=='__main__':
  # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
  random.seed(1)
  np.random.seed(1)
  torch.manual_seed(1)
  
  # åˆ›å»ºLunarLanderContinuousç¯å¢ƒ
  env_name = 'LunarLanderContinuous-v2'
  env = gym.make(env_name)
  
  # åˆ›å»ºDDPGæ™ºèƒ½ä½“
  lunar_lander_ddpg = DDPG(env)
  
  # è®­ç»ƒæ¨¡å¼
  lunar_lander_ddpg.ddpg_train()
  
  # ç¯å¢ƒæ¸…ç†
  env.close()
  print("\nè®­ç»ƒå®Œæˆï¼")
