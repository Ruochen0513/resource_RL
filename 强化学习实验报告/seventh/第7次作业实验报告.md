<h1><center>强化学习实验第七次实验报告</center></h1>
<h4><center>智能科学与技术 2213530 张禹豪</center></h4>

## 一、实验要求
- 掌握策略梯度算法的基本原理和实现细节
- 使用策略梯度算法训练gym中的一款游戏环境并输出训练效果曲线
## 二、实验原理
当用函数表示策略时，我们可以选择一个目标函数，进而优化该目标函数以得到最优策略。这种方法被称为**策略梯度**。策略梯度方法是基于策略的，而之前的方法都是基于值的。其本质区别在于基于策略的方法是直接优化关于策略参数的目标函数，从而得到最优策略；而基于值的方法是通过先估计值再得到最优策略的。
### 2.1 策略表示：从表格到函数
在之前的章节中，策略都是用表格来表示的：所有的状态的动作概率都存储在一个表格中。实际上，策略也可以用函数来表示，记为$\pi(a|s,\theta)$，其中$\theta\in\mathbb{R}^m$是参数向量。
表格法与函数法之间的区别：
- 第一，定义最优策略的方式不同
  当用表格描述策略时，最优策略的定义是能够$最大化所有状态的状态值$，即其状态值大于或等于其他任意策略的状态值。当用函数描述策略时，最优策略的定义为它能够$最大化一个标量目标函数$。
- 第二，更新策略的方式不同
  当用表格描述策略时，可以通过直接改变表格中的元素来直接更新选择某些动作的概率。当用函数描述策略时，只能通过改变函数参数$\theta$来间接更新选择某些动作的概率。
- 第三，查看动作概率的方式不同
  当用表格描述策略时，可以通过查看表格中相应的元素直接获得某个动作的概率。当用函数描述策略时，我们需要将$(s,a)$输入到函数中，通过计算函数值来获得其概率。
由于上述的不同，使用函数表示策略具有诸多优势，它在处理大型状态-动作空间时更加高效，也具有更强的泛化能力。
当用函数表示策略时，我们的任务是最大化一个标量目标函数$J(\theta)$，其中$\theta$代表标量函数的参数。不同的参数对应不同的目标函数值，因此我们需要找到最优的参数从而优化该目标函数。最简单的优化方法是梯度上升：$$\theta_{t+1}=\theta_t+\alpha\nabla_\theta J(\theta_t)$$其中$\nabla_\theta J$是$J$相对于$\theta$的梯度，$\alpha>0$是步长。
### 2.2 目标函数：定义最优策略
在策略梯度方法中，用于定义最优策略的目标函数有如下两种。
#### 目标函数1：平均状态值
第一个常见的目标函数是*平均状态值*，其定义为$$\overline{v}_\pi=\sum_{s\in S}d(s)v_\pi(s)$$其中$d(s)$是状态s的权重，它满足对任何$s\in S$有$d(s) > 0$且$\sum_{s\in S}d(s)=1$。因此，权重$d(s)$也可以理解为状态s的概率分布，那么该目标函数可以重写为$$\overline{v}_\pi = \mathbb{E}_{S\sim d}[v_\pi(S)]$$我们的任务是找到一个最优策略(即最优的$\theta$)来最大化$\overline{v}_\pi$。
#### 目标函数2：平均奖励
第二个常见的目标函数是*平均奖励*。它的定义是$$\overline{r}_\pi = \sum_{s\in S}d_\pi(s)r_\pi(s) = \mathbb E_{S\sim d_\pi[r_\pi(S)]}$$其中$d_\pi$是平稳分布，另外$$r_\pi(s)=\sum_{\alpha\in\pmb{A}}\pi(a|s,\theta)r(s,a)=\mathbb{E}_{A\sim \pi(s,\theta)}[r(s,A)|s]$$是从状态s出发的(单步)即时奖励的期望值。这里$r(s,a)=\mathbb{E}[R|s,a]=\sum_rrp(r|s,a)$。
### 2.3 目标函数的梯度
为了最大化目标函数，可以使用梯度上升的方法。为此，需要首先计算这些目标函数的梯度。下面的定理给出了目标函数梯度的表达式。
**策略梯度定理**：$J(\theta)$的梯度是：$$\nabla_\theta J(\theta)=\sum_{s\in S}\eta(s)\sum_{a\in \mathbb{A}}\nabla_\theta\pi(a|s,\theta)q_\pi(s,a)$$其中$\eta$是状态的概率分布，$\nabla_\theta\pi$是$\pi$是关于$\theta$的梯度。此外，该式有如下等价的形式：$$\nabla_\theta J(\theta)=\mathbb{E}_{S\sim\eta,A\sim\pi(S,\theta)}[\nabla_\theta\ln\pi(A|S,\theta)q_\pi(S,A)]$$其中$\ln$是自然对数。
因为我们需要计算$\ln\pi(a|s,\theta)$，所以必须确保对于所有的$s,a,\theta$，有$$\pi(a|s,\theta)>0$$
- 这可以通过使用 softmax 函数来实现，它可以将向量中的元素从 (−∞, +∞) 归一化到 (0, 1) 。
- 这种基于 softmax 函数的形式可以由一个神经网络实现，其输入是 $s$ ，参数是 $\theta$ 。该网络有 $|A|$个输出，每个输出对应于一个动作 a 的 $π(a|s, \theta)$ 。输出层的激活函数应该是 softmax 。
- 由于对于所有的 a ，$π(a|s, \theta) > 0$ ，参数化策略是随机的，因此具有探索性。
### 2.4 梯度上升算法
现在，介绍第一个用于寻找最优策略的策略梯度算法：
- 最大化 J(θ) 的梯度上升算法为$$
θ_{t+1} = θ_t + α∇_θJ(θ)
= θ_t + αE[∇_θ ln π(A|S, θ_t)q_π(S, A)]$$
- 真实梯度可以用随机梯度代替：$$
θ_{t+1} = θ_t + α∇_θ ln π(a_t|s_t, θ_t)q_π(s_t, a_t)$$
- 此外，由于$q_\pi$是未知的，它可以被近似：$$
θ_{t+1} = θ_t + α∇_θln π(a_t|s_t, θ_t)q(s_t, a_t)$$
**注释**：如何进行采样？
$$E_{S∼d,A∼π}[∇_\theta ln π(A|S, θ_t)q_π(S, A)] \rightarrow ∇_θ ln π(a|s, θ_t)q_π(s, a)$$
- 如何对 S 进行采样？
	- $S ∼ d$，其中分布 $d$是在策略 $π$ 下的长期行为。
- 如何对 A 进行采样？
	- $A ∼ π(A|S, \theta)$。因此，$a_t$ 应该在状态 $s_t$ 下按照 $π(θ_t)$ 进行采样。
- 因此，策略梯度方法是同轨策略（on - policy）的。
### 2.5 REINFORCE算法
回顾一下：$$\theta_{t+1} = \theta_t + α∇_\theta ln π(a_t|s_t, \theta_t)q_π(s_t, a_t)$$被替换为：$$\theta_{t+1} = \theta_t + α∇_\theta ln π(a_t|s_t, \theta_t)q_t(s_t, a_t)$$其中$q_t(s_t, a_t)$是$q_\pi(s_t, a_t)$的一个近似值。
- 如果 $q_π(s_t, a_t)$ 是通过蒙特卡罗估计来近似的，这个算法有一个特定的名字：**REINFORCE** 。
- **REINFORCE** 是最早且最简单的策略梯度算法之一。
![[reinforce.png]]
## 三、算法实现
### 3.1 训练脚本 (Gradient_Pendulm.py)
#### 3.1.1 策略网络定义（PolicyNetwork）
```python
# 定义策略网络

class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.mean_layer = nn.Linear(256, action_dim)
        self.log_std = nn.Parameter(torch.zeros(action_dim))

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        mean = torch.tanh(self.mean_layer(x))
        std = self.log_std.exp()
        return mean, std
```
- ​**​作用​**​：定义了一个神经网络，输入状态，输出动作的均值（`mean`）和标准差（`std`）。
- ​**​结构​**​：
    - 两个全连接层（256节点，ReLU激活）。
    - 均值输出层（`tanh`激活，将动作限制在[-1,1]）。
    - 可学习的对数标准差参数（`log_std`），通过指数运算得到标准差。
- ​**​动作分布：** $a ∼ π(·|s) = N(mean(s), std(s))$，其中std(s) = exp(log_std)。
#### 3.1.2 训练函数（train）
##### 3.1.2.1 环境与参数初始化
```python
env = gym.make('LunarLanderContinuous-v2')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.shape[0]
gamma = 0.99
learning_rate = 3e-4
episodes = 10000
```
**作用​**​：创建LunarLander环境，初始化状态和动作维度，设置超参数（折扣因子、学习率、训练回合数）。
##### 3.1.2.2 数据收集（回合循环）
```python
state, _ = env.reset()
done = False
log_probs = []
rewards_episode = []
```
​**​作用​**​：每个回合开始时重置环境，初始化存储对数概率和奖励的列表
##### 3.1.2.3 动作选择与经验收集
```python
state_tensor = torch.FloatTensor(state).to(device)
mean, std = policy(state_tensor)
dist = torch.distributions.Normal(mean, std)
action = dist.sample()
log_prob = dist.log_prob(action).sum()
next_state, reward, done, _ = env.step(action.cpu().numpy())
```
- ​**​作用​**​：
    1. 将状态转换为GPU张量，输入策略网络得到均值和标准差。
    2. 构建正态分布，采样动作并计算对数概率。
    3. 执行动作，获得下一个状态和奖励。
- ​**​数学公式​**​：
    - 动作采样：`a ∼ N(μ(s), σ(s))`。
    - 对数概率：$log π(a|s) = Σ_i [log N(a_i | μ_i(s), σ_i(s))]$（各维度独立高斯分布）。
##### 3.1.2.4 折扣回报计算
```python
discounted_rewards = []
R = 0
for r in reversed(rewards_episode):
    R = r + gamma * R
    discounted_rewards.insert(0, R)
discounted_rewards = (discounted_rewards - mean) / (std + 1e-7)
```
- **作用​**​：计算每个时间步的折扣回报$G_t = Σ_{k=t}^T γ^{k-t} r_k$，并标准化以减小方差。
- ​**​数学公式​**​：
    - 折扣回报：$G_t = r_t + γ r_{t+1} + γ^2 r_{t+2} + ...$。
    - 标准化：$G_t' = (G_t - μ_G) / σ_G$。
##### 3.1.2.5 策略梯度损失计算
```python
policy_loss = torch.stack([-log_prob * reward for ...]).sum()
```
- ​**​作用​**​：计算策略梯度损失，最大化期望回报。
- ​**​数学公式​**​：
    - 策略梯度：$∇_\theta J(\theta) ≈ E[Σ_t ∇_\theta log π(a_t|s_t) G_t]$。
    - 损失函数：$L = -Σ_t log π(a_t|s_t) G_t'$（负号因PyTorch默认最小化损失）。
##### 3.1.2.6 反向传播与优化
```python
optimizer.zero_grad()
policy_loss.backward()
optimizer.step()
```
- ​**​作用​**​：清空梯度，反向传播计算梯度，更新策略网络参数。
##### 3.1.2.7 算法总结
1. **目标​**​：最大化期望回报$J(\theta) = E[Σ_t γ^t r_t]$。
2. ​**​梯度公式​**​：$∇_θJ(θ)=E[∑^T_{t=0}​∇_θlogπ_θ​(a_t​∣s_t​)G_t​]$
3. ​**​更新步骤​**​：
    - 使用当前策略收集轨迹数据。
    - 计算每个时间步的折扣回报$G_t$。
    - 通过梯度上升更新参数：$θ ← θ + α ∇_θ J(θ)$。
### 3.2 测试脚本 (Gradient_Pendulm_test.py)
在测试脚本中提供了三种测试方式分别是：人类玩家测试、benchmark基准测试与单模型测试。
#### 3.2.1 人类玩家测试模式
```python
def human_play_test(env, num_episodes=5):
    """方向键控制的人类玩家测试"""
    pygame.init()
    screen = pygame.display.set_mode((400, 300))
    pygame.display.set_caption("Lunar Lander Control - 按ESC退出")
    print("\n=== 方向键控制说明 ===")
    print("上方向键: 启动主引擎")
    print("左/右方向键: 控制侧向引擎")
    print("ESC键: 退出测试\n")
    scores = []
    try:
        for episode in range(1, num_episodes+1):
            state, _ = env.reset()
            done = False
            total_reward = 0.0
            clock = pygame.time.Clock()
            # 控制参数
            main_thrust = 0.0
            lateral_thrust = 0.0
            while not done:
                # 处理事件队列
                for event in pygame.event.get():
                    if event.type == pygame.QUIT or \
                      (event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE):
                        pygame.quit()
                        return scores
                # 获取按键状态
                keys = pygame.key.get_pressed()
                # 主引擎控制（上键）
                main_thrust = 1.0 if keys[pygame.K_UP] else 0.0
                # 侧向控制（左右键）
                lateral_thrust = 0.0
                if keys[pygame.K_LEFT]:
                    lateral_thrust = 1.0
                if keys[pygame.K_RIGHT]:
                    lateral_thrust = -1.0
                # 构建动作向量 [主引擎，侧向推力]
                action = [main_thrust, lateral_thrust]
                # 执行动作
                next_state, reward, terminated, truncated, _ = env.step(action)
                total_reward += reward
                done = terminated or truncated
                state = next_state
                # 渲染与帧率控制
                env.render()
                clock.tick(30)
            scores.append(total_reward)
            print(f"回合 {episode}/{num_episodes} 得分: {total_reward:.1f}")
    finally:
        pygame.quit()
    return scores
```
- **功能​**​：允许用户通过键盘手动控制着陆器，测试人类玩家表现。
- ​**​交互逻辑​**​：
    - ​**​上方向键​**​：启动主引擎（垂直推力）
    - ​**​左/右方向键​**​：控制侧向引擎（水平推力）
    - ​**​ESC键​**​：退出测试
- ​**​技术细节​**​：
    - 使用`pygame`处理实时键盘输入和窗口渲染。
    - 帧率控制（`clock.tick(30)`）确保操作流畅。
    - 异常处理保证资源正确释放。
#### 3.2.2 模型测试函数
```python
def model_test(env, model_path, num_episodes=5, render=True):
    """AI模型测试"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # 初始化策略网络
    policy = PolicyNetwork(env.observation_space.shape[0],
                          env.action_space.shape[0]).to(device)
    policy.load_state_dict(torch.load(model_path, map_location=device))
    policy.eval()

    rewards = []
    for _ in range(num_episodes):
        state, _ = env.reset()
        done = False
        episode_reward = 0
        while not done:
            state_tensor = torch.FloatTensor(state).to(device)
            with torch.no_grad():
                mean, _ = policy(state_tensor)
            next_state, reward, terminated, truncated, _ = env.step(mean.cpu().numpy())
            episode_reward += reward
            done = terminated or truncated
            state = next_state
        rewards.append(episode_reward)
    return {
        'mean': np.mean(rewards),
        'max': np.max(rewards),
        'min': np.min(rewards)
    }
```
与训练代码不同的是，当进行测试模型的时候，动作选取使用**确定性执行​**：​
- 直接使用均值动作：`action = mean`
- 消除随机性，保证策略稳定性
#### 3.2.3 基准测试函数
```python
def benchmark_test(model_dir, human_scores):
    """基准对比测试"""
    print("\n=== 基准对比测试 ===")
    # 收集模型文件
    model_files = sorted(
        [f for f in os.listdir(model_dir) if f.endswith('.pth')],
        key=lambda x: int(re.search(r'model_(\d+).pth', x).group(1))
    )
    print(f"找到 {len(model_files)} 个模型文件")
    # 初始化环境
    env = gym.make('LunarLanderContinuous-v2')
    # 测试所有模型
    results = []
    progress = tqdm(model_files, desc="测试进度")
    for model_file in progress:
        model_path = os.path.join(model_dir, model_file)
        res = model_test(env, model_path, num_episodes=5, render=False)
        episode_num = int(re.search(r'model_(\d+).pth', model_file).group(1))
        results.append((episode_num, res))
    # 处理结果
    x = [r[0] for r in results]
    y_mean = [r[1]['mean'] for r in results]
    y_max = [r[1]['max'] for r in results]
    y_min = [r[1]['min'] for r in results]
    # 绘制对比图
    plt.figure(figsize=(15, 8))
    plt.plot(x, y_mean, 'b-', label='AI average')
    plt.fill_between(x, y_min, y_max, color='blue', alpha=0.1)
    plt.axhline(np.mean(human_scores), color='r', linestyle='--',
                label=f'human baseline ({np.mean(human_scores):.1f}±{np.std(human_scores):.1f})')
    plt.title('AI vs human')
    plt.xlabel('training episodes')
    plt.ylabel('score')
    plt.legend()
    plt.grid(True)
    # 保存结果
    plt.savefig('benchmark_comparison.png', dpi=300)
    print("\n对比图已保存为 benchmark_comparison.png")
    env.close()
```
- ​**​功能​**​：对比多个训练阶段的模型与人类玩家的表现。
- ​**​关键步骤​**​：
    1. 通过正则表达式匹配并按训练步数排序模型文件。
    2. 使用进度条（`tqdm`）可视化测试进度。
    3. 绘制折线图展示模型性能随训练的变化，并标注人类基准线。
    4. 图表保存为高分辨率PNG文件。
## 四、实验环境介绍
### LunarLanderContinuous-v2环境介绍
![[lunarlander.gif]]
**介绍**：LunarLanderContinuous-v2 是 OpenAI Gym 提供的强化学习环境之一，属于经典控制问题的连续动作空间版本。它模拟了登月舱（lander）在月球表面着陆的任务，目标是让智能体通过控制引擎的推力，使登月舱安全、平稳地降落在指定着陆点。
**环境概述**:
- **​任务目标​**​：控制登月舱的引擎推力，使其以低速（垂直和水平速度接近零）降落在两个黄色旗帜之间的着陆平台上。
- ​**​动作空间​**​：连续动作空间（与离散版本不同），智能体需要输出两个连续值：
    - ​**​主引擎推力​**​（垂直方向）：取值范围 `[0, 1]`，控制向下推力的大小。
    - ​**​侧引擎推力​**​（水平方向）：取值范围 `[-1, 1]`，控制向左或向右的推力。
- ​**​状态空间​**​：包含 8 个维度的观测值：
    - 登月舱的 `(x, y)` 坐标（相对于着陆点）。
    - 水平和垂直速度 ($v_x$, $v_y$)。
    - 登月舱的倾斜角度 `θ` 和角速度 `ω`。
    - 布尔值表示左/右支脚是否接触地面。
    - 布尔值表示登月舱主体是否接触地面。
- **奖励机制** ：
    从屏幕顶部移动到着陆垫并静止的奖励约为 100-140 分。如果着陆器远离着陆垫，则失去奖励。如果着陆器坠毁，则额外扣除-100 分。如果静止，则额外获得+100 分。每个与地面接触的腿+10 分。主发动机点火每帧-0.3 分。侧发动机点火每帧-0.03 分。解决成功得 200 分。
- **终止条件**： 
	1. 着陆器坠毁（着陆器主体与月球接触）；
	2. 着陆器飞出视野（ `x` 坐标大于 1）；
## 五、实验结果展示
### 5.1 训练结果曲线
模型在一块RTX 2060上训练4000次之后，训练曲线如下：
![[training_curve.png]]
由曲线可以发现，随着训练次数的提升，模型奖励有了大幅度的提升，当模型训练幕数在1000次之后，模型奖励逐渐收敛，证明了策略梯度算法对于该模型训练的有效性。同时由于训练函数中动作采样使用正态分布随机采样，所以奖励曲线存在一定的波动与随机性。
### 5.2 模型测试曲线
在训练过程中，每训练一百步保存一次模型参数，将上述训练得到的每个模型分别测试5次，分别记录最大值、最小值和平均值，其中测试过程中动作选取使用均值动作，确保测试过程中的确定性。测试曲线如下：![[benchmark_comparison.png]]
如图所示，蓝线是训练模型的平均分数，红线则是我自己进行5次游戏的平均得分。可见随着训练幕数的提升，模型得分很快变超越了人类得分，模型分数后续逐渐收敛到200+分左右。这证明了策略梯度算法在训练该游戏模型时的有效性。
由于测试模型时动作选取是确定性选取，所以测试奖励曲线要比训练奖励曲线更加高且稳定。
## 六、实验总结
本实验基于策略梯度算法（Policy Gradient）实现了对LunarLanderContinuous-v2环境的智能控制，验证了策略梯度方法在连续动作空间问题中的有效性。通过构建含双隐藏层的策略网络，以状态为输入、动作分布参数为输出，结合REINFORCE算法实现了策略的渐进式优化。实验表明，经过4000次训练后模型平均奖励收敛至200分以上，显著超越人类玩家基准水平（平均约-50分），成功达成安全着陆目标。
实验关键发现如下：
1. ​**​算法有效性验证​**​：训练曲线显示，随着训练步数增加，模型奖励从初始随机探索阶段的负值快速上升，约1000次迭代后趋于稳定，证实策略梯度能有效探索高回报策略。测试阶段采用确定性动作（均值）使奖励曲线更为平滑，体现出策略的稳定性。
2. ​**​探索与利用平衡​**​：训练阶段通过正态分布采样动作，在探索新策略与利用已知高回报行为间取得平衡，虽导致训练曲线波动，但避免了局部最优陷阱。测试阶段改用确定性策略则充分发挥了学习成果的优势。
3. ​**​奖励机制影响​**​：环境设计的密集奖励信号（如着陆成功+100、支脚触地+10）有效引导策略优化方向，配合折扣因子（γ=0.99）使模型注重长期回报，实现了精准的姿态控制与软着陆。
实验同时暴露以下改进方向：
- ​**​探索效率​**​：当前随机策略依赖初始参数，或导致探索效率低下，可尝试熵正则化项鼓励策略多样性。
- ​**​网络结构优化​**​：现有策略网络层数较浅（2×256），复杂场景下表征能力受限，可试验更深的网络或注意力机制提升状态特征提取能力。
本实验深化了对策略梯度理论的理解，掌握了连续控制任务的实现技巧，为后续深度强化学习研究奠定了实践基础。结果表明，策略梯度算法在复杂控制问题中具有强大潜力，通过算法改进与工程优化可进一步提升其性能与鲁棒性。