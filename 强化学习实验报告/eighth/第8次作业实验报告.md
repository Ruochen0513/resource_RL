<h1><center>强化学习实验第八次实验报告</center></h1>
<h4><center>智能科学与技术 2213530 张禹豪</center></h4>

## 一、实验目的
本实验旨在通过实现和训练PPO（Proximal Policy Optimization，近端策略优化）算法，深入理解其在连续动作空间强化学习任务中的原理与应用。具体目标包括：
- 掌握PPO算法的核心思想、数学推导及其关键实现细节；
- 熟悉PPO在OpenAI Gym经典环境（LunarLanderContinuous-v2）中的训练流程；
- 通过与人类玩家和传统策略梯度算法的对比，验证PPO算法在样本效率、收敛速度和最终性能等方面的优势；
- 培养分析和解决实际强化学习问题的能力，为后续更复杂环境和算法的研究打下基础。
## 二、实验原理
### 2.1 PPO算法基本原理
PPO（近端策略优化，Proximal Policy Optimization）算法是一种基于策略梯度的强化学习方法，它在TRPO（信任区域策略优化）的基础上进行了改进，旨在提高样本效率、实现更稳定的训练过程，同时保持实现的简单性。
PPO算法的核心思想是：在更新策略时，限制新策略与旧策略之间的差异，防止过大的策略更新导致训练不稳定。这种限制通过裁剪目标函数（Clipped Objective Function）来实现，确保策略更新幅度保持在合理范围内。
PPO算法具有以下几个特点：
- 样本效率高：可以多次使用同一批数据进行多次梯度更新
-  实现简单：相比TRPO等复杂算法，PPO只需要一阶优化
- 训练稳定：通过裁剪目标函数限制策略更新幅度
- 适用性广：适用于连续动作空间和离散动作空间
- 并行化友好：可以轻松实现并行数据收集
### 2.2 PPO算法的数学推导
#### 2.2.1 从策略梯度到PPO
回顾策略梯度方法，其目标函数为：
$$J(\theta) = \mathbb{E}_{s \sim d^\pi, a \sim \pi_\theta}[R(s, a)]$$
其中$\pi_\theta$是参数化策略，$d^\pi$是策略$\pi$下的状态分布，$R(s, a)$是奖励函数。
策略梯度的更新方式为：
$$\theta_{t+1} = \theta_t + \alpha \nabla_\theta J(\theta_t)$$
这种直接更新方式可能导致过大的策略变化，引起训练不稳定。
使用策略梯度定理，我们可以得到梯度的表达式：
$$\nabla_\theta J(\theta) = \mathbb{E}_{s \sim d^\pi, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) \cdot Q^\pi(s,a)]$$
为了减少方差，我们可以引入基线函数（通常是状态价值函数），得到基于优势函数的梯度表达式：
$$\nabla_\theta J(\theta) = \mathbb{E}_{s \sim d^\pi, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) \cdot A^\pi(s,a)]$$其中$A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$是优势函数。
#### 2.2.2 重要性采样
PPO使用重要性采样技术，允许使用旧策略$\pi_{\theta_{\text{old}}}$收集的数据来估计新策略$\pi_\theta$的期望回报：
$$J(\theta) = \mathbb{E}_{s \sim d^{\pi{\text{old}}}, a \sim \pi_{\theta_{\text{old}}}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} R(s, a) \right]$$
定义比率$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$，则目标函数可以写为：
$$J(\theta) = \mathbb{E}_t[r_t(\theta) \cdot A_t]$$其中$A_t$是优势函数，估计动作$a_t$在状态$s_t$下的相对价值。
重要性采样使得我们可以使用离线数据（由旧策略收集）来评估新策略的性能，这是PPO算法的关键基础。
#### 2.2.3 裁剪目标函数
直接最大化$J(\theta) = \mathbb{E}_t[r_t(\theta) \cdot A_t]$可能导致过大的策略更新，因为$r_t(\theta)$可能变得很大。TRPO通过添加KL散度约束解决这个问题，而PPO提出了更简单的裁剪目标函数：
$$J^{\text{CLIP}}(\theta) = \mathbb{E}_t[\min(r_t(\theta) \cdot A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \cdot A_t)]$$其中$\epsilon$是裁剪参数（通常设为0.1或0.2）。这个目标函数有两个关键特性：
当优势$A_t > 0$时，新策略只会被鼓励增加动作概率，但增加的幅度被限制在$[1, 1+\epsilon]$范围内。
> 如果$r_t(\theta) > 1+\epsilon$，则$\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \cdot A_t = (1+\epsilon) \cdot A_t < r_t(\theta) \cdot A_t$，因此$\min()$会选择裁剪后的值。
> 这意味着当$r_t(\theta) > 1+\epsilon$时，梯度为零，不再鼓励策略继续增加该动作的概率。
> 当优势$A_t < 0$时，新策略只会被鼓励减少动作概率，但减少的幅度被限制在$[1-\epsilon, 1]$范围内。
> 如果$r_t(\theta) < 1-\epsilon$，则$\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \cdot A_t = (1-\epsilon) \cdot A_t > r_t(\theta) \cdot A_t$（因为$A_t < 0$），因此$\min()$会选择非裁剪的值。
> 这意味着当$r_t(\theta) < 1-\epsilon$时，梯度为零，不再鼓励策略继续减少该动作的概率。

裁剪目标函数的数学表达式可以进一步分解为两种情况：
对于$A_t \geq 0$的情况：
$$J^{\text{CLIP}}t(\theta) = \min(r_t(\theta) \cdot A_t, (1+\epsilon) \cdot A_t)$$对于$A_t < 0$的情况：
$$J^{\text{CLIP}}t(\theta) = \max(r_t(\theta) \cdot A_t, (1-\epsilon) \cdot A_t)$$
这种裁剪机制有效防止了过大的策略更新，提高了训练稳定性。
### 2.3 PPO算法的关键组件
#### 2.3.1 广义优势估计（GAE）
PPO通常使用广义优势估计来计算优势函数：
$$A^{\text{GAE}}(s_t, a_t) = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l}$$其中$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$是时序差分误差；$\gamma$是折扣因子；$\lambda$是GAE参数，控制偏差-方差权衡。
实际实现中，我们可以通过递归方式高效计算GAE：
$$A^{\text{GAE}}t = \delta_t + \gamma \lambda A^{\text{GAE}}{t+1}$$GAE可以看作是不同n步优势估计的加权平均：
$$A^{\text{GAE}}(s_t, a_t) = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} A^{(n)}t$$其中$A^{(n)}t$是n步优势估计：
$$A^{(n)}t = \sum{i=0}^{n-1} \gamma^i r_{t+i} + \gamma^n V(s_{t+n}) - V(s_t)$$GAE参数$\lambda$控制了偏差-方差权衡：
当$\lambda = 0$时，GAE等价于1步TD误差，偏差低但方差高。
当$\lambda = 1$时，GAE等价于蒙特卡洛估计，偏差高但方差低。
实践中通常选择$\lambda = 0.95$左右，平衡偏差和方差。
#### 2.3.2 值函数学习
PPO同时学习一个值函数$V_\phi(s)$，用于估计状态价值和计算优势。值函数的损失通常为均方误差：
$$L^{VF}(\phi) = \mathbb{E}t[(V\phi(s_t) - V_t^{\text{target}})^2]$$其中$V_t^{\text{target}}$是目标值，可以是折扣累积回报或TD目标。
在实际实现中，目标值通常是n步回报或蒙特卡洛回报：
$$V_t^{\text{target}} = \sum_{i=0}^{T-t-1} \gamma^i r_{t+i}$$其中$T$是轨迹长度。
为了增强训练稳定性，有时会对值函数损失进行裁剪：
$$L^{VF}{\text{clip}}(\phi) = \mathbb{E}_t[\max((V\phi(s_t) - V_t^{\text{target}})^2, (V_{\phi_{\text{old}}}(s_t) + \text{clip}(V_\phi(s_t) - V_{\phi_{\text{old}}}(s_t), -\epsilon_v, \epsilon_v) - V_t^{\text{target}})^2)]$$
其中$\epsilon_v$是值函数裁剪参数。
#### 2.3.3 熵正则化
为了鼓励探索，PPO通常包含一个熵正则化项：
$$L^{ENT}(\theta) = -\beta \mathbb{E}{s \sim d^{\pi\theta}, a \sim \pi_\theta}[H(\pi_\theta(\cdot|s))]$$其中$H$是策略的熵，$\beta$是控制探索程度的系数。
策略熵定义为：
$$H(\pi_\theta(\cdot|s)) = -\sum_a \pi_\theta(a|s) \log \pi_\theta(a|s)$$对于连续动作空间，熵的计算为：
$$H(\pi_\theta(\cdot|s)) = \frac{1}{2} \log(2\pi e \sigma^2)$$其中$\sigma$是策略分布的标准差。
熵正则化有几个重要作用：
- 鼓励探索，防止策略过早收敛到次优解
- 增加动作的多样性，有助于发现更好的策略
- 防止策略退化为确定性策略，保持适当的不确定性

熵正则化的权重$\beta$通常随着训练进行而减小，初期保持较大探索，后期逐渐减小探索以便收敛。
#### 2.3.4 完整目标函数
PPO的完整目标函数结合了上述三个组件：
$$L^{\text{TOTAL}}(\theta, \phi) = \mathbb{E}_t[L^{\text{CLIP}}(\theta) - c_1 L^{VF}(\phi) + c_2 L^{ENT}(\theta)]$$其中$c_1$和$c_2$是权重系数。
实际实现中，目标函数通常写为：
$$L^{\text{TOTAL}}(\theta, \phi) = -L^{\text{CLIP}}(\theta) + c_1 L^{VF}(\phi) - c_2 L^{ENT}(\theta)$$其中负号表示最小化损失（等价于最大化原目标函数）。
权重系数$c_1$和$c_2$的选择影响训练过程：
- $c_1$通常设为0.5或1.0，控制值函数学习的重要性
- $c_2$通常设为0.01或更小，控制熵正则化的强度
### 2.4 PPO算法流程
PPO算法的完整流程如下：
> 初始化策略网络参数$\theta$和值函数网络参数$\phi$
&emsp;&emsp;对于每个迭代$k = 1, 2, ...$：
&emsp;&emsp;&emsp;&emsp;使用当前策略$\pi_{\theta_k}$收集一批轨迹数据$\mathcal{D}k = \{(s_t, a_t, r_t, s{t+1})\}$
&emsp;&emsp;&emsp;&emsp;计算每个状态-动作对的回报$R_t$和优势估计$A_t$（使用GAE）
&emsp;&emsp;&emsp;&emsp;通过随机梯度下降最大化目标函数：
$$\theta_{k+1} = \arg\max_\theta \mathbb{E}{(s,a) \sim \mathcal{D}_k} \left[ \min\left(\frac{\pi\theta(a|s)}{\pi_{\theta_k}(a|s)}A_t, \text{clip}\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}, 1-\epsilon, 1+\epsilon\right)A_t \right) + \beta H(\pi_\theta(\cdot|s)) \right]$$&emsp;&emsp;&emsp;&emsp;通过随机梯度下降最小化值函数损失：
$$\phi_{k+1} = \arg\min_\phi \mathbb{E}{(s,a) \sim \mathcal{D}_k} \left[ (V\phi(s) - R_t)^2 \right]$$具体实现中，步骤c和d通常同时进行，使用多个小批量（mini-batch）对同一批数据进行多次更新。

算法的伪代码如下：
```
初始化策略参数θ和值函数参数φ
for 迭代次数 k = 1, 2, ... do
    收集一批轨迹数据D_k = {(s_t, a_t, r_t, s_{t+1})}，使用策略π_θk
    
    计算每个时间步的回报R_t
    计算每个时间步的优势估计A_t（使用GAE）
    
    # 对收集的数据进行多次更新
    for epoch = 1, 2, ..., E do
        将数据D_k随机分成多个小批量
        for 每个小批量(s, a, R, A) do
            计算策略比率r(θ) = π_θ(a|s) / π_θk(a|s)
            计算裁剪目标L^CLIP(θ)
            计算值函数损失L^VF(φ)
            计算熵损失L^ENT(θ)
            计算总损失L^TOTAL(θ, φ) = -L^CLIP(θ) + c_1 * L^VF(φ) - c_2 * L^ENT(θ)
            
            执行一步梯度下降更新θ和φ
        end for
    end for
end for
```
## 三、代码实现
### 3.1 Sample类 - 轨迹采样与GAE计算
```python
class Sample:
    def __init__(self, env):
        self.env = env
        self.gamma = 0.99  # 折扣因子
        self.lamda = 0.95  # GAE参数
        self.batch_state = None
        self.batch_act = None
        self.batch_logp = None
        self.batch_adv = None
        self.batch_val_target = None
        self.index = None
        self.sum_return = 0
```
Sample类用于采样轨迹并计算GAE优势估计：
- env：强化学习环境
- gamma：折扣因子，设为0.99，用于计算折扣回报
- lamda：GAE参数，设为0.95，用于平衡偏差和方差
- batch_state等：存储采样数据的数组
- index：用于随机抽样小批量数据的索引
- sum_return：记录总回报
```python
def sample_one_episode(self, actor_net, critic_net):
    """采样一条轨迹"""
    episode_obs = []
    episode_actions = []
    episode_logps = []
    episode_rewards = []
    episode_vals = []
    cur_obs, _ = self.env.reset()
    done = False
    episode_sum = 0
    while not done:
        episode_obs.append(cur_obs)
        obs_tensor = torch.as_tensor(cur_obs, dtype=torch.float32)
        # 采样连续动作和log概率
        action, logp = actor_net.get_a(obs_tensor)
        value = critic_net.get_v(obs_tensor)
        episode_actions.append(action)
        episode_logps.append(logp)
        episode_vals.append(value)
        # 环境交互
        next_obs, reward, terminated, truncated, _ = self.env.step(action)
        done = terminated or truncated
        cur_obs = next_obs
        episode_rewards.append(reward)
        episode_sum += reward
```
这个方法实现了单条轨迹的采样过程：
1. 初始化空列表，用于存储轨迹中的观察、动作、对数概率、奖励和值函数
2. 重置环境获取初始观察
3. 进入循环，直到轨迹结束：
	- 存储当前观察
	- 将观察转换为张量
	- 使用策略网络采样动作和对数概率
	- 使用价值网络评估当前状态值
	- 存储动作、对数概率和值
	- 执行动作与环境交互，获取下一状态、奖励等信息
	- 更新观察并存储奖励
	- 累加总回报
```python
# GAE优势计算
vals = episode_vals + [0]  # 末状态价值设为0
episode_adv = []
adv = 0

for t in reversed(range(len(episode_rewards))):
    delta = episode_rewards[t] + self.gamma * vals[t+1] - vals[t]
    adv = delta + self.gamma * self.lamda * adv
    episode_adv.insert(0, adv)

# 目标值函数（累计折扣回报）
val_target = []
ret = 0
for r in reversed(episode_rewards):
    ret = r + self.gamma * ret
    val_target.insert(0, ret)
    
return (np.array(episode_obs), np.array(episode_actions),
        np.array(episode_logps).reshape(-1, 1), np.array(episode_adv).reshape(-1, 1),
        np.array(val_target).reshape(-1, 1), episode_sum)
```
这段代码实现了两个重要计算：
1. GAE（广义优势估计）计算：
	- 首先将末状态价值设为0
	- 从轨迹末尾向前遍历，计算时序差分误差delta
	- 使用递推公式计算优势函数：adv = delta + gamma * lambda * adv
	- 这是GAE公式的实现：$A^{\text{GAE}}t = \delta_t + \gamma \lambda A^{\text{GAE}}{t+1}$
2. 目标值函数（折扣回报）计算：
	- 同样从轨迹末尾向前遍历
	- 使用递推公式计算折扣回报：ret = r + gamma * ret
	- 这实现了折扣回报计算：$G_t = r_t + \gamma G_{t+1}$
最后将所有数据转换为NumPy数组并返回。
```python
def get_data(self, start_index, sgd_num):
    """获取mini-batch数据"""
    idx = self.index[start_index:start_index+sgd_num]
    sgd_batch_state = self.batch_state[idx]
    sgd_batch_act = self.batch_act[idx]
    sgd_batch_logp = self.batch_logp[idx]
    sgd_batch_adv = self.batch_adv[idx]
    sgd_batch_val_target = self.batch_val_target[idx]
    
    # 归一化优势
    if sgd_batch_adv.std() > 1e-8:
        sgd_batch_adv = (sgd_batch_adv - sgd_batch_adv.mean()) / (sgd_batch_adv.std() + 1e-8)
    else:
        sgd_batch_adv = sgd_batch_adv - sgd_batch_adv.mean()
        
    # 转换为tensor
    sgd_batch_state = torch.as_tensor(sgd_batch_state, dtype=torch.float32)
    sgd_batch_act = torch.as_tensor(sgd_batch_act, dtype=torch.float32)  # 连续动作使用float类型
    sgd_batch_logp = torch.as_tensor(sgd_batch_logp, dtype=torch.float32)
    sgd_batch_adv = torch.as_tensor(sgd_batch_adv, dtype=torch.float32)
    sgd_batch_val_target = torch.as_tensor(sgd_batch_val_target, dtype=torch.float32)
    
    return sgd_batch_state, sgd_batch_act, sgd_batch_logp, sgd_batch_adv, sgd_batch_val_target
```
这个方法用于从批量数据中提取小批量(mini-batch)数据，用于随机梯度下降：
1. 使用索引获取小批量数据
2. 对优势函数进行归一化处理，这是PPO的常见技巧，可以提高训练稳定性
3. 将NumPy数组转换为PyTorch张量
4. 返回转换后的小批量数据
### 3.2 Actor_Net类 - 策略网络
```python
class Actor_Net(nn.Module):
    def __init__(self, obs_dim, act_dim, hidden_sizes, action_bounds=None):
        super(Actor_Net, self).__init__()
        self.act_dim = act_dim
        self.action_bounds = action_bounds  # 动作范围
        
        # 共享特征提取网络
        self.shared = nn.Sequential(
            layer_init(nn.Linear(obs_dim, hidden_sizes[0])),
            nn.Tanh(),
            layer_init(nn.Linear(hidden_sizes[0], hidden_sizes[1])),
            nn.Tanh()
        )
        
        # 均值网络
        self.mu_net = layer_init(nn.Linear(hidden_sizes[1], act_dim), std=0.01)
        
        # 标准差网络
        self.log_std = nn.Parameter(torch.zeros(act_dim))
```
Actor_Net类实现了策略网络，用于生成连续动作的分布：
1. 初始化函数设置了网络架构：
	- act_dim：动作空间维度
	- action_bounds：动作范围限制
	- shared：共享特征提取网络，使用两个全连接层和Tanh激活函
	- mu_net：输出动作均值的网络层，初始化标准差较小(0.01)
	- log_std：可学习的对数标准差参数
```python
def forward(self, obs, act=None):
    features = self.shared(obs)
    mu = self.mu_net(features)
    
    # 应用动作边界
    if self.action_bounds is not None:
        # 缩放动作到边界范围
        action_low, action_high = self.action_bounds
        mu = torch.tanh(mu) * 0.5 * (action_high - action_low) + 0.5 * (action_high + action_low)
    
    # 获取标准差并限制范围
    std = torch.exp(self.log_std.clamp(-20, 2))
    
    # 创建正态分布
    dist = Normal(mu, std)
    
    if act is not None:
        logp = dist.log_prob(act).sum(dim=-1)  # 连续动作每个维度的log_prob求和
    else:
        logp = None
        
    entropy = dist.entropy().sum(dim=-1) if self.act_dim > 1 else dist.entropy()
    return dist, logp, entropy
```
forward方法实现了策略网络的前向传播：
1. 通过共享特征网络处理观察
2. 计算动作均值
3. 如果指定了动作边界，使用tanh激活函数将动作缩放到指定范围
4. 计算标准差，通过对对数标准差进行指数变换，并限制范围在e^(-20)到e^2之间
5. 创建正态分布对象
6. 如果提供了动作，计算对数概率（多维动作需要求和）
7. 计算策略熵
8. 返回分布、对数概率和熵
```python
def get_a(self, obs):
    """获取连续动作和log概率"""
    with torch.no_grad():
        dist, _, _ = self.forward(obs)
        action = dist.sample()
        logp = dist.log_prob(action).sum(dim=-1)
    return action.detach().numpy(), logp.item()
```
get_a方法用于采样动作：
1. 使用torch.no_grad()上下文，避免计算梯度
2. 调用forward方法获取动作分布
3. 从分布中采样动作
4. 计算采样动作的对数概率
5. 返回动作（转换为NumPy数组）和对数概率（转换为Python标量）
### 3.5 Critic_Net类 - 值函数网络
```python
class Critic_Net(nn.Module):
    def __init__(self, obs_dim, hidden_sizes):
        super(Critic_Net, self).__init__()
        self.critic_net = nn.Sequential(
            layer_init(nn.Linear(obs_dim, hidden_sizes[0])),
            nn.ReLU(),
            layer_init(nn.Linear(hidden_sizes[0], hidden_sizes[1])),
            nn.ReLU(),
            layer_init(nn.Linear(hidden_sizes[1], 1), std=1.0)
        )
        
    def forward(self, obs):
        return self.critic_net(obs).squeeze()
    
    def get_v(self, obs):
        """获取状态价值"""
        with torch.no_grad():
            return self.forward(obs).item()
```
**解释：**
- `Critic_Net`类实现了值函数网络，用于估计每个状态的价值。
- 网络结构为三层全连接网络，前两层使用ReLU激活，最后一层输出单个值（状态价值）。
- `forward`方法返回网络输出，并用`squeeze()`去除多余的维度。
- `get_v`方法用于在无梯度模式下获取单个状态的价值，常用于采样时。
### 3.6 PPO类 - 算法主流程
```python
class PPO:
    def __init__(self, env, hidden_sizes, pi_lr, critic_lr):
        self.env = env
        self.obs_dim = env.observation_space.shape[0]
        self.act_dim = env.action_space.shape[0]  # 连续动作空间维度
        
        # 获取动作范围并转换为张量
        self.action_bounds = [
            torch.as_tensor(env.action_space.low, dtype=torch.float32),
            torch.as_tensor(env.action_space.high, dtype=torch.float32)
        ]

        # 网络初始化
        self.actor = Actor_Net(self.obs_dim, self.act_dim, hidden_sizes, self.action_bounds)
        self.critic = Critic_Net(self.obs_dim, hidden_sizes)
        # 优化器
        self.pi_optimizer = Adam(self.actor.parameters(), lr=pi_lr)
        self.critic_optimizer = Adam(self.critic.parameters(), lr=critic_lr)
        # PPO超参数
        self.clip_ratio = 0.2
        self.epochs = 4000
        self.episodes_num = 10  # 每次更新采集的轨迹数
        self.train_pi_iters = 80
        self.sgd_num = 64  # 批大小
        self.save_freq = 10
        # 训练监控
        self.sampler = Sample(env)
        self.return_traj = []
        self.policy_losses = []
        self.entropy_losses = []
        self.critic_losses = []
        self.kl_divergences = []
        self.recent_rewards = deque(maxlen=100)  # 用于计算移动平均
        # 模型保存
        self.training_path = r'D:\\code\\resource_RL\\ppo_pendulm\\models'
        os.makedirs(self.training_path, exist_ok=True)
        self.best_return = -float('inf')
        self.best_model_path = None
        # 早停机制
        self.early_stop_kl = 0.02  # KL散度阈值
        self.success_threshold = 200  # 模型训练成功标准
```
**解释：**
- `PPO`类是整个算法的核心，负责训练、评估和保存模型。
- 初始化时，获取环境的状态和动作维度，设置动作范围，初始化策略网络和价值网络，以及对应的优化器。
- 设置PPO相关超参数，如裁剪比率、训练轮数、每轮采集的轨迹数、每轮策略更新次数、mini-batch大小、模型保存频率等。
- 还包括用于监控训练过程的变量和早停机制。
#### 3.6.1 策略损失与价值损失
```python
def compute_loss_pi(self, obs, act, logp_old, adv):
    """计算策略损失"""
    dist, logp, entropy = self.actor(obs, act)
    ratio = torch.exp(logp - logp_old.squeeze())
    
    # PPO-clip损失
    clip_adv = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * adv.squeeze()
    loss_pi = -torch.min(ratio * adv.squeeze(), clip_adv).mean()
    
    # 熵正则化
    loss_entropy = entropy.mean()
    
    # KL散度估计
    approx_kl = (logp_old.squeeze() - logp).mean().item()
    
    return loss_pi, loss_entropy, approx_kl

def compute_loss_critic(self, obs, val_target):
    """计算价值函数损失"""
    return ((self.critic(obs) - val_target.squeeze()) ** 2).mean()
```
**解释：**
- `compute_loss_pi`方法计算PPO的裁剪策略损失、熵正则化和KL散度。
  - `ratio`为新旧策略概率比。
  - `clip_adv`为裁剪后的目标。
  - `loss_pi`为PPO的主损失项，取clip前后最小值，防止策略更新过大。
  - `loss_entropy`为策略分布的熵，用于鼓励探索。
  - `approx_kl`为新旧策略的KL散度估计，用于早停。
- `compute_loss_critic`方法计算均方误差损失，用于训练价值网络。
#### 主训练循环
```python
def ppo_train(self):
    """主训练循环"""
    start_time = time.time()
    success_count = 0
    
    print("开始LunarLanderContinuous PPO训练...")
    print(f"目标：回报达到 {self.success_threshold}")
    
    for epoch in range(self.epochs):
        print(f"\n=== 训练轮次 {epoch+1}/{self.epochs} ===")
        self.update()
        
        # 保存最佳模型
        current_return = self.return_traj[-1]
        if current_return > self.best_return:
            self.best_return = current_return
            best_model_path = os.path.join(self.training_path, 'best_actor.pth')
            best_critic_path = os.path.join(self.training_path, 'best_critic.pth')
            torch.save(self.actor.state_dict(), best_model_path)
            torch.save(self.critic.state_dict(), best_critic_path)
            self.best_model_path = best_model_path
            print(f"💾 保存新的最佳模型 (回报: {self.best_return:.2f})")
        
        # 定期保存
        if (epoch + 1) % self.save_freq == 0:
            torch.save(self.actor.state_dict(), os.path.join(self.training_path, f'{epoch + 1}_actor.pth'))
            torch.save(self.critic.state_dict(), os.path.join(self.training_path, f'{epoch + 1}_critic.pth'))
            print(f"💾 保存第{epoch + 1}轮模型")
    
    end_time = time.time()
    training_duration = end_time - start_time
    print(f"\n训练完成，耗时: {training_duration:.2f}秒")
    
    # 绘制训练曲线
    self.plot_training_curves()
```
**解释：**
- `ppo_train`方法是训练主循环。
- 每一轮调用`update`方法进行一次PPO更新。
- 训练过程中保存最佳模型和定期保存模型。
- 训练结束后绘制训练曲线。
## 四、结果展示
### 4.1 训练环境介绍
### LunarLanderContinuous-v2 环境介绍
![[lunarlander.gif]]
**介绍**：LunarLanderContinuous-v2 是 OpenAI Gym 提供的强化学习环境之一，属于经典控制问题的连续动作空间版本。它模拟了登月舱（lander）在月球表面着陆的任务，目标是让智能体通过控制引擎的推力，使登月舱安全、平稳地降落在指定着陆点。
**环境概述**:
- **​任务目标​**​：控制登月舱的引擎推力，使其以低速（垂直和水平速度接近零）降落在两个黄色旗帜之间的着陆平台上。
- ​**​动作空间​**​：连续动作空间（与离散版本不同），智能体需要输出两个连续值：
    - ​**​主引擎推力​**​（垂直方向）：取值范围 `[-1, 1]`，控制垂直推力的大小。
    - ​**​侧引擎推力​**​（水平方向）：取值范围 `[-1, 1]`，控制向左或向右的推力。
    - 动作的第一个坐标确定主引擎的油门，而第二个坐标指定侧向推进器的油门。给定一个动作 `np.array([main, lateral])`，如果 `main < 0`，主引擎将完全关闭，对于 `0 <= main <= 1`，油门从 50% 到 100% 呈仿射缩放（特别是，主引擎在低于 50% 功率时不起作用）。同样，如果 `-0.5 < lateral < 0.5`，则侧向推进器根本不会点火。如果 `lateral < -0.5`，则左侧推进器将点火，如果 `lateral > 0.5`，则右侧推进器将点火。同样，油门在 -1 和 -0.5 之间（以及 0.5 和 1 之间，分别）从 50% 到 100% 呈仿射缩放。
- ​**​状态空间​**​：包含 8 个维度的观测值：
    - 登月舱的 `(x, y)` 坐标（相对于着陆点）。
    - 水平和垂直速度 ($v_x$, $v_y$)。
    - 登月舱的倾斜角度 `θ` 和角速度 `ω`。
    - 布尔值表示左/右支脚是否接触地面。
    - 布尔值表示登月舱主体是否接触地面。
- **奖励机制** ：
    从屏幕顶部移动到着陆垫并静止的奖励约为 100-140 分。如果着陆器远离着陆垫，则失去奖励。如果着陆器坠毁，则额外扣除-100 分。如果静止，则额外获得+100 分。每个与地面接触的腿+10 分。主发动机点火每帧-0.3 分。侧发动机点火每帧-0.03 分。如果 episode 得分至少为 200 分，则认为是一个解决方案。
- **终止条件**： 
	1. 着陆器坠毁（着陆器主体与月球接触）；
	2. 着陆器飞出视野（ `x` 坐标大于 1）；
	3. 着陆器未唤醒。根据 [Box2D 文档](https://box2d.org/documentation/md__d_1__git_hub_box2d_docs_dynamics.html#autotoc_md61)，未唤醒的物体是不移动且不与其他任何物体碰撞的物体
### 4.2 结果展示
#### 4.2.1 参数设置

| 超参数             | 参数设置 |
| --------------- | ---- |
| 隐藏层大小           | 64   |
| 策略网络学习率         | 3e-4 |
| 价值网络学习率         | 1e-4 |
| clip_ratio      | 0.2  |
| 每次更新采集的轨迹数      | 10   |
| 每次更新训练迭代次数      | 80   |
| SGD miniBatch大小 | 64   |
| 训练轮数            | 4000 |
#### 4.2.2 结果展示
##### 训练回报曲线展示
![[eighth/assets/1.png]]
可见随着训练轮数的增加，训练回报逐渐提升，当训练至1250轮左右之后训练回报逐渐收敛到最大稳定在270左右，这已经超过了可以认定作为本环境一个解决方案的回报值(200)。可见PPO算法对于训练解决该强化学习环境的有效性。
##### 其他指标训练曲线展示

![[training_curves.png]]
##### 测试曲线展示
![[eighth/assets/benchmark_comparison.png]]
如上图所示，在训练过程中每训练10轮即保存一次策略网络训练模型，在测试过程中按顺序分别导入上述模型进行测试，每个模型测试5次分别记录最大值、最小值和平均回报。
与训练代码不同的是，当进行测试的时候动作选取会选择**确定性动作**，即直接选取策略分布的均值，不再像训练时那样进行采样，以保证评估策略的真实能力并保持测试稳定性。
如上图，测试所示随训练进行模型回报很快提升并超过人类基准回报分数，之后便始终稳定在200分以上，后期接近300分。这证明了使用PPO训练得到的模型对于解决该强化学习环境的有效性。

#### 4.2.3 与策略梯度算法对比

**训练曲线**

| 策略梯度算法                  | PPO算法                    |
| ----------------------- | ------------------------ |
| ![[training_curve.png]] | ![[eighth/assets/1.png]] |

**测试曲线**

| 策略梯度算法                                       | PPO                                         |
| -------------------------------------------- | ------------------------------------------- |
| ![[seventh/assets/benchmark_comparison.png]] | ![[eighth/assets/benchmark_comparison.png]] |

从上表可以看出，PPO算法在训练过程中表现出更快的收敛速度和更高的稳定性。与传统策略梯度算法相比，PPO的训练曲线波动更小，最终回报更高，且更早达到环境设定的“解决”标准（平均回报200分）。在测试阶段，PPO训练得到的策略在多次评估中表现出更高的平均分数和更小的方差，显著优于传统策略梯度方法，并且在大多数模型保存点上均超过了人类玩家的基准分数。这充分说明了PPO算法在连续动作空间任务中的有效性和鲁棒性。
## 五、实验总结
本次实验通过在LunarLanderContinuous-v2环境下实现和训练PPO算法，系统地验证了其在强化学习中的优越性能。实验结果表明：
- PPO算法能够有效提升智能体的学习效率和最终表现，训练过程中回报迅速提升并稳定收敛，最终平均回报远超环境设定的“解决”标准；
- 与传统策略梯度算法相比，PPO在收敛速度、训练稳定性和最终得分等方面均有明显优势，且对超参数的敏感性较低，易于调试和应用；
- PPO训练得到的智能体在多次测试中表现优异，平均分数和最高分均超过人类玩家基准，展现出较强的泛化能力和鲁棒性；
- 实验过程中对PPO的核心机制（如裁剪目标函数、GAE优势估计、熵正则化等）有了更深入的理解，为后续研究更复杂环境和算法（如多智能体、分层强化学习等）奠定了坚实基础。
综上所述，PPO算法不仅理论上具有创新性，实际应用中也展现出极高的实用价值，是当前强化学习领域极具代表性的主流算法之一。