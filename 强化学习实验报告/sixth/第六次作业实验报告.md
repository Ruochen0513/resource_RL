<h1><center>强化学习实验第六次实验报告</center></h1>
<h4><center>智能科学与技术 2213530 张禹豪</center></h4>

## 一、实验要求
- 学习DQN算法，掌握DQN算法的基本原理
- 选取一款Atari视频游戏，使用DQN算法基于该游戏环境进行训练
## 二、实验原理
### 2.1 DQN算法
#### 2.1.1 简介
深度Q网络（Deep Q-Network, DQN）是DeepMind在2013年提出的结合深度学习和Q-Learning的强化学习算法。其核心创新点在于：
- 使用神经网络近似Q函数
- 引入​**​经验回放​**​和​**​目标网络​**​解决训练不稳定性问题
#### 2.1.2 Q-learning回顾
传统Q-Learning基于贝尔曼最优性方程更新Q值：
$$
Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \right]
$$
**局限性：**
- **维数灾难**：Q-learning需要为每个状态-动作对维护一个Q值，当状态或动作数量增加时，Q表的规模迅速膨胀，Q表的存储和计算需求呈指数级增长，导致算法难以处理高维问题。
- **无法处理连续状态和动作空间**：连续空间中状态和动作的无限性导致Q表无法存储，Q-learning仅适用于离散状态和动作空间，无法直接处理连续空间。
**局限性示例及解决方法：** 
- 在视频游戏或自动驾驶中，若状态空间为图像像素，Q表无法存储，需依赖深度学习（如DQN）解决。
- 同时，深度Q网络（DQN）通过神经网络近似Q函数，解决连续空间问题
#### 2.1.3 DQN核心思想
**DQN算法核心思想**： **用深度神经网络近似Q函数**
##### 2.1.3.1 问题背景
- **Q-Learning的局限性**：传统Q-Learning通过Q表存储状态-动作对的Q值，但当状态空间或动作空间维度较高（如图像输入、连续空间）时，Q表的存储和计算成本呈指数级增长（维数灾难）。
- **解决方案**：DQN使用深度神经网络（如卷积神经网络或全连接网络）直接**近似Q函数**，将Q值的计算转化为函数逼近问题。
##### 2.1.3.2 数学表达
- **Q函数形式**：
$$Q(s,a;\theta)\approx Q^*(s,a)$$
其中，$Q(s,a;\theta)$表示参数为 $\theta$ 的神经网络对状态 $s$ 和动作 $a$ 的Q值估计，$Q^*(s,a)$ 是真实的最优Q值。
- **输入与输出：**
	- **输入**：环境的状态 $s$（如像素矩阵、传感器数据等）。
	- **输出**：对每个可能动作 $a$ 的Q值估计（输出层为动作维度的向量）。
#### 2.1.4 关键技术：经验回放（Experience Replay）
##### 2.1.4.1 问题背景
- **数据相关性**：传统Q-Learning按时间顺序更新Q值，连续经验之间高度相关，导致训练不稳定。
- **解决方案**：通过**经验回放机制**打破数据相关性，提升训练稳定性。
##### 2.1.4.2实现步骤
1. **存储经验**：将智能体与环境交互的每一步经验$(s,a,r,s′,done)$存储到**经验回放缓冲区**（如循环队列）。
2. **随机采样**：在训练时，从缓冲区中**随机采样一批不连续的经验**，形成 minibatch。
3. **去相关性**：通过随机采样，打破经验之间的时序相关性，使训练数据更接近独立同分布，从而稳定学习过程。
#### 2.1.5 关键技术：目标网络（Target Network）
##### 2.1.5.1 问题背景
- **目标漂移（Target Instability）**：直接使用主网络（Primary Network）计算目标Q值会导致目标值不稳定，因为网络参数在每一步更新都会变化。
- **解决方案**：引入**目标网络**，通过**固定参数**来生成稳定的目标Q值。
##### 2.1.5.2 实现细节
- **双网络结构：**
	- **主网络（Q-Network）：** 用于选择动作和更新参数。
	- **目标网络（Target Network）：** 用于生成目标Q值，其参数每隔一定步数（如每1000步）从主网络复制一次。
- **目标值计算** $$y_i=r+\gamma \cdot max_{a'}Q(s',a';\theta^-)$$其中，$\theta^-$ 是目标网络的参数，$\gamma$ 是折扣因子。目标网络的参数 $θ^-$ 仅在定期更新时与主网络同步，从而减少目标值的波动。
#### 2.1.6 损失函数与优化
**损失函数设计**
- **均方误差（MSE）**：通过最小化主网络的预测Q值与目标值$y_i$的均方误差来更新网络参数：$$\mathcal{L}(\theta)=\mathbb{E}[(y_i-Q(s,a;\theta))^2]$$
- **优化方法**：使用梯度下降（如Adam优化器）更新主网络的参数$\theta$，以最小化损失函数。
#### 2.1.7 算法流程
以下是DQN的典型训练流程：
1. **初始化**：
    - 初始化主网络 $Q(s,a;θ)$ 和目标网络 $Q(s,a;θ^−)$，并设置两者的初始参数相同。
    - 初始化经验回放缓冲区。
2. **交互与存储**：
    - 智能体在环境中以 $ϵ-$ 贪婪策略选择动作（平衡探索与利用）。
    - 执行动作并记录经验 $(s,a,r,s′,done)$，存入回放缓冲区。
3. **经验回放与训练**：
    - 当缓冲区积累足够数据后，随机采样一批经验。
    - 对每个样本计算目标值 $y_i=r+γ⋅max_{⁡a′}Q(s′,a′;θ^−)$（目标网络计算）。
    - 计算主网络的预测Q值与目标值的损失，通过反向传播更新主网络参数。
4. **目标网络更新**：
    - 每隔一定步数（如1000步），将主网络的参数复制到目标网络。
### 2.2 Arcade Learning Environment（训练与测试DQN所使用环境框架）
#### 2.2.1 ALE框架简介
**Arcade Learning Environment（ALE）**，通常被称为 Atari，是一个框架，允许研究人员和爱好者开发用于 Atari 2600 roms 的 AI 智能体。它建立在 Atari 2600 模拟器 Stella 之上，将模拟的细节与智能体设计分离。用户可以通过 Gymnasium API、Python 接口和 C++接口与游戏交互。
#### 2.2.2 Atari游戏环境简介
**Atari 2600** 是由 Atari 公司于 1977 年推出的家用游戏机，是最早采用可更换游戏卡带的主机之一，开创了家用游戏机的先河。
**Atari 2600 游戏环境因以下特点成为强化学习（RL）研究的经典基准测试平台：**
- **挑战性：**
	- 高维视觉输入：游戏画面由像素构成，需要算法处理视觉信息。
	- 稀疏奖励：游戏奖励通常非即时且稀疏（如得分需长期策略）。
	- 动作空间复杂性：游戏需要组合动作和策略（如《Breakout》需精准控制球拍）。
-  **开源工具与框架**：
	-  **OpenAI Gym**：通过 `gym[atari]` 包提供 Atari 游戏环境（如 `Breakout-v5`）。
#### 2.2.3 典型Atari游戏环境介绍
##### 2.2.3.1 Breakout（密集奖励场景示例）
![[breakout.gif]]
**描述**：一款著名的雅达利游戏。动态与乒乓相似：你移动球拍，击打屏幕顶部的砖墙。你的目标是摧毁砖墙。你可以尝试打破墙壁，让球在另一边自行造成破坏！你有五条生命。

| Action Space      | Discrete(4)                         |
| ----------------- | ----------------------------------- |
| Observation Space | Box(0, 255, (210, 160, 3), uint8)   |
| Import            | `gymnasium.make("ALE/Breakout-v5")` |
**动作**：Breakout 具有 `Discrete(4)` 的动作空间，下表列出了每个动作的含义。

| Value | Meaning | Value | Meaning |
| ----- | ------- | ----- | ------- |
| 0     | NOOP    | 2     | RIGHT   |
| 1     | FIRE    | 3     | LEFT    |

**奖励**：通过摧毁墙壁上的砖块来获得分数。摧毁砖块的奖励取决于砖块的颜色。
##### 2.2.3.2 MontezumaRevenge（稀疏奖励场景示例）
![[montezuma_revenge.gif]]
**描述**：你的目标是通过穿越皇帝堡垒内部的迷宫般的房间来获得 Montezuma 的宝藏。你必须避开致命的生物，同时收集贵重物品和可以帮助你带着宝藏逃脱的工具。

| Action Space      | Discrete(18)                                |
| ----------------- | ------------------------------------------- |
| Observation Space | Box(0, 255, (210, 160, 3), uint8)           |
| Import            | `gymnasium.make("ALE/MontezumaRevenge-v5")` |
**动作**：MontezumaRevenge 具有动作空间 `Discrete(18)` ，下表列出了每个动作的含义。

| Value | Meaning      | Value | Meaning         | Value | Meaning        |
| ----- | ------------ | ----- | --------------- | ----- | -------------- |
| `0`   | `NOOP`       | `1`   | `FIRE`          | `2`   | `UP`           |
| `3`   | `RIGHT`      | `4`   | `LEFT`          | `5`   | `DOWN`         |
| `6`   | `UPRIGHT`    | `7`   | `UPLEFT`        | `8`   | `DOWNRIGHT`    |
| `9`   | `DOWNLEFT`   | `10`  | `UPFIRE`        | `11`  | `RIGHTFIRE`    |
| `12`  | `LEFTFIRE`   | `13`  | `DOWNFIRE`      | `14`  | `UPRIGHTFIRE`  |
| `15`  | `UPLEFTFIRE` | `16`  | `DOWNRIGHTFIRE` | `17`  | `DOWNLEFTFIRE` |
## 三、代码实现

### 导入库和设置超参数
```python
from __future__ import print_function
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
import os
import numpy as np
import cv2
import sys
import gymnasium as gym
import random
import ale_py
import time

#游戏名
GAME = 'Pong-v5'
ACTIONS = 6          # Pong环境的动作数量 
GAMMA = 0.99         # 奖励折扣因子
OBSERVE = 20000      # 训练前观察步数1
EXPLORE = 1.0e6      # 探索总步数
FINAL_EPSILON = 0.01 # 最终探索率
INITIAL_EPSILON = 1.0 # 初始探索率
REPLAY_MEMORY = 100000 # 经验回放缓冲区大小
BATCH = 64           # 批处理大小
FRAME_PER_ACTION = 1 # 跳帧数 (Pong环境已内置4帧跳帧)

# 检查是否有可用的GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"使用设备: {device}")
```
### 图像预处理函数
```python
# 图像预处理函数
def preprocess(observation):
    """
    对输入图像进行预处理：调整大小并二值化
    输入图像已经是灰度图 (grayscale_obs=True)
    """
    # 调整大小为84x84
    # resized = cv2.resize(observation, (84, 84))
    # 使用阈值128进行二值化，保留关键游戏元素
    _, thresh = cv2.threshold(observation, 128, 255, cv2.THRESH_BINARY)
    # 返回uint8类型的图像，值范围为[0, 255]
    return thresh.astype(np.uint8)
```
在这个函数中使用OpenCV库：
- 将游戏画面转换为灰度图并缩小尺寸，减少计算量。
- 二值化处理突出关键特征
### 神经网络层初始化
```python
def layer_init(layer, std=np.sqrt(1), bias_const=0.0):
    torch.nn.init.orthogonal_(layer.weight, std)  # 正交初始化权重
    torch.nn.init.constant_(layer.bias, bias_const)  # 常数初始化偏置
    return layer
```
- 使用正交初始化帮助网络训练更稳定，避免梯度消失/爆炸。
### 经验回放缓冲区
```python
#定义经验回报类，完成数据的存储和采样
class Experience_Buffer():
    """经验回放缓冲区，用于存储和采样训练数据"""
    def __init__(self,buffer_size = REPLAY_MEMORY):
        self.buffer = []
        self.buffer_size = buffer_size
    def add_experience(self, experience):
        """添加新经验，当缓冲区满时移除最早的经验"""
        if len(self.buffer) >= self.buffer_size:
            self.buffer.pop(0)  # 移除最早的经验
        self.buffer.extend(experience)
    def sample(self,samples_num):
        sample_data = random.sample(self.buffer, samples_num)
        train_s = np.zeros((samples_num,4,84,84))
        train_a = np.zeros((samples_num,ACTIONS))
        train_r = np.zeros((samples_num,1))
        train_s_ = np.zeros((samples_num,4,84,84))
        train_terminal = np.zeros((samples_num,1))
        for i in range(samples_num):
            train_s[i] = sample_data[i][0]
            train_a[i] = sample_data[i][1]
            train_r[i] =sample_data[i][2]
            train_s_[i] = sample_data[i][3]
            train_terminal[i] = sample_data[i][4]
        
        # 归一化像素值到[0,1]范围，这对于神经网络处理很重要
        train_s = train_s.astype(np.float32) / 255.0
        train_s_ = train_s_.astype(np.float32) / 255.0
        
        # 转换为PyTorch张量并移到指定设备
        train_s = torch.as_tensor(train_s, dtype=torch.float32).to(device)
        train_a = torch.as_tensor(train_a, dtype=torch.float32).to(device)
        train_r = torch.as_tensor(train_r, dtype=torch.float32).to(device)
        train_s_ = torch.as_tensor(train_s_, dtype=torch.float32).to(device)
        train_terminal = torch.as_tensor(train_terminal, dtype=torch.float32).to(device)
        
        return train_s, train_a, train_r, train_s_, train_terminal
```
- **作用：** 存储游戏经验（状态、动作、奖励等），用于随机采样以打破数据相关性。
- **关键点​**​：循环覆盖旧数据，随机采样减少过拟合。
- **`add_experience(self, experience)`** 
	- **作用**: 向缓冲区中添加新的经验数据。
	- **参数​**​：
		- `experience`：单条或多条经验数据（格式为 `[s_t, a_t, r_t, s_{t+1}, terminal]`）
	- **实现细节**： 
		- ​**​缓冲区溢出处理​**​
		- **添加新经验​**​
	- **意义**：
		- 确保缓冲区始终保存最新的经验，避免内存溢出。
		- 动态维护数据，优先保留近期经验以提高学习效率。
- **`sample(self, samples_num)`**
	- **作用​**​：从缓冲区中随机采样一批经验数据，并将其转换为适合训练的格式。
	- **参数​**​：
		- `samples_num`：采样的数量（如 `BATCH=32`）。
	- **实现细节**：
		- ​**​随机采样​**
			- 使用 `random.sample(self.buffer, samples_num)` 随机选择 `samples_num` 条经验。​
		- **数据初始化**
			- 创建空的 NumPy 数组存储以下字段：
				- `train_s`：当前状态（形状 `(batch, 4, 84, 84)`）。
				- `train_a`：动作的 one-hot 编码（形状 `(batch, ACTIONS)`）。
				- `train_r`：奖励值（形状 `(batch, 1)`）。
				- `train_s_`：下一状态（形状 `(batch, 4, 84, 84)`）。
				- `train_terminal`：是否终止的布尔标志（形状 `(batch, 1)`）。
		- **填充数据​**
			- 遍历采样的经验数据，将每个字段按索引填充到对应的数组中。
		- **转换为张量**
			- 将 NumPy 数组转换为 PyTorch 张量，以便输入神经网络。
### Q网络定义（Critic_Net）
```python
class Critic_Net(nn.Module):
    def __init__(self,action_dim):
        super(Critic_Net, self).__init__()
        self.layer_1 = layer_init(nn.Conv2d(in_channels=4,out_channels=32,kernel_size=8,stride=4,padding=2))
        self.pool_1 = nn.MaxPool2d(2,stride=2)
        self.layer_2 = layer_init(nn.Conv2d(in_channels=32,out_channels=64,kernel_size=4,stride=2,padding=1))
        self.layer_3 = layer_init(nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,stride=1,padding=1))
        self.critic_fc1 = layer_init(nn.Linear(1600,512))
        self.critic_fc2 = layer_init(nn.Linear(512,action_dim))
        
    def forward(self,state_input):
        # 使用适当的normalized_shape参数来修复层归一化
        x = state_input
        x = F.relu(self.layer_1(x))
        x = self.pool_1(x)
        x = F.relu(self.layer_2(x))
        x = F.relu(self.layer_3(x))
        x = x.view(-1, 1600)
        x = F.relu(self.critic_fc1(x))
        x = self.critic_fc2(x)
        return x
```
- **网络结构**：
	1. ​**​卷积层（Conv2d）​**​：
	    - ​**​第一层​**​：32个8x8卷积核，步长4，padding 2，处理4通道输入（如Atari游戏的帧堆叠）。输出特征图尺寸为21x21。
	    - ​**​最大池化层​**​：2x2窗口，步长2，将特征图尺寸降至10x10。
	    - ​**​第二层​**​：64个4x4卷积核，步长2，padding 1，输出5x5特征图。
	    - ​**​第三层​**​：64个3x3卷积核，步长1，padding 1，保持5x5尺寸不变。
	2. ​**​全连接层​**​：
	    - 将卷积输出的1600维特征（64通道×5×5）映射到512维，再进一步压缩至`action_dim`维度，表示每个动作的Q值。
- ​**​输入​**​：4帧堆叠的84x84图像（状态）。
- ​**​结构​**​：三层卷积提取空间特征，两层全连接输出动作价值。
- ​**​输出​**​：每个可能动作的Q值，用于决策。
### DQN算法实现（DQN类）
#### 初始化与模型加载
```python
class DQN():
    def __init__(self):
        self.tau = 0.01            # 软更新因子
        self.gamma = GAMMA         # 奖励折扣因子
        self.critic_lr = 0.0002    # 学习率
        self.batch_num = BATCH     # 批处理大小
        self.save_freq = 50000     # 模型保存频率
        self.target_update_freq = 3000  # 目标网络更新频率
        self.Q = Critic_Net(action_dim=ACTIONS).to(device)
        self.Q_ = Critic_Net(action_dim=ACTIONS).to(device)
        self.critic_optimizer = Adam(self.Q.parameters(), self.critic_lr)
        self.training_path = './models'
        self.critic_filename = 'deepQ_pong.pth'
        # 确保保存模型的目录存在
        os.makedirs(self.training_path, exist_ok=True)
        # 初始时复制在线网络到目标网络
        self.Q_.load_state_dict(self.Q.state_dict())
```
- **双网络结构：**
	- **主网络（Q-Network）：** 用于选择动作和更新参数。
	- **目标网络（Target Network）：** 用于生成目标Q值，其参数每隔一定步数（如每3000步）从主网络复制一次。
	- 软更新（tau）逐步同步参数。
#### 损失计算
```python
    def compute_loss_critic(self,obs,action,reward,obs_next,terminal):
        """计算批量损失函数，使用双Q学习方法
        参数:
            obs: 当前状态 [batch_size, 4, 84, 84] 
            action: 采取的动作(one-hot) [batch_size, ACTIONS]
            reward: 获得的奖励 [batch_size, 1]
            obs_next: 下一个状态 [batch_size, 4, 84, 84]
            terminal: 终止标志 [batch_size, 1]
        返回:
            critic_loss: MSE损失值
        """
        # 目标网络评估部分
        with torch.no_grad():
            # 1. 使用online网络选择动作（保持一维）
            best_actions = torch.argmax(self.Q(obs_next), dim=1)
            # 2. 使用target网络评估价值（从一维变为二维）
            q_next = self.Q_(obs_next).gather(1, best_actions.unsqueeze(1))
            # 3. 计算目标Q值
            # reward和terminal形状为[batch_size, 1]
            q_target = reward + self.gamma * q_next * (1 - terminal)
            # 4. 统一转为一维张量用于最终的损失计算
            q_target = q_target.squeeze(1)  # [batch_size]
        # 5. 计算当前策略的Q值预测
        q_values = self.Q(obs)  # [batch_size, action_dim]
        # 6. 选出实际采取的动作的Q值（one-hot乘法并求和）
        q_pred = torch.sum(q_values * action, dim=1)  # [batch_size]
        # 7. 计算MSE损失
        critic_loss = F.mse_loss(q_pred, q_target)
        return critic_loss
```
- **均方误差（MSE）**：通过最小化主网络的预测Q值与目标值$y_i$的均方误差来更新网络参数：$$\mathcal{L}(\theta)=\mathbb{E}[(y_i-Q(s,a;\theta))^2]$$
#### ε-贪婪策略
```python
	def epsilon_greedy(self, s_t, epsilon):
		s_t = torch.as_tensor(s_t, dtype=torch.float32).unsqueeze(0)
		a_t = np.zeros([ACTIONS])
		# 计算当前Q值
		with torch.no_grad():
			q = self.Q(s_t).numpy()
		amax = np.argmax(q[0])  # 最优动作
		# 以ε概率随机选择动作
		if np.random.uniform() < 1 - epsilon:
			a_t[amax] = 1  # 利用
		else:
			a_t[random.randrange(ACTIONS)] = 1  # 探索
		return a_t
```
- **作用​**​：平衡探索（随机动作）与利用（已知最优动作）。
### 训练函数
#### `soft_update`方法
- **功能**
通过加权平均更新目标网络参数，实现参数缓慢跟踪主网络（稳定性优化）：
```python
def soft_update(self, target_net, eva_net, tau):
    for target_param, param in zip(target_net.parameters(), eva_net.parameters()):
        target_param.data.copy_(target_param * (1 - tau) + param * tau)
```
- ​**​原理​**​：目标网络参数 `target_param` 逐步向评估网络 `eva_net` 靠近，更新公式为：$$targetparam=targetparam×(1−τ)+param×τ$$
- ​**​用途​**​：稳定训练过程（如DDPG、DQN中常见），避免主网络快速变化导致的价值估计波动。
#### `train_Network`方法
##### **​环境初始化​**​：
```python
    env = gym.make("ALE/Breakout-v5", render_mode="human")  # 创建带渲染的环境
    observation, info = env.reset()
    x_t = preprocess(observation)  # 预处理图像（如裁剪、灰度化、归一化）
    s_t = np.stack((x_t, x_t, x_t, x_t), axis=0)  # 堆叠4帧作为初始状态
 ```
- ​**​输入假设​**​：环境输出单帧图像，通过堆叠4帧捕捉动态信息（如Atari游戏中的运动）。
##### **​探索率设置​**​：
```python
    epsilon = INITIAL_EPSILON  # 初始探索率（如0.1）
    if epsilon > FINAL_EPSILON and t > OBSERVE:
        epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE
```
- ​**​ε-greedy策略​**​：前期高探索率（随机动作），后期逐步降低，转为利用策略网络。
##### 动作选择与执行：
```python
    a_t_index = np.argmax(self.epsilon_greedy(s_t, epsilon))  # 选择动作
    observation, r_t, terminated, truncated, info = env.step(a_t_index)
    terminal = terminated or truncated  # 判断回合是否终止
```
##### **​状态更新与经验存储​**​：
```python
    x_t1 = preprocess(observation)  # 预处理新状态
    x_t1 = np.reshape(x_t1, (1, 80, 80))
    s_t1 = np.append(x_t1, s_t[:3, :, :], axis=0)  # 移除最旧帧，添加新帧
    experience = np.array([s_t, a_t, r_t, s_t1, terminal])  # 存储经验（需修复a_t）
    experience_buffer.add_experience(experience)
    ```
- **经验存储**：将状态转移 (st​,at​,rt​,st+1​,terminal) 存入缓冲区，后续用于批量训练。
##### **​网络训练​**​：
```python
	if t > OBSERVE:  # 观察期结束后开始训练
        train_s, train_a, train_r, train_s_, train_terminal = experience_buffer.sample(BATCH)
        loss = self.compute_loss_critic(train_s, train_a, train_r, train_s_, train_terminal)
        self.critic_optimizer.zero_grad()
        loss.backward()
        self.critic_optimizer.step()
        self.soft_update(self.Q_, self.Q, self.tau)  # 更新目标网络
```
  - ​**​损失计算​**​：需依赖 `compute_loss_critic` 方法。
  - ​**​参数更新​**​：仅更新Critic网络，目标网络通过软更新逐步同步。
##### **​训练终止与模型保存​**​：
```python
    if terminal:  # 回合结束重置环境
        observation, info = env.reset()
        x_t = preprocess(observation)
        s_t = np.stack((x_t, x_t, x_t, x_t), axis=0)
    if t % self.save_freq == 0:  # 定期保存模型
        torch.save(self.Q.state_dict(), os.path.join(self.training_path, self.critic_filename + str(t)))
```
### 测试函数
```python
    def test_Network(self):
        # 创建Atlantis游戏环境
        env = gym.make("ALE/Pong-v5",  obs_type="grayscale")
        env = gym.wrappers.AtariPreprocessing(env, frame_skip = FRAME_PER_ACTION, grayscale_obs= True, scale_obs=False, terminal_on_life_loss=True)
        # 获得第一个状态
        observation, info = env.reset()
        x_t = preprocess(observation)
        
        # 堆叠4帧作为初始状态
        s_t = np.stack((x_t, x_t, x_t, x_t), axis=0)
        
        # 开始测试
        t = 0
        total_reward = 0
        
        while True:
            # 采用贪婪策略选择动作
            a_t = self.epsilon_greedy(s_t, epsilon = 0)
            a_t_index = np.argmax(a_t)
            
            # 执行动作，获取下一个状态
            observation, r_t, terminated, truncated, info = env.step(a_t_index)
            terminal = terminated or truncated
            
            total_reward += r_t
            
            # 处理新的观察
            x_t1 = preprocess(observation)
            x_t1 = np.reshape(x_t1, (1, 84, 84))
            
            # s_t1=[x_t1,xt,xt-1,xt-2]
            s_t1 = np.append(x_t1, s_t[:3, :, :], axis=0)
            
            # 往前推进一步
            s_t = s_t1
            t += 1
            # 如果游戏结束或者达到一定步数，结束测试
            if terminal:
                print(f"游戏结束，总步数: {t}，总得分: {total_reward}")
                break
        env.close()
        return t, total_reward
```
## 四、实验展示
### 4.1 所训练游戏环境介绍
#### Pong
![[pong.gif]]
**描述**：一款著名的雅达利游戏。您控制右侧的球拍，与由计算机控制的左侧球拍进行比赛。你们各自试图将球击回，使其远离自己的球门并进入对手的球门。

| Action Space      | Discrete(6)                       |
| ----------------- | --------------------------------- |
| Observation Space | Box(0, 255, (210, 160, 3), uint8) |
| Import            | `gymnasium.make("ALE/Pong-v5")`   |
**动作**：Pong具有 `Discrete(6)` 的动作空间，下表列出了每个动作的含义。

| Value | Meaning | Value | Meaning   | Value | Meaning  |
| ----- | ------- | ----- | --------- | ----- | -------- |
| 0     | NOOP    | 1     | FIRE      | 2     | RIGHT    |
| 3     | LEFT    | 4     | RIGHTFIRE | 5     | LEFTFIRE |

**奖励**：通过将球击过对手的拍子获得分数。如果球击过你的拍子，则扣分。每当将球击过对手的拍子获取1分，对方的球击过自己的拍子扣去1分。任意一方先拿到21分则游戏结束。
### 4.2 环境依赖
	Python  3.8.20
	Pytorch  1.12.1+cu113
	OpenCV-Python  4.10.0.84
	Gymnasium  0.29.1
### 4.3 结果展示
**在一块RTX 2080TI上训练40小时左右，共训练10,000,000步的结果如下：**
#### 4.3.1 智能体采取的总动作数 (Steps) 随训练步数变化曲线
![[result-steps.png]]
如上图所示，纵轴为应用当前训练步数下的模型后智能体进行游戏所采取的**总动作数**，横轴为训练步数（从0到10,000,000）。对于每个checkpoint都进行了5次测试，取最大值，最小值与平均值。
其中蓝实线为应用当前训练步数下的模型后智能体进行游戏所采取的总动作数的最大值，蓝虚线为最小值，红实线为平均值。
由上图可以发现，随着训练步数的增加，智能体进行游戏所采取的总动作数是逐渐增加的，这证明随着训练的进行，智能体会变得更加**主动**，采取更多动作以获取更多奖励。
#### 4.3.2 智能体获取的奖励随训练步数变化曲线
![[result-score.png]]
如上图所示，纵轴为应用当前训练步数下的模型后智能体进行游戏所获取的**总奖励**，横轴为训练步数（从0到10,000,000）。对于每个checkpoint都进行了5次测试，取最大值，最小值与平均值。
其中蓝实线为应用当前训练步数下的模型后智能体进行游戏所获取的总奖励的最大值，蓝虚线为最小值，红实线为平均值。
由上图可以发现，当不进行或进行少量训练时（加上图的最左边部分），智能体完全不会进行游戏，奖励保持为-21左右（即完全输给对方）。随着训练的进行，智能体获取的奖励逐渐增加，到训练500,0000steps左右时，智能体已经每局游戏平均能够获得10分奖励。这证明了DQN对于训练此类游戏模型的有效性。
受训练时间影响，后续训练并未继续，但就曲线趋势来看，只要训练时间（训练步数）足够，智能体的奖励能够达到最大值21。
#### 4.3.3 智能体平均每步动作获取的奖励随训练步数变化曲线
![[result-score-per-step.png]]
为了更好地衡量随着训练的进行，智能体是否变得更加“智能”，我使用scores/steps，即**平均每步动作获取的奖励**来衡量智能体的游戏能力。
如上图所示，纵轴为应用当前训练步数下的模型后智能体进行游戏平均每步动作获取的奖励，横轴为训练步数（从0到10,000,000）。对于每个checkpoint都进行了5次测试，取最大值，最小值与平均值。其中蓝实线为应用当前训练步数下的模型后智能体进行游戏平均每步动作获取的奖励的最大值，蓝虚线为最小值，红实线为平均值。
通过上述曲线可以发现，当训练步数达到300,0000左右时，智能体**平均每步动作获取的奖励**显著提升，这证明了随着训练的进行，智能体变得更加“**智能**”与**高效**，智能体更加注重每步动作的“**性价比**”。
当训练到10,000,000steps，智能体**平均每步动作获取的奖励**已经十分接近0，我相信随着训练的进行，智能体的平均每步动作获取的奖励将达到0以上。
### 4.4 附录-调参过程
最初我选取课堂给的源代码更换为Pong环境来进行训练，其他参数均未改变，以此训练了大约300,0000步左右效果提升并不明显，于是进行了如下调参历程。
#### 4.4.1 更改 INITIAL_EPSILON = 1.0
源代码的INITIAL_EPSILON = 0.1，epsilon的衰减范围为从0.1-0.000001。这十分的不合理，过低的初始EPSILON值扼杀了智能体选取动作的随机探索性，导致智能体无法探索到更多可能的状态空间，导致效果不好。因此更改INITIAL_EPSILON = 1.0，FINAL_EPSILON = 0.01。
#### 4.4.2 更改 OBSERVE = 20000
经验回放缓冲区需要足够的样本才能开始有效的训练。在初始阶段，如果立即开始训练，缓冲区中的数据可能太少，导致采样到的样本不够多样，甚至可能重复，影响训练效果。因此，设置一个观察期，先积累一定数量的经验，再开始训练，有助于模型有足够的数据进行学习，避免早期的不稳定。
源代码的训练前观察步数仅为100，这过于少导致训练前缓冲区采样的样本过少，影响了早期的训练，因此修改为了OBSERVE = 20000。
#### 4.4.3 修改REPLAY_MEMORY = 100000 # 经验回放缓冲区大小
在上文原理中已经提到了经验缓冲区的重要作用，但源代码的经验缓冲区大小为50,000，这实际上大小并不足够，可能导致无法存储足够的智能体与环境交互的历史经验（状态、动作、奖励、下一状态等），导致无法覆盖足够的状态-动作空间，导致训练效果不佳。
DQN的论文中最初设定的经验缓冲区大小为1,000,000，因此我最初设定REPLAY_MEMORY = 1000000，但训练到一定步数后，电脑内存占用达到99%导致电脑卡死。进过计算发现，
- ​**​容量​**​：存储 `1,000,000` 个样本（`REPLAY_MEMORY=1000000`）。
- ​**​单个样本内存​**​：
    - 状态 `s_t` 和 `s_t1`：每个为 `4x84x84` 的 `uint8` 数组（灰度图），各占 ​**​28,224 字节​**​。
    - 动作 `a_t`：`4` 维 `float32` 的 one-hot 向量，占 ​**​16 字节​**​。
    - 奖励 `r_t` 和终止标志 `terminal`：各占 ​**​4 字节​**​ 和 ​**​1 字节​**​。
    - ​**​总计约 `56,469 字节/样本`​**​。
- ​**​总内存​**​：`1,000,000 x 56,469 ≈ 56.46 GB`
这大大超过了我的PC的内存大小（16GB），故只能设定REPLAY_MEMORY = 100000。
#### 4.4.4 修改图像预处理部分
源代码中_, thresh = cv2.threshold(observation, 1, 255, cv2.THRESH_BINARY)将游戏图像进行二值化处理，灰度值小于1的部分设为0，大于1的设为255，这其实并不合理，经过实际测试发现，应用此预处理之后，整个游戏画面都被二值化为白色，导致无法正常训练。
后续我首先改为observation = observation.astype(np.float32) / 255.0将游戏画面归一化到0-1而不是进行二值化处理，但这样会将画面像素转变为float32类型，导致存储大小增大了4倍，内存再次爆炸。
因此我最终选择画面依旧进行二值化，只是将二值化的阈值调为了128，使得二值化之后画面各部分能够有效区分。
## 五、总结

本次实验基于DQN算法在Atari Pong游戏环境中进行了强化学习训练，成功实现了智能体从零开始学习并逐步提升游戏表现的目标。通过实验，验证了DQN算法在处理高维状态空间和稀疏奖励问题上的有效性，并深入掌握了相关技术细节与调参经验。
### 5.1 实验成果
1. ​**​算法有效性验证​**​：
    - 智能体经过10,000,000步训练后，平均每局得分从初始的-21分（完全失败）提升至+10分，证明DQN能够有效学习游戏策略。
    - 通过经验回放与目标网络技术，成功解决了训练过程中Q值估计不稳定的问题，训练曲线呈现稳定上升趋势。
2. ​**​性能指标分析​**​：
    - ​**​总动作数​**​：从初始的不足200步提升至稳定3000+步，表明智能体具备持续对抗能力。
    - ​**​奖励效率​**​：平均每步奖励从-0.1提升至接近0，反映出动作选择策略的优化。
3. ​**​训练过程可视化​**​：
    - 通过步数、总奖励、奖励效率三组曲线，完整呈现了智能体从随机探索到策略优化的学习轨迹。
### 5.2 关键技术实践
1. ​**​图像预处理​**​：
    - 采用灰度化+二值化+帧堆叠方案，将210x160 RGB图像压缩为4通道84x84特征，在保留关键信息的同时降低计算量。
2. ​**​网络结构设计​**​：
    - 构建3层卷积+2层全连接的Q网络，通过正交初始化提升训练稳定性，网络参数量控制在合理范围（约1.2M）。
3. ​**​训练策略优化​**​：
    - 设置20000步观察期保证经验多样性
    - 采用动态衰减的ε-greedy策略（1.0→0.01）平衡探索与利用
    - 实现软更新目标网络（τ=0.01）提升训练稳定性
### 5.3 挑战与改进
1. ​**​训练资源限制​**​：
    - 受限于GPU显存，经验回放缓冲区仅设置100,000容量，可能影响长期记忆能力。未来可尝试优先经验回放（PER）提升数据利用率。
2. ​**​性能提升空间​**​：
    - 当前最佳得分15分（满分21分）表明策略仍有优化空间，可尝试Dueling DQN、NoisyNet等改进算法。
3. ​**​环境适配优化​**​：
    - 针对Pong的连续击球特性，可试验帧跳过策略调整（当前为4帧）以捕捉更精细的球拍控制时机。
### 5.4 实验收获
本实验完整实现了DQN算法在复杂游戏环境中的应用，深化了对以下要点的理解：
- 经验回放对数据去相关性的重要作用
- 目标网络在稳定Q值估计中的核心价值
- ε-greedy策略的动态平衡机制
- 图像预处理对高维状态空间处理的关键性
通过系统的调参实践与结果分析，积累了深度强化学习工程实现的宝贵经验，为后续研究更复杂的DRL算法（如PPO、SAC）奠定了坚实基础。实验结果表明，DQN作为经典值迭代算法，在适度调参后仍能在中等复杂度环境中展现出色性能。